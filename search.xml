<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Computer_Fundamentals</title>
    <url>/2021/10/13/Computer-Fundamentals/</url>
    <content><![CDATA[<h1 id="计算机基础"><a href="#计算机基础" class="headerlink" title="计算机基础"></a>计算机基础</h1><h2 id="计算机网络"><a href="#计算机网络" class="headerlink" title="计算机网络"></a>计算机网络</h2><ul>
<li>计算机网络体系结构<ul>
<li><a href="https://imgtu.com/i/5KpnqU"><img src="https://z3.ax1x.com/2021/10/13/5KpnqU.png" alt="5KpnqU.png"></a></li>
<li>应用层<ul>
<li>应用层用来规定应用进程在通信时所遵循的协议，应用层的许多协议都是基于客户服务器方式。</li>
<li>涉及到的协议：<ul>
<li>域名系统DNS：将域名和IP地址相互映射的一个分布式数据库，能够使人更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。</li>
<li>HTTP协议：超文本传输协议，所有的万维网文件都必须遵守这个标准。设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法。</li>
<li>邮件传输协议：SMTP</li>
</ul>
</li>
</ul>
</li>
<li>运输层<ul>
<li>只有位于网络边缘部分的主机的协议栈才有运输层，和网络层不同，网络层是为主机之间提供逻辑通信，<font color=red>而运输层为应用程序之间提供通用的数据传输服务（端到端服务）。运输层拥有流量控制（防止过载，即过多的数据注入到网络中），拥塞控制（抑制数据的传输速率）等功能。</font>为上层协议提供端到端的可靠和透明的数据传输服务，上层服务用户不必关心通信子网的实现细节。</li>
<li>运输层主要使用以下两种协议<ul>
<li>传输控制协议TCP：面向连接的协议，提供可靠的数据传输服务。</li>
<li>用户数据报协议UDP：无连接的协议，不提供可靠交付。</li>
</ul>
</li>
<li>运行在TCP上的协议<ul>
<li>HTTP协议（超文本传输协议）：web服务器传输超文本到本地浏览器的传送协议。端口：80</li>
<li>HTTPS（安全超文本传输协议），HTTP协议的安全版本。端口：443</li>
<li>FTP（文件传输协议），用于文件传输，端口：21</li>
<li>SMTP（简单邮件传输协议），用来发送电子邮件。端口：25</li>
<li>SSH，用于加密安全登录用</li>
</ul>
</li>
<li>运行在UDP上的协议<ul>
<li>DNS（域名服务），用于完成地址查找，邮件转发等工作</li>
<li>SNMP（简单网络管理协议），用于网络信息的收集和网络管理</li>
<li>NTP（网络时间协议），用于网络同步。</li>
<li>DHCP（动态主机配置协议），动态配置IP地址。</li>
</ul>
</li>
</ul>
</li>
<li>网络层<ul>
<li><font color=red>网络层采用了IP数据报服务：通过IP寻址来建立两个节点之间的连接，</font>之后把运输层产生的报文段或用户数据报封装成分组和包进行传送，在发送分组时不需要先建立连接，每一个分组独立发送，与其前后的分组无关，在这个过程中网络层不提供端到端的可靠传输服务，尽最大努力进行交付（由网络的主机中的运输层复杂可靠交付）</li>
<li>相关协议：<ul>
<li>与IP协议配套使用的<font color=red>地址解析协议ARP</font></li>
<li>ARP协议完成了IP地址与物理地址的映射，用于动态解析以太网硬件的地址。（IP地址到MAC地址之间的映射）</li>
</ul>
</li>
<li>IP地址部分参考博客<a href="https://blog.csdn.net/w372426096/article/details/78484119">IP地址详解</a></li>
<li>IP地址：整个得因特网就是一个单一的，抽象的网络。IP地址就是给因特网上的每一个主机（或路由器）的每一个接口分配一个在全世界范围内唯一的32位的标识符。</li>
<li>所谓分类的IP地址，就是将IP地址划分为若干固定类，每一类地址都由两个固定长度的字段组成，其中一个字段是<strong>网络号 net-id</strong>，它标志主机（或路由器）所连接到的网络，一个网络号在整个因特网范围内必须是唯一的。而另一个字段则是<strong>主机号 host-id</strong>，它标志该主机（或路由器），一个主机号在它前面的网络号所指明的网络范围内必须是唯一的。由此可见，一个IP地址在整个因特网范围内是唯一的。<ul>
<li><a href="https://imgtu.com/i/5QkpZt"><img src="https://z3.ax1x.com/2021/10/14/5QkpZt.png" alt="5QkpZt.png"></a></li>
<li>地址划分</li>
<li><a href="https://imgtu.com/i/5QkNe1"><img src="https://z3.ax1x.com/2021/10/14/5QkNe1.png" alt="5QkNe1.png"></a></li>
<li><a href="https://imgtu.com/i/5QkjYT"><img src="https://z3.ax1x.com/2021/10/14/5QkjYT.png" alt="5QkjYT.png"></a></li>
<li>子网掩码<ul>
<li>路由器会把子网掩码和收到的数据报地址的目的IP地址145.13.310进行按位与操作，得出所要找的子网网络地址</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>数据链路层<ul>
<li><font color=red>数据链路层：两台设备之间的数据传输，可以看成是在一条管道上进行的，传送的数据单位是帧（每一帧包括数据和必要的控制信息），数据链路层保证传输数据的正确性。</font></li>
</ul>
</li>
<li>物理层<ul>
<li>物理层考虑的是怎样才能在连接各种计算机的传输媒体上传输数据比特流。</li>
</ul>
</li>
</ul>
</li>
<li>TCP与UDP<ul>
<li>TCP&#x2F;IP即传输控制协议，是面向连接的协议，发送数据前要先建立连接，TCP提供可靠的服务，也就是说，通过TCP连接传输的数据不会丢失，没有重复，并且按顺序到达。<ul>
<li>其传送的运输协议数据单元是TCP报文</li>
<li>支持点对点单播，不支持多播。广播</li>
<li>提供可靠服务</li>
<li>复杂。用于大多数应用。如：万维网，电子邮件，文件传送等。</li>
</ul>
</li>
<li>UDP它是属于TCP&#x2F;IP协议族中的一种，是无连接的协议，发送数据之前不需要建立连接，是没有可靠性的协议，因为不需要建立连接所以可以在网络上以任何可能的路径传输，因此，能否到达目的地，到达目的地的时间以及内容的正确性都是不能被保证的。<ul>
<li>无连接的协议，提供无连接服务</li>
<li>其传送的运输协议数据单元TPDU是UDP报文或用户数据报</li>
<li>支持单播，多播，广播</li>
<li>不提供可靠交付</li>
<li>简单，适用于很多应用。</li>
</ul>
</li>
</ul>
</li>
<li><strong>三次握手，四次挥手</strong><ul>
<li><strong>三次握手：</strong>TCP建立连接的过程</li>
<li>客户端向服务器发送SYN—》服务端返回SYN，ACK—》客户端发送ACK</li>
<li>三次握手的目的是建立可靠的通信信道，主要目的就是双方确认自己与对方的发送与接收正常。</li>
<li><strong>四次挥手：</strong>数据传输结束后，通信双方都可释放连接，我们将释放连接的过程称为四次挥手。<ul>
<li>客户端对服务器发送关闭连接的请求</li>
<li>服务器收到客户端的关闭连接请求后，回复一个确认收到的消息</li>
<li>服务器确定不会再给客户发消息后，对客户端发送，准备关闭连接的消息</li>
<li>客户端收到服务器要关闭连接的消息后，给服务器发送：已收到关闭连接的消息</li>
</ul>
</li>
</ul>
</li>
<li><strong>HTTP协议</strong><ul>
<li>HTTP是一个基于TCP&#x2F;IP通信协议来传输数据的协议。HTTP协议工作于客户端-服务器端架构之上，实现可靠性的传输文字，图片，音频，视频等超文本数据的规范，格式简称为“超文本传输协议”。HTTP协议属于应用层，用户访问的第一层就是HTTP<ul>
<li>简单快速：客户端向服务器发送请求时，只需要传送请求方法和路径即可。</li>
<li>灵活：HTTP允许传输任意类型的数据对象。</li>
<li>无连接：限制每次连接只处理一个请求。服务器处理完客户请求，并收到客户应答后，即断开连接。</li>
<li>无状态：协议对于事务处理没有记忆能力。</li>
</ul>
</li>
</ul>
</li>
<li>HTTPS于HTTP<ul>
<li>端口不同，HTTP是80，HTTPS是443</li>
<li>安全性：HTTP是超文本传输协议，信息是明文传输，HTTPS是通过SSL加密处理的传输协议，更加安全。</li>
<li>HTTPS需要拿到CA证书，需要付费</li>
</ul>
</li>
<li>一次完整的HTTP请求<ul>
<li>HTTP协议采用请求&#x2F;响应模型。客户端向服务器发送一个请求报文，请求报文包含请求方法，URL，协议版本，请求头部和请求数据。服务器以一个状态作为响应，响应内容包括协议版本，成功或者错误的代码，服务器信息，响应头部和响应数据。</li>
<li>Web浏览器与Web服务器之间将完成下列7个步骤：<ul>
<li>建立TCP连接，三次握手</li>
<li>Web浏览器向Web服务器发送请求行</li>
<li>Web浏览器发送请求头，浏览器发送请求命令之后，还要以头信息的形式向Web服务器发送一些别的信息，之后浏览器发送了一空白行来通知服务器，它已经结束了该头信息的发送。</li>
<li>Web服务器应答：客户机向服务器发出请求后，服务器会向客户机回送应答，（HTTP&#x2F;1.1 200 OK）应答的第一部分是协议的版本号和应答状态码</li>
<li>Web服务器发送应答头：正如客户端会随同请求发送关于自身的信息一样，服务器也会随应答向用户发送关于它自己的数据及被请求的文档。</li>
<li>Web服务器向浏览器发送数据：Web服务器向浏览器发送头信息后，它会发送一个空白行来表示头信息的发送到此为止，接着，它就以Content-Type应答头信息所描述的格式发送用户所请求的实际数据。</li>
<li>Web服务器关闭TCP连接，四次挥手。</li>
</ul>
</li>
</ul>
</li>
<li>输入网址到获取页面的过程<ul>
<li>浏览器搜索自身的DNS缓存，搜索操作系统的DNS缓存，读取本地Host文件和向本地DNS服务器进行查询等。</li>
<li>浏览器获得域名对应的IP地址之后，浏览器向服务器请求建立连接，发起三次握手</li>
<li>TCP&#x2F;IP链接建立起来后，浏览器向服务器发送HTTP请求</li>
<li>服务器接收到这个请求，并根据路径参数映射到特定的请求处理器进行处理，并将处理结果以及相应的视图返回给浏览器</li>
<li>浏览器解析并渲染视图，若遇到js文件，css文件，图片等静态资源的引用，则重复上述步骤并向服务器请求这些资源。</li>
<li>浏览器根据其请求到的资源，数据渲染页面，最终向用户呈现一个完整的页面。</li>
</ul>
</li>
<li>HTTP版本对比<ul>
<li>HTTP1.0<ul>
<li>是一种无状态，无连接的应用层协议（短连接）</li>
<li>浏览器的每次请求都需要与服务器建立一个TCP连接，服务器处理完成后立即断开TCP连接（无连接），服务器不跟踪每个客户端也不记录过去的请求（无状态）</li>
</ul>
</li>
<li>HTTP1.1（长连接）<ul>
<li>默认持久连接节省通信量，只要客户端服务端任意一端没有明确提出断开TCP连接，就一直保持连接。（长连接）.</li>
</ul>
</li>
<li>HTTP2.0<ul>
<li>服务器推送（就是服务器可以对一个客户端请求发送多个响应。服务器向客户端推送资源无需客户端明确的请求）</li>
</ul>
</li>
</ul>
</li>
<li>GET方法和POST方法（HTTP协议中的请求方式）<ul>
<li>GET:用于请求访问已经被URI（统一资源标识符）识别的资源，可以通过URL传参给服务器</li>
<li>POST：用于传输信息给服务器，主要功能与GET方法类似，但一般推荐使用POST方式</li>
<li>GET方法与POST方法的区别：<ul>
<li>功能上：GET一般用来从服务器获取资源，POST一般用来更新服务器上的资源</li>
<li>安全性：GET是不安全的，因为GET请求提交的数据将明文出现在URL上（请求头上），可能会泄露私密信息；POST请求参数则被包装到请求体中，相对更安全。</li>
<li>数据量：GET传输的数据量小，因为受URL长度限制，但效率较高；POST可以传输大量数据，所以上传文件只能用POST方式。</li>
</ul>
</li>
</ul>
</li>
<li>Session和Cookie的对比<ul>
<li><font color=red>Cookie是由Web服务器保存在用户浏览器上的文件（key-value格式），可以包含用户相关的信息。</font>客户端向服务器发起请求时，会携带服务器端之前创建的cookie，服务器端通过cookie中携带的数据区分不同的用户。</li>
<li>Session：<font color=red>session是浏览器和服务器对话过程中，服务器会分配一块存储空间给session。</font>服务器默认会为客户浏览器的cookie中设置 sessionid，这个sessionid就和cookie对应，浏览器在向服务器请求过程中传输的cookie 包含 sessionid ，服务器根据传输cookie 中的 sessionid 获取出会话中存储的信息，然后确定会话的身份信息。</li>
<li>区别：<ul>
<li>安全性：cookie数据存放在客户端上，安全性较差，session数据放在服务器上，安全性相对更高</li>
<li>大小限制：cookie有大小限制，单个cookie保存的数据不能超过4K,session无此限制，理论上只与服务器的内存大小有关</li>
<li>服务器资源消耗：session是保存在服务器端上会存在一段时间才会消失，当访问增多，对服务器性能有影响。</li>
<li>实现机制：session的实现常常依赖于Cookie机制，通过Cookie机制回传SessionID。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h2><ul>
<li>进程与线程<ul>
<li>进程是资源分配的基本单位。线程是独立调度的基本单位。一个进程中可以有多个线程，他们共享进程资源。</li>
<li>区别：<ul>
<li>拥有资源：进程是资源分配的基本单位，但是线程不拥有资源，线程可以访问隶属进程的资源。</li>
<li>调度：线程是独立调度的基本单位，在同一进程中，线程的切换不会引起进程的切换，从一个进程的线程切换到另一个进程的线程时，会引起进程切换。</li>
<li>系统开销：由于创建或撤销进程时，系统都要为之分配或回收资源，所付出的开销远大于创建或撤销线程时的开销。线程切换时只需保存和设置少量寄存器内容，开销很小。</li>
<li>通信方面：线程间可以通过直接读写同一进程中的数据进行通信，但是进程通信需要借助IPC（进程间通信）</li>
</ul>
</li>
</ul>
</li>
<li>进程通信<ul>
<li>进程通信：进程间传输信息。进程同步：控制多个进程按一定顺序执行</li>
<li>通信方式：<ul>
<li>管道<ul>
<li>管道是通过pipe函数创建的。</li>
<li>支持单向交替传输</li>
<li>只能在父子进程或者兄弟进程中使用</li>
</ul>
</li>
<li>FIFO</li>
<li>消息队列</li>
<li>信息量<ul>
<li>它是一个计数器，用于为多个进程提供对共享数据对象的访问。</li>
</ul>
</li>
<li>共享存储<ul>
<li>允许多个进程共享一个给定的存储区。因为数据不需要在进程间复制，所以这是最快的一种IPC。</li>
</ul>
</li>
<li>套接字<ul>
<li>它可以用于不同机器之间的进程通信</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>死锁<ul>
<li>死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞现象，若无外力作用，它们都将无法推进下去。</li>
<li>产生死锁的必要条件<ul>
<li>互斥</li>
<li>占有和等待</li>
<li>不可抢占</li>
<li>环路等待</li>
</ul>
</li>
</ul>
</li>
<li>虚拟内存<ul>
<li>虚拟内存的目的是为了让物理内存扩充成更大的逻辑内存，从而让程序获得更多的可用内存。</li>
<li>页面置换算法</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>计算机基础</tag>
      </tags>
  </entry>
  <entry>
    <title>Git</title>
    <url>/2023/11/16/Git/</url>
    <content><![CDATA[<h1 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul>
<li><a href="https://www.liaoxuefeng.com/wiki/896043488029600/897271968352576">git学习网站(参考该教程学习并写的博客)</a></li>
</ul>
<ul>
<li>分布式版本控制系统</li>
<li>Linux之父Linus Torvalds两周用C写了一个分布式版本控制系统，一个月内Linux 系统的源码已经由Git管理了。这就是鸡欧帝的实力吗？亏贼！</li>
<li>集中式版本控制系统和分布式版本控制系统的区别<ul>
<li>本质区别：你的本地是否有完整的版本库历史。<ul>
<li>假设SVN服务器消失了，你失去了所有的历史信息，因为你的本地只有当前版本以及部分历史信息。</li>
<li>假设Github服务器消失了，你不会丢掉任何git历史信息，因为你的本地有完整的版本库信息，你可以把本地的git库重新上传到其他的git服务器上去。</li>
</ul>
</li>
<li>集中式版本控制器很依赖中央服务器，需要网络，当网络不好时，每次拉取，提交文件非常缓慢。</li>
</ul>
</li>
</ul>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ul>
<li>linux:sudo apt-get install git</li>
<li>mac:安装一个Xcode</li>
<li>windows:安装一个gitbash</li>
</ul>
<h2 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h2><ul>
<li><p>创建一个目录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ <span class="hljs-built_in">mkdir</span> learngit<br>$ <span class="hljs-built_in">cd</span> learngit<br>$ <span class="hljs-built_in">pwd</span><br>/Users/michael/learngit<br></code></pre></td></tr></table></figure>
</li>
<li><p>git init   通过<code>git init</code>命令把这个目录变成Git可以管理的仓库：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ git init<br>Initialized empty Git repository <span class="hljs-keyword">in</span> /Users/michael/learngit/.git/<br></code></pre></td></tr></table></figure>
</li>
<li><p>之后可以在此目录下创建文件</p>
</li>
<li><p>把一个文件放入git仓库分为两步</p>
<ul>
<li><p><strong>git add</strong> 命令，把文件添加到仓库</p>
</li>
<li><p><strong>git commit</strong> 命令，把文件提交到仓库  -m “注释”。-m””后面输入本次提交的说明</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ git add readme.txt<br>$ git commit -m <span class="hljs-string">&quot;wrote a readme file&quot;</span><br>[master (root-commit) eaadf4e] wrote a readme file<br> 1 file changed, 2 insertions(+)<br> create mode 100644 readme.txt<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="版本管理"><a href="#版本管理" class="headerlink" title="版本管理"></a>版本管理</h3><ul>
<li><p><strong>git status</strong> 命令可以查看仓库当前的状态（可以知道文件是否有修改）</p>
</li>
<li><p><strong>git diff + 文件名</strong> 命令可以查看当前文件和仓库中文件的差异（查看修改的内容）</p>
</li>
</ul>
<h4 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h4><ul>
<li><strong>git log</strong>  命令显示从近到最远的提交日志<ul>
<li><strong>git log –pretty&#x3D;oneline</strong>   加上参数后，会精简输出内容</li>
</ul>
</li>
<li><strong>git reset</strong>  回退版本<ul>
<li><strong>git reset –hard HEAD^</strong>  表示回退到前一个版本，<strong>HEAD^</strong>代表前一个版本，<strong>HEAD^^</strong>代表前两个版本，以此类推</li>
<li><strong>git reset –hard HEAD~100</strong>   回退版本，数字代表回退到前多少个版本</li>
<li><strong>git reset –hard 1094a</strong>  加上版本号的前几位即可，会回退到指定的版本</li>
</ul>
</li>
<li><strong>git reflog</strong>  查看命令历史<ul>
<li>可以通过查看命令历史，重返未来的某个版本。（对于当前版本来说是未来，因为当前版本是从未来某个版本回退回来的）</li>
</ul>
</li>
</ul>
<h4 id="工作区和暂存区"><a href="#工作区和暂存区" class="headerlink" title="工作区和暂存区"></a>工作区和暂存区</h4><ul>
<li><p>工作区</p>
<ul>
<li>在git仓库中目前的文件，就是工作区</li>
</ul>
</li>
<li><p>版本库</p>
<ul>
<li><p>工作区目录下有一个隐藏文件**.git**，是git的版本库</p>
</li>
<li><p>git版本库中最重要的就是<strong>stage(index)<strong>暂存区，git自动创建的一个</strong>master</strong>，以及指向<strong>master</strong>的一个指针叫<strong>HEAD</strong></p>
<p><img src="https://www.liaoxuefeng.com/files/attachments/919020037470528/0" alt="版本库"></p>
</li>
</ul>
<ul>
<li><p>git add命令就是把提交的所有修改放到暂存区(stage)，git commit 命令就是一次性把暂存区的所有修改提交至版本库(master)</p>
</li>
<li><p>查看工作区和版本库中最新版本的区别：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">git diff HEAD -- readme.txt<br></code></pre></td></tr></table></figure>
</li>
<li><pre><code class="bash"># git reset命令既可以回退版本，也可以把暂存区的修改回退到工作区。当我们用HEAD时，表示最新的版本。
git reset HEAD readme.txt
# 丢弃工作区的修改
git checkout -- readme.txt
#git checkout其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。
# 删除文件，然后commit文件就删除了
rm test.txt

<figure class="highlight clean"><table><tr><td class="code"><pre><code class="hljs clean"><br>### 远程仓库<br><br>* 采用github作为远程在线的git服务器<br><br>* 注册一个github账号，本地git仓库和github仓库之间的传输是通过SSH加密的<br><br>* 步骤<br><br>  ````bash<br>  #第<span class="hljs-number">1</span>步：创建SSH Key。在C盘当前用户的主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和#id_rsa.pub这两个文件。有的话也可以删掉重新创建ssh key。如果没有，打开Shell（Windows下打开Git #Bash），创建SSH Key：<br>  ssh-keygen -t rsa -C <span class="hljs-string">&quot;youremail@example.com&quot;</span><br>  <br>  #第二步：在github中将公钥id_rsa.pub的内容添加<br>  <br>  #为什么GitHub需要SSH Key呢？因为GitHub需要识别出你推送的提交确实是你推送的，而不是别人冒充的，而Git支持SSH协议，所以，GitHub只要知道了你的公钥，就可以确认只有你自己才能推送。<br>  <br>  #当然，GitHub允许你添加多个Key。假定你有若干电脑，你一会儿在公司提交，一会儿在家里提交，只要把每台电脑的Key都添加到GitHub，就可以在每台电脑上往GitHub推送了。<br>  <br>  #最后友情提示，在GitHub上免费托管的Git仓库，任何人都可以看到喔（但只有你自己才能改）。所以，不要把敏感信息放进去。<br>  <br>  #如果你不想让别人看到Git库，有两个办法，一个是交点保护费，让GitHub把公开的仓库变成私有的，这样别人就看不见了（不可读更不可写）。另一个办法是自己动手，搭一个Git服务器，因为是你自己的Git服务器，所以别人也是看不见的。这个方法我们后面会讲到的，相当简单，公司内部开发必备。<br>  <br>  #确保你拥有一个GitHub账号后，我们就即将开始远程仓库的学习。<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p><img src="D:\图片\gtihub添加ssh公钥.png"></p>
</li>
</ul>
</li>
</ul>
<h4 id="添加远程库"><a href="#添加远程库" class="headerlink" title="添加远程库"></a>添加远程库</h4><ul>
<li><p>如果你在本地建好了一个git仓库后，想在github上新建一个git仓库，并且可以实现两个仓库之间的远程同步</p>
<ul>
<li><p>第一步：在github创建一个仓库</p>
</li>
<li><p>第二步：和新创建的远程github仓库进行关联</p>
<ul>
<li><pre><code class="bash">git remote add origin git@github.com:zt3019/learngit.git
<figure class="highlight crmsh"><table><tr><td class="code"><pre><code class="hljs crmsh"><br>- 第三步：将本地库的所有内容推送到远程库上<br><br>  - ````bash<br>    <span class="hljs-comment">#添加关联后，远程库的名字叫origin，这是git的默认叫法，也可以修改</span><br>    git push -u origin <span class="hljs-keyword">master</span><br>    <span class="hljs-title">#由于远程库是空的，我们第一次推送master</span>分支时，加上了-u参数，Git不但会把本地的<span class="hljs-literal">master</span>分支内容推送的远程新的<span class="hljs-literal">master</span>分支，还会把本地的<span class="hljs-literal">master</span>分支和远程的<span class="hljs-literal">master</span>分支关联起来，在以后的推送或者拉取时就可以简化命令。<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p>在这之后，需要提交代码到远程库只需要执行push命令</p>
<ul>
<li><pre><code class="bash">git push origin master
#查看远程库信息
git remote -v
#删除远程库
git remote rm origin

<figure class="highlight autohotkey"><table><tr><td class="code"><pre><code class="hljs autohotkey"><br>#### 从远程库克隆<br><br>* 如果是先创建远程库，然后从远程库克隆<br><br>  - 先登录远程库，如github，创建一个新的仓库<br><br>  - 远程库创建好之后，用git clone克隆一个本地库<br><br>    ````bash<br><span class="hljs-title">    git clone git@github.com:</span>zt3019/gitskills.git<br>    ````<br><br>  - 之后就可以在本地看到创建的gitskills仓库了<br><br>* Git支持多种协议，包括`https`，但`ssh`协议速度最快。<br><br>### 分支管理<br><br>* 分支在实际中有什么用呢？假设你准备开发一个新功能，但是需要两周才能完成，第一周你写了<span class="hljs-number">50</span>%的代码，如果立刻提交，由于代码还没写完，不完整的代码库会导致别人不能干活了。如果等代码全部写完再一次提交，又存在丢失每天进度的巨大风险。<br><br>  现在有了分支，就不用怕了。你创建了一个属于你自己的分支，别人看不到，还继续在原来的分支上正常工作，而你在自己的分支上干活，想提交就提交，直到开发完毕后，再一次性合并到原来的分支上，这样，既安全，又不影响别人工作。<br><br>* 每次提交，git都把他们串成一条时间线，这条时间线就是一个分支。主分支即**master**分支，**HEAD**严格来说不是指向提交，而是指向**master**，**master**才是指向提交的，所以，**HEAD**指向的就是当前分支。<br><br>* 例如：git新增一个dev分支，git创建了一个指针叫dev，指向master相同的提交，再把`HEAD`指向`dev`，就表示当前分支在dev上。如果在dev上操作完成之后，将`dev`合并到`master`，将`master`指向`dev`当前的提交，就完成了合并。所以git合并分支非常快，修改一下指针的指向。合并完成后，我们可以删除`dev`分支，而删除`dev`分支就是把`dev`指针给删掉，删掉后，我们就剩下了一条`master`分支<br><br><span class="hljs-title">* [廖雪峰的官方网站中的git教程](https:</span>//www.liaoxuefeng.com/wiki/<span class="hljs-number">896043488029600</span>/<span class="hljs-number">900003767775424</span>)<br><br>* 实操：<br><br>  - 第一步：创建`dev`分支<br><br>    - ````bash<br>      git switch -c dev<br><span class="hljs-title">      #git checkout命令加上-b参数表示创建并切换,相当于下面两条语句:</span><br>      git branch dev<br>      git switch dev<br>      #使用git branch命令查看当前分支<br>      git branch<br>      ````<br><br>  - 第二步：切换到`dev`分支后，在`dev`分支修改文件内容<br><br>    - ```bash<br>      #在dev分支修改git文件内容之后，提交修改的文件<br>      git add readme.txt<br>      git commit -m <span class="hljs-string">&quot;branch test&quot;</span><br>      <br>      # 切换回master分支，我们无法看到刚才修改的内容，因为刚才的添加提交是在dev分支上的<br>      git switch master<br>      <br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li></li>
</ul>
</li>
<li><p>第三步：把<code>dev</code>分支的工作成果合并到<code>master</code>分支上</p>
<ul>
<li><pre><code class="bash">#git merge命令用于合并指定分支到当前分支。合并后，再查看readme.txt的内容，就可以看到，和dev分支的最新提交是完全一样的。
git merge dev
#合并完成后，我们可以在master分支上看到刚才修改的内容，然后就可以删除dev分支了
git branch -d dev
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><code class="hljs mipsasm"><br>    - 因为创建、合并和删除分支非常快，所以Git鼓励你使用分支完成某个任务，合并后再删掉分支，这和直接在master分支上工作效果是一样的，但过程更安全。<br><br>  - 建议使用git <span class="hljs-keyword">switch来切换分支，更易于理解</span><br><span class="hljs-keyword"></span><br><span class="hljs-comment">#### 解决分支冲突</span><br><br>* 分支合并是有可能会出现分支冲突的，比如下面的一个案例<br><br>  - 创建一个新的分支，修改文件内容，进行<span class="hljs-keyword">add并commit</span><br><span class="hljs-keyword"></span><br>    - ````<span class="hljs-keyword">bash</span><br><span class="hljs-keyword"></span>      <span class="hljs-comment">#创建feature分支并切换到该分支</span><br>      git <span class="hljs-keyword">switch </span>-c feature<br>      <span class="hljs-comment">#修改readme.txt文件内容，添加一行：creating a new branch named feature</span><br>      <span class="hljs-comment">#然后添加到暂存区并提交</span><br>      git <span class="hljs-keyword">add </span>readme.txt<br>      git commit -m <span class="hljs-string">&quot;new a branch named feature1&quot;</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p>切换到master分支，也修改文件内容，进行add并commit</p>
<ul>
<li><pre><code class="bash">#切换分支
git switch master
#修改readme.txt文件内容，添加一行：分支冲突吧你
#然后添加到暂存区并提交
git add readme.txt
git commit -m &quot;add chinese&quot;
<figure class="highlight ruby"><table><tr><td class="code"><pre><code class="hljs ruby"><br>- 在master进行合并，将feature分支合并进来<br><br>  - <span class="hljs-string">``</span><span class="hljs-string">``</span>bash<br>    <span class="hljs-comment">#合并分支</span><br>    git merge feature<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>两个分支都对readme.txt文件内容进行了不同的修改，无法进行快速合并，git会发出报错信息，<code>readme.txt</code>文件存在冲突，必须手动解决冲突之后再提交。git status可以查看到冲突的文件。</p>
</li>
<li><p><img src="D:\图片\分支冲突.png" alt="分支冲突"></p>
</li>
</ul>
</li>
<li><p>出现冲突之后，我们需要解决冲突</p>
<ul>
<li><pre><code class="bash">vim readme.txt
#Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容
#重新编辑冲突了的文件，保留你需要的内容，去除冲突的部分
#然后重新add,commit
git add readme.txt 
git commit -m &quot;conflict fixed&quot;
#git会显示&quot;conflict fixed&quot;
<figure class="highlight dsconfig"><table><tr><td class="code"><pre><code class="hljs dsconfig"><br>- 最后工作<br><br>  - ````<span class="hljs-string">bash</span><br>    <span class="hljs-comment">#从git提交的分支图可以看到我们解决分支冲突的过程</span><br>    <span class="hljs-string">git</span> <span class="hljs-string">log</span> <span class="hljs-built_in">--graph</span> <span class="hljs-built_in">--pretty=oneline</span> <span class="hljs-built_in">--abbrev-commit</span><br>    <br>    <span class="hljs-comment">#最后，我们可以将feature分支删除</span><br>    <span class="hljs-string">git</span> <span class="hljs-string">branch</span> -<span class="hljs-string">d</span> <span class="hljs-string">feature</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p><img src="D:\图片\git分支图.png"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="分支管理策略"><a href="#分支管理策略" class="headerlink" title="分支管理策略"></a>分支管理策略</h4><ul>
<li><p>一般情况下，合并分支时，git会使用<code>fast forward</code>模式,这种模式下，删除分支后，会丢掉分支信息</p>
</li>
<li><p>如果要强制禁用<code>Fast forward</code>模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。</p>
</li>
<li><p><a href="https://www.liaoxuefeng.com/wiki/896043488029600/900005860592480">关于分支管理策略的参考博客</a></p>
</li>
<li><p>通过一个案例来演示：</p>
<ul>
<li><pre><code class="bash">#创建并切换dev分支
git switch -c dev
#修改readme.txt文件内容并提交
git add readme.txt 
git commit -m &quot;add merge&quot;
#切换回master
git switch master
#回到master分支之后，合并dev分支,使用--no-ff参数，表示禁用Fast forward
git merge --no-ff -m &quot;merge with no-ff&quot; dev
#再查看git提交的分支树
git log --graph --pretty=oneline --abbrev-commit
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><code class="hljs mipsasm"><br>  - ![](D:\图片\分支策略.png)<br><br><span class="hljs-comment">#### Bug分支</span><br><br>- 我们在开发过程中，遇到了<span class="hljs-keyword">bug，我们可以通过创建一个新的临时分支来修复，修复之后，合并分支，然后将临时分支删除。举个栗子：</span><br><span class="hljs-keyword"></span><br>  - 当我们收到一个修复<span class="hljs-keyword">bug的任务，我们创建一个`issue-3208`来修复它，但是我们在dev分支上进行的工作只完成了一半，不能够提交。而且我们必须先解决bug</span><br><span class="hljs-keyword"></span><br>    - ````<span class="hljs-keyword">bash</span><br><span class="hljs-keyword"></span>      <span class="hljs-comment">#stash功能，可以把当前工作现场储存起来，等后续恢复现场后继续工作</span><br>      git status<br>      <span class="hljs-comment">#执行status命令之后，再用git status查看工作区，就是干净的</span><br>      <span class="hljs-comment">#首先要确定是在哪个分支上修复bug,例如需要在master分支上修复</span><br>      git <span class="hljs-keyword">switch </span>master<br>      <br>      <span class="hljs-comment">#在master分支上创建临时修复bug的分支</span><br>      git <span class="hljs-keyword">switch </span>-c issue<span class="hljs-number">-3208</span><br>      <br>      <br>      <span class="hljs-comment">#在新建的分支修复bug，修复完成之后，进行add&amp;commit</span><br>      git <span class="hljs-keyword">add </span>readme.txt<br>      git commit -m <span class="hljs-string">&quot;fix bug 3208&quot;</span><br>      <br>      <span class="hljs-comment">#修复完成之后，切换到master分支，并完成合并，最后删除issue-3208分支</span><br>      git <span class="hljs-keyword">switch </span>master<br>      <br>      git merge --no-ff -m <span class="hljs-string">&quot;merged bug fix 3208&quot;</span> issue<span class="hljs-number">-101</span><br>      <br>      <span class="hljs-comment">#完成bug修复之后，继续回到dev分支干活</span><br>      git <span class="hljs-keyword">switch </span>dev<br>      <br>      <span class="hljs-comment">#通过git status 命令发现工作区是干净的</span><br>      <span class="hljs-comment">#通过stash list 命令查看，隐藏的工作区</span><br>      git stash list <br>      <br>      <span class="hljs-comment">#一是用git stash apply恢复，但是恢复后，stash内容并不删除，你需要用git stash drop来删除；</span><br>      <span class="hljs-comment">#另一种方式是用git stash pop，恢复的同时把stash内容也删了：</span><br>      git stash pop<br>      <br>      <span class="hljs-comment">#可以进行多次stash，恢复的时候，先查看git stash list 的列表，然后恢复指定的stash</span><br>      git stash apply stash@&#123;<span class="hljs-number">0</span>&#125;<br>      <br>      <span class="hljs-comment">#在master分支上修复的bug，想要合并到当前dev分支，可以用git cherry-pick &lt;commit&gt;命令，把bug提交的修改“复制”到当前分支，避免重复劳动。</span><br></code></pre></td></tr></table></figure>

-
</code></pre>
</li>
</ul>
</li>
</ul>
<h4 id="Feature分支"><a href="#Feature分支" class="headerlink" title="Feature分支"></a>Feature分支</h4><ul>
<li><p>在开发中，有新功能进来的时候，创建一个feature分支，在上面开发完成之后合并，最终删除该分支</p>
<ul>
<li><pre><code class="bash">#开发一个新需求
git switch -c feature-vulcan

#开发完成后进行add&amp;commit

#之后应该切换到dev并进行合并，然后删除新建的分支

#新需求砍掉了
git branch -D feature-vulcan

#-D参数强行删除
<figure class="highlight perl"><table><tr><td class="code"><pre><code class="hljs perl"><br>  - 如果要丢弃一个没有被合并过的分支，可以通过<span class="hljs-string">`git branch -D &lt;name&gt;`</span>强行删除。<br><br><span class="hljs-comment">#### 多人协作</span><br><br>* 当你从远程仓库克隆时，git自动把本地的<span class="hljs-string">`master`</span>分支和远程的<span class="hljs-string">`master`</span>分支对应起来了，并且远程仓库的默认名称是<span class="hljs-string">`origin`</span>。<br><br>  - <span class="hljs-string">``</span><span class="hljs-string">``</span>bash<br>    <span class="hljs-comment"># 查看远程库的详细信息</span><br>    git remote -v<br>    <span class="hljs-comment">#上面显示了可以抓取和推送的origin的地址。如果没有推送权限，就看不到push的地址。</span><br>    <span class="hljs-comment">#推送分支，就是把该分支上的所有本地提交到远程库，推送时，要指定本地分支，git就会把该分支推送到远程库对应的远程分支上</span><br>    git <span class="hljs-keyword">push</span> origin master<br>    git <span class="hljs-keyword">push</span> origin dev<br>    <br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p><code>master</code>分支是主分支，因此要时刻与远程同步；</p>
</li>
<li><p><code>dev</code>分支是开发分支，团队所有成员都需要在上面工作，所以也需要与远程同步；</p>
</li>
<li><p>bug分支只用于在本地修复bug，就没必要推到远程了，除非老板要看看你每周到底修复了几个bug；</p>
</li>
<li><p>feature分支是否推到远程，取决于你是否和你的小伙伴合作在上面开发。</p>
</li>
</ul>
</li>
<li><p>因此，多人协作的工作模式通常是这样：</p>
<ol>
<li>首先，可以试图用<code>git push origin &lt;branch-name&gt;</code>推送自己的修改；</li>
<li>如果推送失败，则因为远程分支比你的本地更新，需要先用<code>git pull</code>试图合并；</li>
<li>如果合并有冲突，则解决冲突，并在本地提交；</li>
<li>没有冲突或者解决掉冲突后，再用<code>git push origin &lt;branch-name&gt;</code>推送就能成功！</li>
</ol>
<p>如果<code>git pull</code>提示<code>no tracking information</code>，则说明本地分支和远程分支的链接关系没有创建，用命令<code>git branch --set-upstream-to &lt;branch-name&gt; origin/&lt;branch-name&gt;</code>。</p>
<p>这就是多人协作的工作模式，一旦熟悉了，就非常简单。</p>
</li>
</ul>
<h4 id="Rebase"><a href="#Rebase" class="headerlink" title="Rebase"></a>Rebase</h4><ul>
<li>rebase操作可以把本地未push的分叉提交历史整理成直线；</li>
<li>rebase的目的是使得我们在查看历史提交的变化时更容易，因为分叉的提交需要三方对比。</li>
<li><a href="rebase%E6%8F%8F%E8%BF%B0">https://www.liaoxuefeng.com/wiki/896043488029600/1216289527823648</a></li>
</ul>
<h3 id="标签管理"><a href="#标签管理" class="headerlink" title="标签管理"></a>标签管理</h3><h4 id="创建标签"><a href="#创建标签" class="headerlink" title="创建标签"></a>创建标签</h4><ul>
<li><p>发布一个版本时，通常现在版本库中打一个标签（tag），这样就确定了打标签时刻的版本。标签就是版本库的一个快照。commit号太长了，不方便寻找，使用标签号进行提交发版，更加清晰明了。</p>
</li>
<li><p>打标签</p>
<ul>
<li><pre><code class="bash">#切换到需要打标签的分支上
git switch master
#git tag &lt;name&gt;命令打上标签,不加名字可以查看标签
git tag v1.0
#默认标签是打在最新提交的commit上的，要为之前的commit打上标签，需要找到commit id
git tagv0.9 5f23208

# git 命令查看标签，不是按时间顺序列出，而是按照字母排序的
git tag
#使用git show &lt;tagname&gt;查看标签详细信息
git show v1.0

#创建带有说明的标签，用-a指定标签名，-m指定说明文字
git tag -a v0.1 -m &quot;version 0.1 released&quot; 4d39864

<figure class="highlight crmsh"><table><tr><td class="code"><pre><code class="hljs crmsh"><br>  - 标签总是和某个commit挂钩。如果这个commit既出现在<span class="hljs-literal">master</span>分支，又出现在dev分支，那么在这两个分支上都可以看到这个标签。<br><br><span class="hljs-comment">#### 操作标签</span><br><br>- 可以删除标签，推送标签至远程<br><br>  - ````bash<br>    <span class="hljs-comment">#删除标签</span><br>    git <span class="hljs-keyword">tag</span> <span class="hljs-title">-d</span> v0.<span class="hljs-number">1</span><br>    <br>    <span class="hljs-comment">#推送某个标签到远程</span><br>    git push origin v1.<span class="hljs-number">0</span><br>    <span class="hljs-comment">#一次性推送全部尚未推送到远程的本地标签</span><br>    git push origin --tags<br>    <br>    <span class="hljs-comment">#如果标签已经推送至远程库，删除远程标签需要两步，先删除本地，再删除远程</span><br>    git <span class="hljs-keyword">tag</span> <span class="hljs-title">-d</span> v0.<span class="hljs-number">9</span><br>    <br>    git push origin -d <span class="hljs-keyword">tag</span> <span class="hljs-title">v0</span>.<span class="hljs-number">9</span><br>    <span class="hljs-comment">#之后可以去远程库查看标签是否已经删除</span><br>    <br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
<h3 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a>远程仓库</h3><ul>
<li>在GitHub上，可以任意Fork开源仓库；</li>
<li>自己拥有Fork后的仓库的读写权限；</li>
<li>可以推送pull request给官方仓库来贡献代码。</li>
<li>gitee国内的git托管服务商，有中文，不容易出现网络问题</li>
</ul>
<h3 id="通用配置"><a href="#通用配置" class="headerlink" title="通用配置"></a>通用配置</h3><ul>
<li><p>有时候一些隐私文件也在git工作目录中，但是不能提交他们。比如保存了数据库密码的配置文件</p>
</li>
<li><p>可以在Git工作区的根目录下创建一个特殊的<code>.gitignore</code>文件，然后把要忽略的文件名填进去，Git就会自动忽略这些文件。</p>
</li>
<li><p>忽略文件的原则是：</p>
<ol>
<li>忽略操作系统自动生成的文件，比如缩略图等；</li>
<li>忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的<code>.class</code>文件；</li>
<li>忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。</li>
</ol>
</li>
<li><p>不需要从头写<code>.gitignore</code>文件，GitHub已经为我们准备了各种配置文件，只需要组合一下就可以使用了。所有配置文件可以直接在线浏览：<a href="https://github.com/github/gitignore">https://github.com/github/gitignore</a></p>
</li>
<li><pre><code class="bash">#强制添加文件到git
git add -f App.class
#或者你发现，可能是.gitignore写得有问题，需要找出来到底哪个规则写错了，可以用git check-ignore命令检查
git check-ignore -v App.class
<figure class="highlight mel"><table><tr><td class="code"><pre><code class="hljs mel"><br>* 把指定文件排除在<span class="hljs-string">`.gitignore`</span>规则外的写法就是<span class="hljs-string">`!`</span>+文件名，所以，只需把例外文件添加进去即可。<br><br>* 忽略某些文件时，需要编写<span class="hljs-string">`.gitignore`</span>；<br><br>* <span class="hljs-string">`.gitignore`</span>文件本身要放到版本库里，并且可以对<span class="hljs-string">`.gitignore`</span>做版本管理！<br><br>* 可以给git命令设置简称<br><br>  - <span class="hljs-string">``</span><span class="hljs-string">``</span>bash<br>    git config --<span class="hljs-keyword">global</span> <span class="hljs-keyword">alias</span>.st status<br>    git config --<span class="hljs-keyword">global</span> <span class="hljs-keyword">alias</span>.co checkout<br>    git config --<span class="hljs-keyword">global</span> <span class="hljs-keyword">alias</span>.ci commit<br>    git config --<span class="hljs-keyword">global</span> <span class="hljs-keyword">alias</span>.br branch<br>    #--<span class="hljs-keyword">global</span>参数是全局参数，也就是这些命令在这台电脑的所有Git仓库下都有用<br>    git config --<span class="hljs-keyword">global</span> <span class="hljs-keyword">alias</span>.unstage <span class="hljs-string">&#x27;reset HEAD&#x27;</span><br>    #执行上面的命令之后<br>    git unstage test.py == git reset HEAD test.py<br>    <br>    git config --<span class="hljs-keyword">global</span> <span class="hljs-keyword">alias</span>.lg <span class="hljs-string">&quot;log --color --graph --pretty=format:&#x27;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#x27; --abbrev-commit&quot;</span><br>    <br>    git lg<br></code></pre></td></tr></table></figure>

- 每个仓库的Git配置文件都放在`.git/config`文件中

- 别名就在`[alias]`后面，要删除别名，直接把对应的行删掉即可。

- 而当前用户的Git配置文件放在用户主目录下的一个隐藏文件`.gitconfig`中

- 配置别名也可以直接修改这个文件，如果改错了，可以删掉文件重新通过命令配置。
</code></pre>
</li>
</ul>
<h2 id="利用git分支，多端更新博客"><a href="#利用git分支，多端更新博客" class="headerlink" title="利用git分支，多端更新博客"></a>利用git分支，多端更新博客</h2><ul>
<li>参考博客<a href="https://blog.csdn.net/K1052176873/article/details/122879462">链接</a></li>
</ul>
<h3 id="需求：想在不同地方的电脑更新博客"><a href="#需求：想在不同地方的电脑更新博客" class="headerlink" title="需求：想在不同地方的电脑更新博客"></a>需求：想在不同地方的电脑更新博客</h3><h3 id="解决原理"><a href="#解决原理" class="headerlink" title="解决原理"></a>解决原理</h3><ul>
<li><p>hexo博客目录结构</p>
<ul>
<li><a href="https://imgse.com/i/pFtSSW8"><img src="https://s11.ax1x.com/2024/02/20/pFtSSW8.png" alt="目录结构"></a></li>
</ul>
</li>
<li><p>hexo生成的静态页面文件默认放在master分支上，是_config.yml配置文件配置的</p>
<ul>
<li><a href="https://imgse.com/i/pFYzbsH"><img src="https://s11.ax1x.com/2024/02/20/pFYzbsH.png" alt="pFYzbsH.png"></a></li>
</ul>
</li>
<li><p>每次写完博客执行hexo deploy的时候，hexo会帮我们把生成好的静态页面文件推到master分支上。</p>
</li>
<li><p>第一次部署好博客时，github给我们创建的唯一一个分支就是master分支，同时也会是github的默认分支。每次git clone 或 git pull时拉取的都是默认分支的代码</p>
</li>
<li><p><strong>hexo delpoy执行时，推送静态页面文件到github，和是否是默认分支是无关的。这是由_config.yml配置文件决定的，写着什么分支就推送到什么分支</strong></p>
</li>
<li><p>根据hexo提交的特性，现在hexo生成的静态博客文件都放在master分支上。所以我们可以新创建一个hexo分支，然后把hexo分支设置为默认分支。</p>
</li>
</ul>
<h3 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h3><ul>
<li><strong>把新创建的hexo设置为默认分支，用于存放博客的所有需要的源文件，master分支依然存放静态文件</strong></li>
<li>在旧电脑上，把必要的博客源文件上传git push到hexo分支。在新电脑上git clone +”仓库地址”，把hexo分支的文件下载下来，剩下的就是安装好hexo环境，随后就可以在新电脑上hexo deploy推送生成的静态页面到master分支上。（因为克隆下来的博客源文件中的_config.yml配置文件写的是master分支）</li>
<li>这种方式创建的两个分支，实际上是完全独立的两个分支。一个是静态页面的文件，一个是博客的源文件。通过<code>git add . git commit -m &quot;&quot; git push </code>来更新源文件分支，<code>hexo deploy</code>来更新静态页面</li>
</ul>
<h4 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h4><ol>
<li><p>在github创建一个hexo分支，并设置其为默认分支</p>
</li>
<li><p>打包将要推送到github上的文件</p>
<ul>
<li><p>clone该仓库到本地（clone的是hexo默认分支）</p>
</li>
<li><p>clone的文件夹里仅留下.git 文件夹，其他的文件都删除</p>
</li>
<li><p>找见我们hexo原位置，将hexo文件夹内除.deploy_git 以外都复制到clone下来的文件夹中</p>
</li>
<li><p>可以配置一下<code>.gitignore</code>文件，减少一些非必要文件的上传</p>
<ul>
<li><pre><code class="bash">.DS_Store
Thumbs.db
db.json
*.log
node_modules/
public/
.deploy*/

<figure class="highlight dsconfig"><table><tr><td class="code"><pre><code class="hljs dsconfig"><br>* 如果已经<span class="hljs-string">clone</span>过主题文件，那么需要把<span class="hljs-string">theme</span>主题文件夹里的 .<span class="hljs-string">git</span> 也删除。因为<span class="hljs-string">git</span>不能嵌套上传，最好是显示隐藏文件，检查一下有没有，否则上传的时候会出错，导致你的主题文件无法上传，这样你的配置在别的电脑上就用不了了。<br>* 最后将<span class="hljs-string">clone</span>并修改之后的文件夹推送到远程库<br><br><span class="hljs-comment">## 问题记录</span><br><br>* 网络不好的时候用<span class="hljs-string">http</span>提交或者拉取代码会失败，可以考虑使用<span class="hljs-string">ssh</span>的方式<br><br>  - 设置 远程仓库的提交链接：<br>  - <span class="hljs-string">git</span> <span class="hljs-string">remote</span> <span class="hljs-built_in">set-url</span> <span class="hljs-string">origin</span> <span class="hljs-string">git</span>@<span class="hljs-string">github</span>.<span class="hljs-string">com:zt3019/</span><span class="hljs-string">zt3019</span>.<span class="hljs-string">github</span>.<span class="hljs-string">io</span>.<span class="hljs-string">git</span><br>  <br>  - ````<span class="hljs-string">bash</span><br>    -- <span class="hljs-string">ssh</span>方式<br>    <span class="hljs-string">git</span> <span class="hljs-string">pull</span> <span class="hljs-string">git</span>@<span class="hljs-string">github</span>.<span class="hljs-string">com:zt3019/</span><span class="hljs-string">zt3019</span>.<span class="hljs-string">github</span>.<span class="hljs-string">io</span>.<span class="hljs-string">git</span><br>    <span class="hljs-string">git</span> <span class="hljs-string">push</span> <span class="hljs-string">git</span>@<span class="hljs-string">github</span>.<span class="hljs-string">com:zt3019/</span><span class="hljs-string">zt3019</span>.<span class="hljs-string">github</span>.<span class="hljs-string">io</span>.<span class="hljs-string">git</span><br>    -- <span class="hljs-string">https</span>方式<br>    <span class="hljs-string">git</span> <span class="hljs-string">pull</span> <span class="hljs-string">https</span>://<span class="hljs-string">github</span>.<span class="hljs-string">com</span>/<span class="hljs-string">zt3019</span>/<span class="hljs-string">zt3019</span>.<span class="hljs-string">github</span>.<span class="hljs-string">io</span><br>    <span class="hljs-string">git</span> <span class="hljs-string">push</span> <span class="hljs-string">https</span>://<span class="hljs-string">github</span>.<span class="hljs-string">com</span>/<span class="hljs-string">zt3019</span>/<span class="hljs-string">zt3019</span>.<span class="hljs-string">github</span>.<span class="hljs-string">io</span><br>    <br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Elasticsearch_learning</title>
    <url>/2024/04/15/Elasticsearch-learning/</url>
    <content><![CDATA[<h1 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h1><ul>
<li><p>Elaticsearch，简称为es，<strong>es是一个开源的高扩展的分布式全文检索引擎 <strong>，它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。es也使用Java开发并使用 <code>Lucene</code> 作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的 RESTful API 来隐藏 Lucene 的复杂性，从而让全文搜索变得简单。</strong>近实时搜索平台框架</strong></p>
</li>
<li><p>Lucene是apache软件基金会的项目，<strong>是一个开放源代码的全文检索引擎工具包</strong>，但它不是一个完整的全文检索引擎，而是一个全文检索引擎的架构</p>
</li>
<li><p>类似的工具有<code>Solr</code></p>
</li>
<li><p>ELK</p>
<ul>
<li>ELK是<code>Elasticsearch</code>、<code>Logstash</code>、<code>Kibana</code>三大开源框架首字母大写简称。市面上也被成为 ElasticStack 。</li>
<li>Elasticsearch 是一个基于 Lucene、分布式、通过 Restful 方式进行交互的<strong>近实时搜索平台框架</strong>。</li>
<li>Logstash是 ELK 的<strong>中央数据流引擎</strong>，用于从不同目标（文件&#x2F;数据存储&#x2F;MQ）收集的不同格式数据，经过过滤后支持输出到不同目的地（文件&#x2F;MQ&#x2F;redis&#x2F;elasticsearch&#x2F;kafka等）。</li>
<li>Kibana可以将 elasticsearch 的数据<strong>通过友好的页面展示出来，提供实时分析的功能</strong>。</li>
</ul>
</li>
<li><p><strong>ELK 不仅仅适用于日志分析，它还可以支持其它任何数据分析和收集的场景，日志分析和收集只是更具有代表性，并非唯一性</strong>。</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/358744225">参考链接1</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/chenhuabin/p/13800715.html">参考链接2</a></p>
</li>
<li><p><a href="https://learnku.com/docs/elasticsearch73/7.3/data-in-documents-and-indices/6446">参考链接3</a></p>
</li>
</ul>
<h2 id="ES核心概念"><a href="#ES核心概念" class="headerlink" title="ES核心概念"></a>ES核心概念</h2><ul>
<li><p>ES是面向文档的<strong>非关系型数据库</strong></p>
</li>
<li><p>elasticsearch(集群)中可以包含多个索引(数据库)，每个索引中可以包含多个类型(表)，每个类型下又包含多个文档(行)，每个文档中又包含多个字段(列)</p>
</li>
<li><p><code>GET /zt_test/_doc/2</code></p>
<ul>
<li><p>zt_test：index的概念：<strong>索引是文档(Document)的容器，是一类文档的集合。</strong>三种意思：</p>
<ul>
<li>索引（名次）：相当于关系型数据库中的(database),索引由其名称(<strong>必须为全小写字符</strong>)进行标识</li>
<li>索引（动词）：保存一个文档到索引（名词）的过程。类似于sql中的insert或update</li>
<li>倒排索引：关系型数据库通过给某个字段增加一个b+树索引到指定的列，提升检索速度。es使用了一个叫做倒排索引的结构来达到相同的目的。</li>
</ul>
</li>
<li><p>_doc：type的概念</p>
<ul>
<li><p>之前的版本中，索引和文档中间还有个类型的概念，每个索引下可以建立多个类型，文档存储时需要指定index和type。从6.0.0开始单个索引中只能有一个类型，</p>
<p>7.0.0以后将将不建议使用，8.0.0 以后完全不支持。</p>
<h5 id="弃用该概念的原因："><a href="#弃用该概念的原因：" class="headerlink" title="弃用该概念的原因："></a>弃用该概念的原因：</h5><p>我们虽然可以通俗的去理解Index比作 SQL 的 Database，Type比作SQL的Table。但这并不准确，因为如果在SQL中,Table 之前相互独立，同名的字段在两个表中毫无关系。</p>
<p>但是在ES中，同一个Index 下不同的 Type 如果有同名的字段，他们会被 Luecence 当作同一个字段 ，并且他们的定义必须相同。所以我觉得Index现在更像一个表，</p>
<p>而Type字段并没有多少意义。目前Type已经被Deprecated，在7.0开始，一个索引只能建一个Type为<code>_doc</code></p>
</li>
</ul>
</li>
<li><p>2:document的概念</p>
<ul>
<li><code>Document</code> Index 里面单条的记录称为Document（文档）。<strong>等同于关系型数据库表中的行</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p>文档</p>
<ul>
<li>es是面向文档的，那么就意味着索引和收索数据的最小单位就是文档</li>
</ul>
</li>
<li><p>类型</p>
<ul>
<li><p>类型是文档的逻辑容器，就像关系型数据库一样，表格是行的容器。</p>
</li>
<li><p>类型中对于字段的定义称为映射，比如name映射为字符串类型</p>
</li>
<li><p><code>Type</code> 可以理解成关系数据库中Table。</p>
<p>之前的版本中，索引和文档中间还有个类型的概念，每个索引下可以建立多个类型，文档存储时需要指定index和type。从6.0.0开始单个索引中只能有一个类型，</p>
<p>7.0.0以后将将不建议使用，8.0.0 以后完全不支持。</p>
<h5 id="弃用该概念的原因：-1"><a href="#弃用该概念的原因：-1" class="headerlink" title="弃用该概念的原因："></a>弃用该概念的原因：</h5><p>我们虽然可以通俗的去理解Index比作 SQL 的 Database，Type比作SQL的Table。但这并不准确，因为如果在SQL中,Table 之前相互独立，同名的字段在两个表中毫无关系。</p>
<p>但是在ES中，同一个Index 下不同的 Type 如果有同名的字段，他们会被 Luecence 当作同一个字段 ，并且他们的定义必须相同。所以我觉得Index现在更像一个表，</p>
<p><strong>而Type字段并没有多少意义。目前Type已经被Deprecated，在7.0开始，一个索引只能建一个Type为<code>_doc</code></strong></p>
</li>
</ul>
</li>
<li><p>索引</p>
<ul>
<li>索引是映射类型的容器，es中的索引是一个非常大的文档集合。索引存储了映射类型的字段和其他设置。</li>
</ul>
</li>
<li><p>物理设计</p>
<ul>
<li>一个es集群至少有一个节点，一个节点就是一个es进程，节点可以有多个索引。如果创建索引，那么索引将会有5个分片（primary shard，又称主分片）构成的，每一个主分片会有一个副本(replica shard，又称复制分片)</li>
<li>一个分片是一个<code>lucene</code>索引，一个包含倒排索引的文件目录，倒排索引可以使es在不扫描全部文档的情况下，可以知道哪些文档包含特定的关键字。</li>
</ul>
</li>
<li><p>倒排索引</p>
<ul>
<li>elasticsearch 使用的是一种称为<code>倒排索引</code>的结构，采用<code>Lucene</code>倒排索引作为底层。这种结构适用于快速的全文搜索， 一个索引由文档中所有不重复的列表构成，对于每一个词，都有一个包含它的文档列表。</li>
</ul>
</li>
</ul>
<h2 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h2><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><ul>
<li><p>IK分词器</p>
<ul>
<li><p>分词：即把一段中文或者别的内容划分成一个个的关键字，我们在搜索时候会把自己的信息进行分词，是因为数据库中或者索引库中的数据也会进行分词，然后进行一个匹配操作</p>
</li>
<li><p><code>ik_max_word</code> 是<strong>细粒度分词</strong>，会穷尽一个语句中所有分词可能 <code>ik_smart</code> 是<strong>粗粒度分词</strong>，优先匹配最长词，不会有重复的数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">GET _analyze<br>&#123;<br>  &quot;analyzer&quot;:&quot;ik_smart&quot;,<br>  &quot;text&quot;: &quot;梦想家&quot;<br>&#125;<br><br>GET _analyze<br>&#123;<br>  &quot;analyzer&quot;:&quot;ik_max_word&quot;,<br>  &quot;text&quot;: &quot;梦想家&quot;<br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="Rest风格"><a href="#Rest风格" class="headerlink" title="Rest风格"></a>Rest风格</h3><ul>
<li><strong>一种软件架构风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制</strong>。</li>
<li>RESTFUL是一种网络应用程序的设计风格和开发方式，<strong>基于HTTP</strong>，<strong>可以使用 XML 格式定义或 JSON 格式定义</strong>。最常用的数据格式是JSON。由于JSON能直接被JavaScript读取，所以，<strong>使用JSON格式的REST风格的API具有简单、易读、易用的特点</strong>。</li>
</ul>
<h3 id="增删改查操作"><a href="#增删改查操作" class="headerlink" title="增删改查操作"></a>增删改查操作</h3><ul>
<li><p>创建一个索引</p>
<ul>
<li><p>PUT &#x2F;索引名&#x2F;类型名&#x2F;文档id {请求id}</p>
</li>
<li><pre><code class="shell"># 命令解释 
# PUT 创建命令 test1 索引 type1 类型 1 id
#type的概念已经被高版本弃用，不在使用，写成_doc即可
PUT /test1/type1/1
&#123;
  &quot;name&quot;:&quot;大数据梦想家&quot;,
  &quot;age&quot;:21
&#125;
# 也可以创建一个索引，_id会自动生成一个
POST /zt_test/_doc/
&#123;
  &quot;name&quot;:&quot;dcx&quot;,
  &quot;age&quot;:3208
&#125;
get /test1
#查看test1 的结构，我们没有指定文档字段类型，但是es会默认配置类型

get _cat/health
#查看集群的健康情况
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><br>- put创建索引，并设置字段类型<br><br>  ````shell<br>  PUT /test2<br>  &#123;<br>    <span class="hljs-string">&quot;mappings&quot;</span>: &#123;<br>      <span class="hljs-string">&quot;properties&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;name&quot;</span>:&#123;<br>          <span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;text&quot;</span><br>        &#125;,<br>        <span class="hljs-string">&quot;age&quot;</span>:&#123;<br>          <span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;long&quot;</span><br>        &#125;,<br>        <span class="hljs-string">&quot;birthday&quot;</span>:&#123;<br>          <span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;date&quot;</span><br>        &#125;<br>      &#125;<br>    &#125;<br>  &#125;<br>  <br>  PUT /test3/_doc/1<br>  &#123;<br>    <span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;大数据梦想家&quot;</span>,<br>    <span class="hljs-string">&quot;age&quot;</span>:21,<br>    <span class="hljs-string">&quot;birthday&quot;</span>:<span class="hljs-string">&quot;2000-02-06&quot;</span><br>  &#125;<br>  <br>  <span class="hljs-comment">#查看索引结构</span><br>  GET /索引名/_mapping?pretty<br>  GET /patent_index/_mapping?pretty<br>  <br>  <span class="hljs-comment">#根据查到的结构创建索引</span><br>  <span class="hljs-comment">#settings可以设置一些配置参数</span><br>  <span class="hljs-comment">#number_of_shards：分片数量（按机器节点数量计算，创建后不能更改）</span><br>  <span class="hljs-comment">#number_of_replicas：副本数量</span><br>  PUT comprehensive_suggest_predict_v3_zt<br>  &#123;<br>    <span class="hljs-string">&quot;settings&quot;</span>: &#123;<br>      <span class="hljs-string">&quot;number_of_shards&quot;</span>: 6,<br>      <span class="hljs-string">&quot;number_of_replicas&quot;</span>: 1<br>    &#125;, <br>      <span class="hljs-string">&quot;mappings&quot;</span> : &#123;<br>        <span class="hljs-string">&quot;properties&quot;</span> : &#123;<br>          <span class="hljs-string">&quot;title&quot;</span> : &#123;<br>            <span class="hljs-string">&quot;type&quot;</span> : <span class="hljs-string">&quot;completion&quot;</span>,<br>            <span class="hljs-string">&quot;analyzer&quot;</span> : <span class="hljs-string">&quot;standard&quot;</span>,<br>            <span class="hljs-string">&quot;preserve_separators&quot;</span> : <span class="hljs-literal">true</span>,<br>            <span class="hljs-string">&quot;preserve_position_increments&quot;</span> : <span class="hljs-literal">true</span>,<br>            <span class="hljs-string">&quot;max_input_length&quot;</span> : 50,<br>            <span class="hljs-string">&quot;contexts&quot;</span> : [<br>              &#123;<br>                <span class="hljs-string">&quot;name&quot;</span> : <span class="hljs-string">&quot;tag&quot;</span>,<br>                <span class="hljs-string">&quot;type&quot;</span> : <span class="hljs-string">&quot;CATEGORY&quot;</span><br>              &#125;<br>            ]<br>          &#125;<br>        &#125;<br>      &#125;<br>  &#125;<br>  <br>  <br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>更新数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">PUT /test3/_doc/1<br>&#123;<br>  &quot;name&quot;:&quot;大数据梦想家&quot;,<br>  &quot;age&quot;:21,<br>  &quot;birthday&quot;:&quot;2000-02-06&quot;<br>&#125;<br><span class="hljs-meta prompt_"># </span><span class="language-bash">这种put方式更新数据，如果遗漏了字段，那么数据就会被新数据覆盖，一般不用这种方式更新数据</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">可以用post的方式，update数据</span><br>post /test3/_doc/1/_update<br>&#123;<br>  &quot;doc&quot;:&#123;<br>    &quot;age&quot;:24<br>  &#125;<br>&#125;<br><br><br></code></pre></td></tr></table></figure>
</li>
<li><p>删除索引</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">delete语句删除索引</span><br>delete test3<br><span class="hljs-meta prompt_"># </span><span class="language-bash">指定<span class="hljs-built_in">id</span>删除</span><br>DELETE policy_project_library_index/_doc/1827261404195741698<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>查询数据</p>
<ul>
<li><p>先创建一个索引，并添加一些样例数据</p>
</li>
<li><p>全匹配查询：<code>GET book/_search</code></p>
<ul>
<li><pre><code class="shell"># 全匹配查询
GET book/_search
&#123;
  &quot;query&quot;: &#123;
    &quot;match_all&quot;: &#123;&#125;
  &#125;
&#125;
#size设置分页查询
GET book/_search
&#123;
  &quot;query&quot;: &#123;
    &quot;match_all&quot;: &#123;&#125;
  &#125;,
  &quot;size&quot;: 100
&#125;

<figure class="highlight nsis"><table><tr><td class="code"><pre><code class="hljs nsis"><br>- <span class="hljs-string">``</span><span class="hljs-string">``</span>shell<br>  PUT /alice/<span class="hljs-literal">user</span>/<span class="hljs-number">1</span><br>  &#123;<br>    <span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;爱丽丝&quot;</span>,<br>    <span class="hljs-string">&quot;age&quot;</span>:<span class="hljs-number">21</span>,<br>    <span class="hljs-string">&quot;desc&quot;</span>:<span class="hljs-string">&quot;在最美的年华，做最好的自己！&quot;</span>,<br>    <span class="hljs-string">&quot;tags&quot;</span>:[<span class="hljs-string">&quot;技术宅&quot;</span>,<span class="hljs-string">&quot;温暖&quot;</span>,<span class="hljs-string">&quot;思维活跃&quot;</span>]<br>  &#125;<br>  <br>  PUT /alice/<span class="hljs-literal">user</span>/<span class="hljs-number">2</span><br>  &#123;<br>    <span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;张三&quot;</span>,<br>    <span class="hljs-string">&quot;age&quot;</span>:<span class="hljs-number">23</span>,<br>    <span class="hljs-string">&quot;desc&quot;</span>:<span class="hljs-string">&quot;法外狂徒&quot;</span>,<br>    <span class="hljs-string">&quot;tags&quot;</span>:[<span class="hljs-string">&quot;渣男&quot;</span>,<span class="hljs-string">&quot;交友&quot;</span>]<br>  &#125;<br>  <br>  PUT /alice/<span class="hljs-literal">user</span>/<span class="hljs-number">3</span><br>  &#123;<br>    <span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;路人甲&quot;</span>,<br>    <span class="hljs-string">&quot;age&quot;</span>:<span class="hljs-number">24</span>,<br>    <span class="hljs-string">&quot;desc&quot;</span>:<span class="hljs-string">&quot;不可描述&quot;</span>,<br>    <span class="hljs-string">&quot;tags&quot;</span>:[<span class="hljs-string">&quot;靓仔&quot;</span>,<span class="hljs-string">&quot;网游&quot;</span>]<br>  &#125;<br>  <br>  <br>  PUT /alice/<span class="hljs-literal">user</span>/<span class="hljs-number">4</span><br>  &#123;<br>    <span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;爱丽丝学Java&quot;</span>,<br>    <span class="hljs-string">&quot;age&quot;</span>:<span class="hljs-number">25</span>,<br>    <span class="hljs-string">&quot;desc&quot;</span>:<span class="hljs-string">&quot;技术成就自我！&quot;</span>,<br>    <span class="hljs-string">&quot;tags&quot;</span>:[<span class="hljs-string">&quot;思维敏捷&quot;</span>,<span class="hljs-string">&quot;喜欢学习&quot;</span>]<br>  &#125;<br>  <br>  PUT /alice/<span class="hljs-literal">user</span>/<span class="hljs-number">5</span><br>  &#123;<br>    <span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;爱丽丝学Python&quot;</span>,<br>    <span class="hljs-string">&quot;age&quot;</span>:<span class="hljs-number">26</span>,<br>    <span class="hljs-string">&quot;desc&quot;</span>:<span class="hljs-string">&quot;人生苦短，我用Python！&quot;</span>,<br>    <span class="hljs-string">&quot;tags&quot;</span>:[<span class="hljs-string">&quot;好学&quot;</span>,<span class="hljs-string">&quot;勤奋刻苦&quot;</span>]<br>  &#125;<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li></li>
<li><pre><code class="shell"># 搜索到指定 id 的文档信息
get alice/user/1
# 条件查询_search?q=
# 一般不用这种方式
GET alice/user/_search?q=name:爱丽丝

# 查询
get alice/user/_search
&#123;
  &quot;query&quot;:&#123;
    &quot;match&quot;: &#123;
      &quot;name&quot;: &quot;张三&quot;
    &#125;
  &#125;
&#125;
#展示想要的字段，指定字段即可
get alice/user/_search
&#123;
  &quot;query&quot;:&#123;
    &quot;match&quot;: &#123;
      &quot;name&quot;: &quot;张三&quot;
    &#125;
  &#125;,
  &quot;_source&quot;:[&quot;name&quot;,&quot;age&quot;]
&#125;
<figure class="highlight nsis"><table><tr><td class="code"><pre><code class="hljs nsis"><br>- 排序查询<br><br>  <span class="hljs-string">``</span><span class="hljs-string">``</span>shell<br>  <span class="hljs-comment"># 排序查询，可以指定from(开始位置)，size(几条数据)</span><br>  GET alice/<span class="hljs-literal">user</span>/_search<br>  &#123;<br>    <span class="hljs-string">&quot;query&quot;</span>:&#123;<br>      <span class="hljs-string">&quot;match&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;爱丽丝&quot;</span><br>      &#125;<br>    &#125;,<br>    <span class="hljs-string">&quot;sort&quot;</span>: [<br>      &#123; <br>        <span class="hljs-string">&quot;age&quot;</span>: &#123; <br>          <span class="hljs-string">&quot;order&quot;</span>: <span class="hljs-string">&quot;asc&quot;</span><br>        &#125;<br>       &#125;<br>     ],<br>     <span class="hljs-string">&quot;from&quot;</span>:<span class="hljs-number">0</span>,<br>     <span class="hljs-string">&quot;size&quot;</span>: <span class="hljs-number">2</span><br>  &#125;<br></code></pre></td></tr></table></figure>

- 排序支持的属性
  - 数字
  - 日期
  - ID
</code></pre>
</li>
<li><p>分页查询</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">学到这里，我们也可以看到，我们的查询条件越来越多，开始仅是简单查询，慢慢增加条件查询，增加排序，对返回结果进行限制。所以，我们可以说，对 于 elasticsearch 来说，所有的查询条件都是可插拔的。比如说，我们在查询中，仅对返回结果进行限制:</span><br><br>GET alice/user/_search<br>&#123; <br>  &quot;query&quot;:<br>  &#123;&quot;match_all&quot;: &#123;&#125;<br>  &#125;,<br>  &quot;from&quot;:0,  # 从第n条开始<br>  &quot;size&quot;:4   # 返回n条数据<br>  &#125;<br></code></pre></td></tr></table></figure>
</li>
<li><p>布尔查询</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">多条件查询</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">通过bool属性内使用must来作为查询条件</span><br>GET alice/user/_search<br>&#123;<br>  &quot;query&quot;:&#123;<br>    &quot;bool&quot;: &#123;<br>      &quot;must&quot;:[<br>         &#123;<br>          &quot;match&quot;:&#123;<br>          &quot;name&quot;:&quot;爱丽丝&quot;<br>         &#125;<br>      &#125;,<br>      &#123;<br>        &quot;match&quot;:&#123;<br>          &quot;age&quot;:25<br>          &#125;<br>        &#125;<br>      ]<br>    &#125;<br>  &#125;<br>&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">查询条件1或条件2</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">should</span><br>GET alice/user/_search<br>&#123;<br>  &quot;query&quot;:&#123;<br>    &quot;bool&quot;: &#123;<br>      &quot;should&quot;:[<br>         &#123;<br>          &quot;match&quot;:&#123;<br>          &quot;name&quot;:&quot;爱丽丝&quot;<br>         &#125;<br>      &#125;,<br>      &#123;<br>        &quot;match&quot;:&#123;<br>          &quot;age&quot;:25<br>          &#125;<br>        &#125;<br>      ]<br>    &#125;<br>  &#125;<br>&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">查询不是的条件：must_not</span><br>GET alice/user/_search<br>&#123;<br>  &quot;query&quot;:&#123;<br>    &quot;bool&quot;: &#123;<br>      &quot;must_not&quot;:[<br>      &#123;<br>        &quot;match&quot;:&#123;<br>          &quot;age&quot;:25<br>          &#125;<br>        &#125;<br>      ]<br>    &#125;<br>  &#125;<br>&#125;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">exists关键字存在</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">存在age字段的数据</span><br>GET alice/user/_search<br>&#123;<br>  &quot;query&quot;:&#123;<br>    &quot;bool&quot;: &#123;<br>      &quot;must_not&quot;:[<br>      &#123;<br>        &quot;exists&quot;:&#123;<br>          &quot;field&quot;:&quot;age&quot;<br>          &#125;<br>        &#125;<br>      ]<br>    &#125;<br>  &#125;<br>&#125;<br><br><br></code></pre></td></tr></table></figure>
</li>
<li><p>filter过滤</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">filter进行数据过滤</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">查询名称为爱丽丝，年龄&gt;=10and年龄&lt;=25</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">range查询</span><br>get alice/user/_search<br>&#123;<br>  &quot;query&quot;:&#123;<br>    &quot;bool&quot;:&#123;<br>      &quot;must&quot;:[<br>        &#123;<br>          &quot;match&quot;:&#123;<br>            &quot;name&quot;:&quot;爱丽丝&quot;<br>          &#125;<br>        &#125;<br>        ],<br>      &quot;filter&quot;: [<br>        &#123;&quot;range&quot;: &#123;<br>          &quot;age&quot;: &#123;<br>            &quot;gte&quot;: 10,<br>            &quot;lte&quot;: 25<br>          &#125;<br>        &#125;&#125;<br>      ]<br>    &#125;<br>  &#125;<br>&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">gt表示大于</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">gte表示大于等于</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">lt表示小于</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">lte表示小于等于</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>短语检索</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">查询tags中包含<span class="hljs-string">&quot;男&quot;</span>的数据</span><br>GET alice/user/_search<br>&#123;<br>  &quot;query&quot;:&#123;<br>    &quot;match&quot;:&#123;<br>      &quot;tags&quot;:&quot;男&quot;<br>    &#125;<br>  &#125;<br>&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">匹配多个标签</span><br>GET alice/user/_search<br>&#123;<br>  &quot;query&quot;:&#123;<br>    &quot;match&quot;:&#123;<br>      &quot;tags&quot;:&quot;男 女&quot;<br>    &#125;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
</li>
<li><p>精确查询</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">term查询是直接通过倒排索引指定的词条进程精确查找的</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">term,不经过分词，直接查询精确的值</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">match,会使用分词器解析(先分析文档，然后再通过分析的文档进行查询)</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">创建一个索引，并指定类型</span><br>PUT testdb<br>&#123;<br>  &quot;mappings&quot;: &#123;<br>    &quot;properties&quot;: &#123;<br>    <br>      &quot;name&quot;:&#123;<br>        &quot;type&quot;: &quot;text&quot;<br>      &#125;,<br>    &quot;desc&quot;:&#123;<br>      &quot;type&quot;:&quot;keyword&quot;<br>     &#125;<br>    &#125;<br>  &#125;<br>&#125;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">插入数据</span><br>PUT testdb/_doc/1<br>&#123;<br>  &quot;name&quot;:&quot;爱丽丝学大数据name&quot;,<br>  &quot;desc&quot;:&quot;爱丽丝学大数据desc&quot;<br>&#125;<br><br>PUT testdb/_doc/2<br>&#123;<br>  &quot;name&quot;:&quot;爱丽丝学大数据name2&quot;,<br>  &quot;desc&quot;:&quot;爱丽丝学大数据desc2&quot;<br>&#125;<br><span class="hljs-meta prompt_"># </span><span class="language-bash">keyword类型不会被分析器处理</span><br>GET _analyze<br>&#123;<br>  &quot;analyzer&quot;: &quot;keyword&quot;,<br>  &quot;text&quot;: &quot;爱丽丝学大数据 name&quot;<br>&#125;<br><span class="hljs-meta prompt_"># </span><span class="language-bash">text 会被分析器分析 查询</span><br>GET _analyze<br>&#123;<br>  &quot;analyzer&quot;: &quot;standard&quot;,<br>  &quot;text&quot;: &quot;爱丽丝学大数据 name&quot;<br>&#125;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">text 会被分析器分析 查询</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">通过查询可以验证</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">name是text类型，会被分析器分析，可以查出两条数据</span><br>GET testdb/_search         <br>&#123;<br>  &quot;query&quot;: &#123;<br>    &quot;term&quot;: &#123;<br>        &quot;name&quot;: &quot;爱&quot;<br>      &#125;<br>    &#125;<br>&#125;<br><span class="hljs-meta prompt_"># </span><span class="language-bash">keyword 不会被分析所以直接查询</span> <br><span class="hljs-meta prompt_">#</span><span class="language-bash">desc是keyword类型,不会被分析器分析，要完全匹配才可查出数据</span><br>GET testdb/_search          <br>&#123;<br>  &quot;query&quot;: &#123;<br>    &quot;term&quot;: &#123;<br>        &quot;desc&quot;: &quot;爱丽丝学大数据desc&quot;<br>      &#125;<br>    &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure>

<ul>
<li>分词解析器<ul>
<li><strong>keyword 字段类型不会被分析器分析</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>查找多个精确值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">插入一些测试数据</span><br>PUT testdb/_doc/3<br>&#123;<br>  &quot;t1&quot;:&quot;22&quot;,<br>  &quot;t2&quot;:&quot;2021-03-01&quot;<br>&#125;<br><br>PUT testdb/_doc/4<br>&#123;<br>  &quot;t1&quot;:&quot;33&quot;,<br>  &quot;t2&quot;:&quot;2021-03-01&quot;<br>&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">查询t1=22 or t1=23的数据</span><br>GET testdb/_search<br>&#123;<br>  &quot;query&quot;: &#123;<br>    &quot;bool&quot;:&#123;<br>      &quot;should&quot;: [<br>        &#123;<br>          &quot;term&quot;: &#123;<br>            &quot;t1&quot;:&quot;22&quot;<br>          &#125;<br>        &#125;,<br>        &#123;<br>          &quot;term&quot;: &#123;<br>            &quot;t1&quot;:&quot;33&quot;<br>          &#125;<br>        &#125;<br>      ]<br>    &#125;<br>  &#125;<br>&#125;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">可以证明：就算是term精确查询，也能够查询多个值。</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">也可以直接用这种替代</span><br>GET testdb/_doc/_search<br>&#123;<br>  &quot;query&quot;:&#123;<br>    &quot;terms&quot;:&#123;<br>      &quot;t1&quot;:[&quot;22&quot;,&quot;33&quot;]<br>    &#125;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>高亮显示</p>
<ul>
<li><p>设置highlight属性，对查询的结果的指定字段做高亮显示</p>
</li>
<li><pre><code class="shell">#观察返回的结果，我们可以发现搜索相关的结果，被加上了高亮标签&lt;em&gt;
GET alice/user/_search
&#123;
  &quot;query&quot;:&#123;
     &quot;match&quot;: &#123;
       &quot;name&quot;: &quot;爱丽丝&quot;
     &#125;
  &#125;,
  &quot;highlight&quot;:&#123;
    &quot;fields&quot;: &#123;
      &quot;name&quot;: &#123;&#125;
    &#125;
  &#125;
&#125;

#自定义样式
GET alice/user/_search
&#123;
  &quot;query&quot;:&#123;
     &quot;match&quot;: &#123;
       &quot;name&quot;: &quot;爱丽丝&quot;
     &#125;
  &#125;,
  &quot;highlight&quot;:&#123;
    &quot;pre_tags&quot;: &quot;&lt;b class=&#39;key&#39; style=&#39;color:red&#39;&gt;&quot;, 
    &quot;post_tags&quot;: &quot;&lt;/b&gt;&quot;,
    &quot;fields&quot;: &#123;
      &quot;name&quot;: &#123;&#125;
    &#125;
  &#125;
&#125;
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><br><span class="hljs-comment">#### 字段类型</span><br><br>* 字符串类型<br><br>  - **text,keyword**<br><br>* 数值类型<br><br>  - **long,<span class="hljs-built_in">integer</span>,short,byte,double,<span class="hljs-built_in">float</span>,half_float,scaled_float**<br><br>* 日期类型<br><br>  - **<span class="hljs-built_in">date</span>**<br><br>  - ````shell<br>    PUT my_index<br>    &#123;<br>      <span class="hljs-string">&quot;mappings&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;_doc&quot;</span>: &#123;<br>          <span class="hljs-string">&quot;properties&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;date&quot;</span>: &#123;<br>              <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;date&quot;</span> <br>            &#125;<br>          &#125;<br>        &#125;<br>      &#125;<br>    &#125;<br>    <br>    PUT my_index/_doc/1<br>    &#123; <span class="hljs-string">&quot;date&quot;</span>: <span class="hljs-string">&quot;2015-01-01&quot;</span> &#125; <br>    <br>    PUT my_index/_doc/2<br>    &#123; <span class="hljs-string">&quot;date&quot;</span>: <span class="hljs-string">&quot;2015-01-01T12:10:30Z&quot;</span> &#125; <br>    <br>    PUT my_index/_doc/3<br>    &#123; <span class="hljs-string">&quot;date&quot;</span>: 1420070400001 &#125; <br>    <br>    GET my_index/_search<br>    &#123;<br>      <span class="hljs-string">&quot;sort&quot;</span>: &#123; <span class="hljs-string">&quot;date&quot;</span>: <span class="hljs-string">&quot;asc&quot;</span>&#125; <br>    &#125;<br>    <span class="hljs-comment">#es的date类型支持多种格式，上面的添加数据都可以</span><br>    <span class="hljs-comment">#也可以指定格式</span><br>    PUT my_index<br>    &#123;<br>      <span class="hljs-string">&quot;mappings&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;_doc&quot;</span>: &#123;<br>          <span class="hljs-string">&quot;properties&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;date&quot;</span>: &#123;<br>              <span class="hljs-string">&quot;type&quot;</span>:   <span class="hljs-string">&quot;date&quot;</span>,<br>              <span class="hljs-string">&quot;format&quot;</span>: <span class="hljs-string">&quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot;</span><br>            &#125;<br>          &#125;<br>        &#125;<br>      &#125;<br>    &#125;<br>    <span class="hljs-comment">#规定格式后，新增的数据必须符合这个格式，不然就会跑错</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>可用的格式</p>
<ul>
<li>yyyy-MM-dd HH:mm:ss</li>
<li>yyyy-MM-dd</li>
<li>epoch_millis (毫秒值)</li>
</ul>
</li>
</ul>
</li>
<li><p>布尔值类型</p>
<ul>
<li><strong>boolean</strong></li>
</ul>
</li>
<li><p>二进制类型</p>
<ul>
<li><strong>binary</strong></li>
</ul>
</li>
<li><p>复杂类型</p>
<ul>
<li><p>Array</p>
<ul>
<li>在Elasticsearch中，<strong>数组不需要专用的字段数据类型</strong>。默认情况下，<strong>任何字段都可以包含零个或多个值</strong>，但是，数组中的所有值都<strong>必须具有相同的数据类型。</strong></li>
</ul>
</li>
<li><p>object</p>
<ul>
<li><p><strong>object类型的字段，也可以有多个值</strong>，形成List<object>的数据结构。</p>
</li>
<li><p><strong>object不允许彼此独立地索引查询</strong></p>
</li>
<li><pre><code class="shell"># 添加 属性为object的字段 field3
PUT toherotest/_mapping/_doc 
&#123;
  &quot;properties&quot;: &#123;
    &quot;field3&quot;: &#123;
      &quot;type&quot;: &quot;object&quot;
    &#125;
  &#125;
&#125;
# 新增数据
POST /toherotest/_doc/3
&#123;
  &quot;field3&quot;:[ &#123; &quot;name&quot;:&quot;tohero1&quot;, &quot;age&quot;:1 &#125;, &#123; &quot;name&quot;:&quot;tohero2&quot;, &quot;age&quot;:2 &#125; ]
&#125;
  
POST /toherotest/_doc/4
&#123;
  &quot;field3&quot;: [ &#123; &quot;name&quot;:&quot;tohero1&quot;, &quot;age&quot;:2 &#125;, &#123; &quot;name&quot;:&quot;tohero2&quot;, &quot;age&quot;:1 &#125; ]
&#125;

#执行查询语句 
GET /toherotest/_doc/_search
&#123;
  &quot;query&quot;: &#123;
    &quot;bool&quot;: &#123;
      &quot;must&quot;: [
        &#123;
          &quot;term&quot;: &#123;
            &quot;field3.name&quot;: &quot;tohero1&quot;
          &#125;
        &#125;,
        &#123;
          &quot;term&quot;: &#123;
            &quot;field3.age&quot;: 1
          &#125;
        &#125;
      ]
    &#125;
  &#125;
&#125;

#查询结果
&#123;
  &quot;took&quot;: 2,
  &quot;timed_out&quot;: false,
  &quot;_shards&quot;: &#123;
    &quot;total&quot;: 5,
    &quot;successful&quot;: 5,
    &quot;skipped&quot;: 0,
    &quot;failed&quot;: 0
  &#125;,
  &quot;hits&quot;: &#123;
    &quot;total&quot;: 2,
    &quot;max_score&quot;: 1.287682,
    &quot;hits&quot;: [
      &#123;
        &quot;_index&quot;: &quot;toherotest&quot;,
        &quot;_type&quot;: &quot;_doc&quot;,
        &quot;_id&quot;: &quot;4&quot;,
        &quot;_score&quot;: 1.287682,
        &quot;_source&quot;: &#123;
          &quot;field3&quot;: [
            &#123;
              &quot;name&quot;: &quot;tohero1&quot;,
              &quot;age&quot;: 2
            &#125;,
            &#123;
              &quot;name&quot;: &quot;tohero2&quot;,
              &quot;age&quot;: 1
            &#125;
          ]
        &#125;
      &#125;,
      &#123;
        &quot;_index&quot;: &quot;toherotest&quot;,
        &quot;_type&quot;: &quot;_doc&quot;,
        &quot;_id&quot;: &quot;3&quot;,
        &quot;_score&quot;: 1.287682,
        &quot;_source&quot;: &#123;
          &quot;field3&quot;: [
            &#123;
              &quot;name&quot;: &quot;tohero1&quot;,
              &quot;age&quot;: 1
            &#125;,
            &#123;
              &quot;name&quot;: &quot;tohero2&quot;,
              &quot;age&quot;: 2
            &#125;
          ]
        &#125;
      &#125;
    ]
  &#125;
&#125;

#可以看到，两条数据都出来了
#得出结论：object不允许彼此独立地索引查询
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk"><br>    - 如果希望object能被独立的索引，可以考虑nested类型<br><br>  - nested<br><br>* GEO地理位置类型<br><br>  - 地图：Geo-point   （web开发常用地图类型）<br><br>  - 形状：Geo-shape<br><br>  - ````shell<br>    <span class="hljs-comment">#案例</span><br>    PUT my_index<br>    &#123;<br>      <span class="hljs-string">&quot;mappings&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;_doc&quot;</span>: &#123;<br>          <span class="hljs-string">&quot;properties&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;location&quot;</span>: &#123;<br>              <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;geo_point&quot;</span><br>            &#125;<br>          &#125;<br>        &#125;<br>      &#125;<br>    &#125;<br>    <br>    PUT my_index<span class="hljs-regexp">/_doc/</span><span class="hljs-number">1</span><br>    &#123;<br>      <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Geo-point as an object&quot;</span>,<br>      <span class="hljs-string">&quot;location&quot;</span>: &#123; <br>        <span class="hljs-string">&quot;lat&quot;</span>: <span class="hljs-number">41.12</span>,<br>        <span class="hljs-string">&quot;lon&quot;</span>: -<span class="hljs-number">71.34</span><br>      &#125;<br>    &#125;<br>    <br>    PUT my_index<span class="hljs-regexp">/_doc/</span><span class="hljs-number">2</span><br>    &#123;<br>      <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Geo-point as a string&quot;</span>,<br>      <span class="hljs-string">&quot;location&quot;</span>: <span class="hljs-string">&quot;41.12,-71.34&quot;</span> <br>    &#125;<br>    <br>    PUT my_index<span class="hljs-regexp">/_doc/</span><span class="hljs-number">3</span><br>    &#123;<br>      <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Geo-point as a geohash&quot;</span>,<br>      <span class="hljs-string">&quot;location&quot;</span>: <span class="hljs-string">&quot;drm3btev3e86&quot;</span> <br>    &#125;<br>    <br>    PUT my_index<span class="hljs-regexp">/_doc/</span><span class="hljs-number">4</span><br>    &#123;<br>      <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Geo-point as an array&quot;</span>,<br>      <span class="hljs-string">&quot;location&quot;</span>: [ -<span class="hljs-number">71.34</span>, <span class="hljs-number">41.12</span> ] <br>    &#125;<br>    <br>    <span class="hljs-comment">#查询某个点方圆200km</span><br>    GET <span class="hljs-regexp">/my_locations/</span>_search<br>    &#123;<br>        <span class="hljs-string">&quot;query&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;bool&quot;</span> : &#123;<br>                <span class="hljs-string">&quot;must&quot;</span> : &#123;<br>                    <span class="hljs-string">&quot;match_all&quot;</span> : &#123;&#125;<br>                &#125;,<br>                <span class="hljs-string">&quot;filter&quot;</span> : &#123;<br>                    <span class="hljs-string">&quot;geo_distance&quot;</span> : &#123;<br>                        <span class="hljs-string">&quot;distance&quot;</span> : <span class="hljs-string">&quot;200km&quot;</span>,<br>                        <span class="hljs-string">&quot;pin.location&quot;</span> : &#123;<br>                            <span class="hljs-string">&quot;lat&quot;</span> : <span class="hljs-number">40</span>,<br>                            <span class="hljs-string">&quot;lon&quot;</span> : -<span class="hljs-number">70</span><br>                        &#125;<br>                    &#125;<br>                &#125;<br>            &#125;<br>        &#125;<br>    &#125;<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li></li>
</ul>
</li>
<li><p>还有一些其他的类型参考<a href="https://zhuanlan.zhihu.com/p/136981393">链接</a></p>
</li>
</ul>
<h3 id="创建别名"><a href="#创建别名" class="headerlink" title="创建别名"></a>创建别名</h3><ul>
<li><p><strong>创建别名有很多种方法，可以创建索引同时创建别名，也可以在创建索引后创建，既可以让别名指向多个索引，也可以让别名指向一个索引的部分数据，甚至指向一个字段</strong></p>
</li>
<li><p>合理的使用别名可以方便进行索引的更新切换</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">创建一个索引进行测试</span><br>PUT /zt_user/_doc/1<br>&#123;<br>  &quot;birth-year&quot;:2000<br>&#125;<br><br><br>PUT /zt_user/_doc/2<br>&#123;<br>  &quot;birth-year&quot;:2001<br>&#125;<br><br>PUT /zt_user/_doc/3<br>&#123;<br>  &quot;birth-year&quot;:2004<br>&#125;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">zt_user2</span><br>PUT /zt_user2/_doc/1<br>&#123;<br>  &quot;birth-year&quot;:2000<br>&#125;<br></code></pre></td></tr></table></figure>


</li>
<li><p><strong>创建索引同时创建别名</strong></p>
<ul>
<li><pre><code class="shell">
#创建一个索引，并同时创建别名
#一个名叫myusers的索引，直接指向zt_user
#一个名叫2000的索引，过滤birth-year=2000的数据
PUT /zt_user
&#123;
  &quot;mappings&quot;: &#123;
    &quot;properties&quot;: &#123;
      &quot;birth-year&quot;:&#123;
        &quot;type&quot;: &quot;integer&quot;
      &#125;
    &#125;
  &#125;,
  &quot;aliases&quot;: &#123;
    &quot;myusers&quot;: &#123;&#125;,
    &quot;2000&quot;:&#123;
      &quot;filter&quot;: &#123;
        &quot;term&quot;: &#123;
          &quot;birth-year&quot;: 2000
        &#125;
      &#125;
    &#125;
  &#125;
&#125;
<figure class="highlight ruby"><table><tr><td class="code"><pre><code class="hljs ruby"><br>- **创建索引后创建别名**<br><br>  - <span class="hljs-string">``</span><span class="hljs-string">``</span>shell<br>    <span class="hljs-comment"># 创建一个名为2004的别名，过滤birth-year=2004的数据</span><br>    <span class="hljs-variable constant_">PUT</span> /zt_user/_alias/<span class="hljs-number">2004</span> <br>    &#123;<br>      <span class="hljs-string">&quot;filter&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;term&quot;</span>: &#123;<br>          <span class="hljs-string">&quot;birth-year&quot;</span>: <span class="hljs-number">2004</span><br>        &#125;<br>      &#125;<br>    &#125;<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p><strong>创建字段别名</strong></p>
<ul>
<li><pre><code class="shell"># 字段设置别名，例如为username设置一个别名为name
PUT /zt_user1
&#123;
  &quot;mappings&quot;: &#123;
    &quot;properties&quot;: &#123;
      &quot;username&quot;:&#123;
        &quot;type&quot;: &quot;keyword&quot;
      &#125;,
      &quot;name&quot;:&#123;
        &quot;type&quot;:&quot;alias&quot;,
        &quot;path&quot;:&quot;username&quot;
    &#125;
    &#125;
  &#125;
&#125;
<figure class="highlight clean"><table><tr><td class="code"><pre><code class="hljs clean"><br>### 修改删除别名<br><br>* **为同一索引创建多个别名**<br><br>  - ````shell<br>    # 创建别名<br>    POST /_aliases<br>    &#123;<br>      <span class="hljs-string">&quot;actions&quot;</span>: [<br>        &#123;<br>          <span class="hljs-string">&quot;add&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;index&quot;</span>: <span class="hljs-string">&quot;zt_user&quot;</span>,<br>            <span class="hljs-string">&quot;alias&quot;</span>: <span class="hljs-string">&quot;zt_user_copy1&quot;</span><br>          &#125;<br>        &#125;,<br>        &#123;<br>          <span class="hljs-string">&quot;add&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;index&quot;</span>: <span class="hljs-string">&quot;zt_user&quot;</span>,<br>            <span class="hljs-string">&quot;alias&quot;</span>: <span class="hljs-string">&quot;zt_user_copy2&quot;</span><br>          &#125;<br>        &#125;<br>      ]<br>    &#125;<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li></li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>删除一个别名</strong></p>
<ul>
<li><pre><code class="shell">#删除一个别名
POST /_aliases
&#123;
  &quot;actions&quot;: [
    &#123;
      &quot;remove&quot;: &#123;
        &quot;index&quot;: &quot;zt_user&quot;,
        &quot;alias&quot;: &quot;zt_user_copy2&quot;
      &#125;
    &#125;
  ]
&#125;
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><code class="hljs dockerfile"><br>* **重命名一个别名**<br><br>  - 重命名操作，先remove再执行<span class="hljs-keyword">add</span><span class="language-bash">，而且这一操作是原子操作</span><br><br>  - ````<span class="hljs-keyword">shell</span><span class="language-bash"></span><br><span class="language-bash">    POST /_aliases</span><br>    &#123;<br>      <span class="hljs-string">&quot;actions&quot;</span>: [<br>        &#123;<br>          <span class="hljs-string">&quot;remove&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;index&quot;</span>: <span class="hljs-string">&quot;zt_user&quot;</span>,<br>            <span class="hljs-string">&quot;alias&quot;</span>: <span class="hljs-string">&quot;zt_user_copy1&quot;</span><br>          &#125;<br>        &#125;,<br>        &#123;<br>          <span class="hljs-string">&quot;add&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;index&quot;</span>: <span class="hljs-string">&quot;zt_user&quot;</span>,<br>            <span class="hljs-string">&quot;alias&quot;</span>: <span class="hljs-string">&quot;zt_user_copy2&quot;</span><br>          &#125;<br>        &#125;<br>      ]<br>    &#125;<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p><strong>创建一个别名指向多个索引</strong></p>
<ul>
<li><p><code>indices</code>:index的复数</p>
</li>
<li><pre><code class="shell">POST /_aliases
&#123;
  &quot;actions&quot;: [
    &#123;
      &quot;add&quot;: &#123;
        &quot;indices&quot;: [&quot;zt_user&quot;,&quot;zt_user2&quot;],
        &quot;alias&quot;: &quot;zt_user_copy&quot;
      &#125;
    &#125;
  ]
&#125;
<figure class="highlight prolog"><table><tr><td class="code"><pre><code class="hljs prolog"><br>  - <br><br>* **创建有过滤条件的别名**<br><br>  - <span class="hljs-string">``</span><span class="hljs-string">``</span>shell<br>    <span class="hljs-symbol">POST</span> /<span class="hljs-symbol">_aliases</span><br>    &#123;<br>      <span class="hljs-string">&quot;actions&quot;</span>: [<br>        &#123;<br>          <span class="hljs-string">&quot;add&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;index&quot;</span>: <span class="hljs-string">&quot;zt_user&quot;</span>,<br>            <span class="hljs-string">&quot;alias&quot;</span>: <span class="hljs-string">&quot;zt_user_copy&quot;</span>,<br>            <span class="hljs-string">&quot;filter&quot;</span>: &#123;<span class="hljs-string">&quot;term&quot;</span>: &#123; <span class="hljs-string">&quot;birth-year&quot;</span>:<span class="hljs-number">2000</span> &#125;&#125;<br>          &#125;<br>        &#125;<br>      ]<br>    &#125;<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p><strong>创建可写入别名</strong></p>
<ul>
<li><p>如果一个别名单独指向一个索引，那么使用别名进行写入操作是不会有问题的，但是，一个别名指向多个索引时，是不能进行写入操作的，因为es不知道将文档写入到哪一个索引。<strong>可以在创建别名时设置为可写来解决问题</strong></p>
</li>
<li><p>尽量不要这么弄吧，感觉很乱qaq…</p>
</li>
<li><pre><code class="shell">POST /_aliases
&#123;
  &quot;actions&quot;: [
    &#123;
      &quot;add&quot;: &#123;
        &quot;index&quot;: &quot;zt_user&quot;,
        &quot;alias&quot;: &quot;zt_user_copy&quot;,
        &quot;is_write_index&quot;:true
      &#125;
    &#125;,
    &#123;
      &quot;add&quot;: &#123;
        &quot;index&quot;: &quot;zt_user2&quot;,
        &quot;alias&quot;: &quot;zt_user_copy&quot;
      &#125;
    &#125;
  ]
&#125;
<figure class="highlight ruby"><table><tr><td class="code"><pre><code class="hljs ruby"><br>  - <br><br><span class="hljs-comment">### 查看别名信息</span><br><br>* **查看别名是否存在**<br>  - <span class="hljs-string">``</span><span class="hljs-variable constant_">HEAD</span> /_alias/myusers<span class="hljs-string">``</span><br>* **查看别名信息**<br>  - <span class="hljs-string">``</span><span class="hljs-variable constant_">GET</span> /_cat/aliases/myusers<span class="hljs-string">``</span> 查看指定别名<br>  - <span class="hljs-string">``</span><span class="hljs-variable constant_">GET</span> /_cat/aliases<span class="hljs-string">``</span>查看集群中所有别名<br><br><span class="hljs-comment">### 聚合（Aggregations）</span><br><br>* 聚合框架有助于基于搜索查询提供聚合数据。它基于称为聚合的简单构建块，可以进行组合以构建复杂的数据摘要。<br><br>* 列一些比较常见的聚合查询<br><br>  <span class="hljs-string">``</span><span class="hljs-string">``</span>shell<br>  <span class="hljs-comment"># 过滤数据一部分数据</span><br>  <span class="hljs-comment"># 利用聚合aggs:求project_money的平均值</span><br>  <span class="hljs-variable constant_">POST</span> /policy_enterprise_detail_all/_search<br>  &#123;<br>    <span class="hljs-string">&quot;size&quot;</span>: <span class="hljs-number">0</span>,<br>    <span class="hljs-string">&quot;query&quot;</span>: &#123;<br>      <span class="hljs-string">&quot;bool&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;must&quot;</span>: [<br>          &#123;<span class="hljs-string">&quot;match&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;city_id&quot;</span>: <span class="hljs-string">&quot;4401&quot;</span><br>          &#125;&#125;<br>        ]<br>      &#125;<br>    &#125;, <br>    <span class="hljs-string">&quot;aggs&quot;</span>: &#123;<br>      <span class="hljs-string">&quot;avg_money&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;avg&quot;</span>: &#123;<span class="hljs-string">&quot;field&quot;</span>: <span class="hljs-string">&quot;project_money&quot;</span>&#125;<br>      &#125;<br>    &#125;<br>  &#125;<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>地理边界聚合</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">PUT /museums<br>&#123;<br>    &quot;mappings&quot;: &#123;<br>        &quot;properties&quot;: &#123;<br>            &quot;location&quot;: &#123;<br>                &quot;type&quot;: &quot;geo_point&quot;<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br><br>POST /museums/_bulk?refresh<br>&#123;&quot;index&quot;:&#123;&quot;_id&quot;:1&#125;&#125;<br>&#123;&quot;location&quot;: &quot;52.374081,4.912350&quot;, &quot;name&quot;: &quot;NEMO Science Museum&quot;&#125;<br>&#123;&quot;index&quot;:&#123;&quot;_id&quot;:2&#125;&#125;<br>&#123;&quot;location&quot;: &quot;52.369219,4.901618&quot;, &quot;name&quot;: &quot;Museum Het Rembrandthuis&quot;&#125;<br>&#123;&quot;index&quot;:&#123;&quot;_id&quot;:3&#125;&#125;<br>&#123;&quot;location&quot;: &quot;52.371667,4.914722&quot;, &quot;name&quot;: &quot;Nederlands Scheepvaartmuseum&quot;&#125;<br>&#123;&quot;index&quot;:&#123;&quot;_id&quot;:4&#125;&#125;<br>&#123;&quot;location&quot;: &quot;51.222900,4.405200&quot;, &quot;name&quot;: &quot;Letterenhuis&quot;&#125;<br>&#123;&quot;index&quot;:&#123;&quot;_id&quot;:5&#125;&#125;<br>&#123;&quot;location&quot;: &quot;48.861111,2.336389&quot;, &quot;name&quot;: &quot;Musée du Louvre&quot;&#125;<br>&#123;&quot;index&quot;:&#123;&quot;_id&quot;:6&#125;&#125;<br>&#123;&quot;location&quot;: &quot;48.860000,2.327000&quot;, &quot;name&quot;: &quot;Musée d&#x27;Orsay&quot;&#125;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">地理边界聚合</span><br>POST /museums/_search?size=0<br>&#123;<br>    &quot;query&quot; : &#123;<br>        &quot;match&quot; : &#123; &quot;name&quot; : &quot;musée&quot; &#125;<br>    &#125;,<br>    &quot;aggs&quot; : &#123;<br>        &quot;viewport&quot; : &#123;<br>            &quot;geo_bounds&quot; : &#123;<br>                &quot;field&quot; : &quot;location&quot;, <br>                &quot;wrap_longitude&quot; : true <br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">地理重心聚合</span><br>POST /museums/_search?size=0<br>&#123;<br>    &quot;aggs&quot; : &#123;<br>        &quot;centroid&quot; : &#123;<br>            &quot;geo_centroid&quot; : &#123;<br>                &quot;field&quot; : &quot;location&quot; <br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br><br><br></code></pre></td></tr></table></figure>
</li>
<li><p>最大聚合</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">POST /sales/_search?size=0<br>&#123;<br>    &quot;aggs&quot; : &#123;<br>        &quot;max_price&quot; : &#123; &quot;max&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
</li>
<li><p>最小聚合</p>
<ul>
<li><code>min</code></li>
</ul>
</li>
<li><p>计数</p>
<ul>
<li><code>value_count</code></li>
</ul>
</li>
<li><p>统计聚合</p>
<ul>
<li><p><code>多值</code>指标聚合，可根据从聚合文档中提取的数值计算统计信息。这些值可以从文档中的特定数字字段中提取，也可以由提供的脚本生成。</p>
</li>
<li><p>返回的信息包括<code>min</code>，<code>max</code>，<code>sum</code>，<code>count</code> 和 <code>avg</code>。</p>
</li>
<li><pre><code class="shell">POST /exams/_search?size=0
&#123;
    &quot;aggs&quot; : &#123;
        &quot;grades_stats&quot; : &#123; &quot;stats&quot; : &#123; &quot;field&quot; : &quot;grade&quot; &#125; &#125;
    &#125;
&#125;
<figure class="highlight ada"><table><tr><td class="code"><pre><code class="hljs ada"><br>- 过滤器聚合<br><br>  ````shell<br>  POST /sales/_search?size=<span class="hljs-number">0</span><br>  &#123;<br>      <span class="hljs-string">&quot;aggs&quot;</span> : &#123;<br>          <span class="hljs-string">&quot;t_shirts&quot;</span> : &#123;<br>              <span class="hljs-string">&quot;filter&quot;</span> : &#123; &quot;<span class="hljs-type">term</span><span class="hljs-string">&quot;: &#123; &quot;</span><span class="hljs-keyword">type</span><span class="hljs-string">&quot;: &quot;</span>t-shirt<span class="hljs-string">&quot; &#125; &#125;,</span><br><span class="hljs-string">              &quot;</span>aggs<span class="hljs-string">&quot; : &#123;</span><br><span class="hljs-string">                  &quot;</span>avg_price<span class="hljs-string">&quot; : &#123; &quot;</span>avg<span class="hljs-string">&quot; : &#123; &quot;</span>field<span class="hljs-string">&quot; : &quot;</span>price<span class="hljs-string">&quot; &#125; &#125;</span><br><span class="hljs-string">              &#125;</span><br><span class="hljs-string">          &#125;</span><br><span class="hljs-string">      &#125;</span><br><span class="hljs-string">  &#125;</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3><figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"> 查询数据量</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">根据etl_time聚合数量</span><br>GET policy_index_all/_search<br>&#123;<br>  &quot;size&quot;: 0,<br>  &quot;aggs&quot;: &#123;<br>    &quot;by_etltime&quot;: &#123;<br>      &quot;terms&quot;: &#123;<br>        &quot;field&quot;: &quot;etl_time&quot;  <br>      &#125;,<br>      &quot;aggs&quot;: &#123;<br>        &quot;docs_count&quot;: &#123;<br>          &quot;value_count&quot;: &#123;<br>            &quot;field&quot;: &quot;_id&quot; <br>          &#125;<br>        &#125;  <br>      &#125;<br>    &#125;<br>  &#125;<br>&#125;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">直接查询所有数量</span><br>GET policy_index_all/_search<br>&#123;<br>  &quot;size&quot;: 0, <br>  &quot;aggs&quot;: &#123;<br>    &quot;total_docs&quot;: &#123;<br>      &quot;value_count&quot;: &#123;<br>        &quot;field&quot;: &quot;_id&quot;<br>      &#125;<br>    &#125;<br>  &#125;<br>&#125;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">删除数据</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">删除小于yyyy-MM-<span class="hljs-built_in">dd</span>的数据</span><br>POST policy_index_all/_delete_by_query<br>&#123;<br>  &quot;query&quot;: &#123;<br>    &quot;range&quot;: &#123;<br>      &quot;etl_time&quot;: &#123;<br>        &quot;lt&quot;: &quot;2023-12-04&quot;<br>      &#125;<br>    &#125;<br>  &#125;<br>&#125;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">删除etl_time为空的</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">删除etl_time不存在的数据</span><br>POST policy_index_all/_delete_by_query<br>&#123;<br>  &quot;query&quot;: &#123;<br>    &quot;bool&quot;: &#123;<br>      &quot;must_not&quot;: &#123;<br>        &quot;exists&quot;: &#123;<br>          &quot;field&quot;: &quot;etl_time&quot;<br>        &#125;<br>      &#125;<br>    &#125;<br>  &#125;<br>&#125;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">查询数据</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">term,不经过分词，直接查询精确的值</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">match,会使用分词器解析(先分析文档，然后再通过分析的文档进行查询)</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">includes指定字段</span><br>GET enterprise_data_index_all_v8/_search<br>&#123;<br>  &quot;query&quot;: &#123;<br>    &quot;bool&quot;: &#123;<br>      &quot;must&quot;: [<br>        &#123;<br>          &quot;term&quot;: &#123;<br>            &quot;eid&quot;: &#123;<br>              &quot;value&quot;: &quot;4c8bd41b-2534-43c0-8643-5bf4dca1991e&quot;<br>            &#125;<br>          &#125;<br>        &#125;,<br>        &#123;<br>          &quot;match&quot;: &#123;<br>            &quot;new_status_code&quot;: &quot;1&quot;<br>          &#125;<br>        &#125;<br>      ]<br>    &#125;<br>  &#125;,<br>  &quot;_source&quot;: &#123;<br>    &quot;includes&quot;: [<br>      &quot;name&quot;,<br>      &quot;granted_invent_num&quot;,<br>      &quot;granted_appearance_num&quot;,<br>      &quot;grant_utility_model_num&quot;<br>    ]<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>HA&amp;ZooKeeper</title>
    <url>/2021/06/02/HAHA-zk/</url>
    <content><![CDATA[<h1 id="HA-ZooKeeper"><a href="#HA-ZooKeeper" class="headerlink" title="HA&amp;ZooKeeper"></a>HA&amp;ZooKeeper</h1><h2 id="zookeeper"><a href="#zookeeper" class="headerlink" title="zookeeper"></a>zookeeper</h2><h3 id="zk概述"><a href="#zk概述" class="headerlink" title="zk概述"></a>zk概述</h3><ul>
<li>ZooKeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。</li>
<li>设计模式：是一个基于观察者模式设计的分布式服务管理框架，它辅助存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应。</li>
<li>Zookeeper特点，数据结构<ul>
<li><a href="https://imgtu.com/i/2lpCjI"><img src="https://z3.ax1x.com/2021/06/02/2lpCjI.png" alt="2lpCjI.png"></a></li>
<li><a href="https://imgtu.com/i/2lpmCQ"><img src="https://z3.ax1x.com/2021/06/02/2lpmCQ.png" alt="2lpmCQ.png"></a></li>
</ul>
</li>
</ul>
<h3 id="zk实战应用"><a href="#zk实战应用" class="headerlink" title="zk实战应用"></a>zk实战应用</h3><ul>
<li><p>客户端命令</p>
</li>
<li><p>启动客户端：zkCli.sh</p>
<table>
<thead>
<tr>
<th>命令基本语法</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>help</td>
<td>显示所有操作命令</td>
</tr>
<tr>
<td>ls path [watch]</td>
<td>使用 ls 命令来查看当前znode中所包含的内容</td>
</tr>
<tr>
<td>ls2 path [watch]</td>
<td>查看当前节点数据并能看到更新次数等数据</td>
</tr>
<tr>
<td>create</td>
<td>普通创建-s  含有序列-e  临时（重启或者超时消失）</td>
</tr>
<tr>
<td>get path [watch]</td>
<td>获得节点的值</td>
</tr>
<tr>
<td>set</td>
<td>设置节点的具体值</td>
</tr>
<tr>
<td>stat</td>
<td>查看节点状态</td>
</tr>
<tr>
<td>delete</td>
<td>删除节点</td>
</tr>
<tr>
<td>rmr</td>
<td>递归删除节点</td>
</tr>
</tbody></table>
</li>
</ul>
<h3 id="zk内部原理"><a href="#zk内部原理" class="headerlink" title="zk内部原理"></a>zk内部原理</h3><ul>
<li><p>节点类型</p>
<p><a href="https://imgtu.com/i/2lpq2j"><img src="https://z3.ax1x.com/2021/06/02/2lpq2j.png" alt="2lpq2j.png"></a></p>
</li>
<li><p>Stat结构体</p>
<ol>
<li><p>czxid-创建节点的事务zxid</p>
<p>每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。</p>
<p>事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。</p>
</li>
</ol>
</li>
<li><p>监听器原理</p>
<p> <img src="C:\Users\Hasee\AppData\Roaming\Typora\typora-user-images\image-20210602195353003.png" alt="image-20210602195353003"></p>
</li>
<li><p>选举机制</p>
<ul>
<li><a href="https://imgtu.com/i/2l9oO1"><img src="https://z3.ax1x.com/2021/06/02/2l9oO1.png" alt="2l9oO1.png"></a><ol>
<li>集群会通过比较先会比较zxid,若zxid相同则比较myid选择leader(每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID)</li>
<li>若第一次启动集群（zxid相等）myid如图所示：则大致流程如下：<ol>
<li>server1投票给自己，但是票数没到集群机器的半数以上，选举无法进行。server1状态为LOOKING</li>
<li>server2启动，发起选举，投票给自己，1发现2的myid大，更改选票为2。同理选举依然没法完成。</li>
<li>3启动，发起选举，投给自己，1，2发现3的myid大。则1，2将选票投给3，3获得三票。成为Leader,1,2更改状态为FOLLOWING。3更改状态为LEADING。</li>
<li>4启动，发起选举，1，2，3不在是LOOKING状态，不会更改选票信息。交换选票信息后，3为3票，4为一票，少数服从多数，更改选票信息为server3，更改状态为FOLLOWING。</li>
<li>同4一样</li>
</ol>
</li>
</ol>
</li>
</ul>
</li>
<li><p>ZAB</p>
<ul>
<li><p>ZAB协议是专门为zookeeper实现分布式协调功能而设计。zookeeper主要是根据ZAB协议是实现分布式系统数据一致性</p>
</li>
<li><p>写数据流程：</p>
<p><a href="https://imgtu.com/i/2lERBj"><img src="https://z3.ax1x.com/2021/06/02/2lERBj.png" alt="2lERBj.png"></a></p>
<ul>
<li>三个服务器会对是否写数据提出意见，都同意那正常执行写操作</li>
<li>一个server不同意而总票数支持同意，该server自杀再重启，再朝Leader同步信息</li>
<li>leader不同意，但总票数同意。leader自杀重启，重新选举leader，重启后再向leader同步数据</li>
<li>为什么会不同意：查看zxid，如果发来的zxid事物请求比自身的大才会同意</li>
<li>总之就是要尽可能的保持集群数据的一致性。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h2><h3 id="HA概述"><a href="#HA概述" class="headerlink" title="HA概述"></a>HA概述</h3><ul>
<li>HA（High Availablity）</li>
<li>实现高可用最重要的是消除单点故障</li>
<li>防止出现namenode节点挂了，无法正常提供服务</li>
</ul>
<h3 id="HA工作机制"><a href="#HA工作机制" class="headerlink" title="HA工作机制"></a>HA工作机制</h3><ul>
<li><p>故障转移机制 HDFS-HA</p>
<p><a href="https://imgtu.com/i/2lA1SS"><img src="https://z3.ax1x.com/2021/06/02/2lA1SS.png" alt="2lA1SS.png"></a></p>
</li>
<li><p>故障转移机制</p>
<ul>
<li>为了保证数据不丢失，一个失效后另一个补上不会丢失数据，日志文件写到第三方平台（相当于2nn）</li>
<li>新激活的namenode 可以从第三方，读取edits文件将缺少的数据补上，防止数据丢失</li>
<li>Zkfc检测到假死后是通过zookeeper服务器将信息传到其它的namenode的zkfc</li>
</ul>
</li>
<li><p>YARN-HA</p>
<p><a href="https://imgtu.com/i/2lAaF0"><img src="https://z3.ax1x.com/2021/06/02/2lAaF0.png" alt="2lAaF0.png"></a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>HA</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive_WindowFunction</title>
    <url>/2022/02/25/Hive-WindowFunction/</url>
    <content><![CDATA[<h1 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h1><h2 id="partition-by-子句"><a href="#partition-by-子句" class="headerlink" title="partition by 子句"></a>partition by 子句</h2><ul>
<li><p>窗口函数</p>
<ul>
<li><p>窗口函数，也叫OLAP函数（Online Anallytical Processing,联机分析处理），可以对数据库进行实时分析处理。</p>
</li>
<li><p>基本语法：</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql">&lt;窗口函数&gt; <span class="hljs-keyword">over</span> (<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> &lt;用于分组的列名&gt;<br>                <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> &lt;用于排序的列名&gt;)<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>语法中&lt;窗口函数&gt;的位置，可以放下以下两种函数：</p>
<ul>
<li><p>专用窗口函数，包括后面要讲到的rank,dense_rank,row-number等专用窗口函数</p>
</li>
<li><p>聚合函数，sum,avg,count,max,min等</p>
</li>
<li><pre><code>select *,
   rank() over (partition by 班级
                 order by 成绩 desc) as ranking
from 班级表
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><br>  ![](https://pic2.zhimg.com/v2<span class="hljs-number">-451</span>c70aa24c68aa7142693fd27c85605_r.jpg)<br><br>* 简单来说，窗口函数具有以下功能<br>  <br>  - 同时具有分组和排序的功能<br>  - 不减少原表的行数<br><br>## <span class="hljs-keyword">window</span>子句<br><br>* 如果只使用<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> 子句，未指定<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> 的话，我们的聚合就是分组内的聚合<br><br>* 使用了<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span>子句，未使用<span class="hljs-keyword">window</span>子句的情况下，默认从起点到当前行。<br><br>* 当同一个<span class="hljs-keyword">select</span>查询中存在多个窗口函数时，他们互相之间是没有影响的。每个窗口函数应用自己的规则。<br><br>* <span class="hljs-keyword">window</span>子句<br><br>  - <span class="hljs-keyword">PRECEDING</span>：往前<br><br>  - <span class="hljs-keyword">FOLLOWING</span>：往后<br><br>  - <span class="hljs-keyword">CURRENT</span> <span class="hljs-keyword">ROW</span>：往前行<br><br>  - <span class="hljs-keyword">UNBOUNDED</span>：起点<br><br>  - <span class="hljs-keyword">UNBOUNDED</span> <span class="hljs-keyword">PRECEDING</span> : 表示从前面的起点<br><br>  - <span class="hljs-keyword">UNBOUNDED</span> <span class="hljs-keyword">FOLLOWING</span> ：表示到后面的终点<br><br>  - ````sqlite<br>    <span class="hljs-keyword">select</span> <span class="hljs-type">name</span>,orderdate,<span class="hljs-keyword">cost</span>,<br>    sum(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>() <span class="hljs-keyword">as</span> sample1,<span class="hljs-comment">--所有行相加</span><br>    sum(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-type">name</span>) <span class="hljs-keyword">as</span> sample2,<span class="hljs-comment">--按name分组，组内数据相加</span><br>    sum(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-type">name</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate) <span class="hljs-keyword">as</span> sample3,<span class="hljs-comment">--按name分组，组内数据累加</span><br>    sum(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-type">name</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate <span class="hljs-keyword">rows</span> <span class="hljs-keyword">between</span> <span class="hljs-keyword">UNBOUNDED</span> <span class="hljs-keyword">PRECEDING</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">current</span> <span class="hljs-keyword">row</span> )  <span class="hljs-keyword">as</span> sample4 ,<span class="hljs-comment">--和sample3一样,由起点到当前行的聚合</span><br>    sum(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-type">name</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate <span class="hljs-keyword">rows</span> <span class="hljs-keyword">between</span> <span class="hljs-number">1</span> <span class="hljs-keyword">PRECEDING</span>   <span class="hljs-keyword">and</span> <span class="hljs-keyword">current</span> <span class="hljs-keyword">row</span>) <span class="hljs-keyword">as</span> sample5, <span class="hljs-comment">--当前行和前面一行做聚合</span><br>    sum(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-type">name</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate <span class="hljs-keyword">rows</span> <span class="hljs-keyword">between</span> <span class="hljs-number">1</span> <span class="hljs-keyword">PRECEDING</span>   <span class="hljs-keyword">AND</span> <span class="hljs-number">1</span> <span class="hljs-keyword">FOLLOWING</span>  ) <span class="hljs-keyword">as</span> sample6,<span class="hljs-comment">--当前行和前边一行及后面一行</span><br>    sum(<span class="hljs-keyword">cost</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-type">name</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate <span class="hljs-keyword">rows</span> <span class="hljs-keyword">between</span> <span class="hljs-keyword">current</span> <span class="hljs-keyword">row</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">UNBOUNDED</span> <span class="hljs-keyword">FOLLOWING</span> ) <span class="hljs-keyword">as</span> sample7 <span class="hljs-comment">--当前行及后面所有行</span><br>    <span class="hljs-keyword">from</span> t_window;<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">name</span>    orderdate   cost    sample1 sample2 sample3 sample4 sample5 sample6 sample7<br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span>  <span class="hljs-number">10</span>  <span class="hljs-number">661</span> <span class="hljs-number">176</span> <span class="hljs-number">10</span>  <span class="hljs-number">10</span>  <span class="hljs-number">10</span>  <span class="hljs-number">56</span>  <span class="hljs-number">176</span><br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">05</span>  <span class="hljs-number">46</span>  <span class="hljs-number">661</span> <span class="hljs-number">176</span> <span class="hljs-number">56</span>  <span class="hljs-number">56</span>  <span class="hljs-number">56</span>  <span class="hljs-number">111</span> <span class="hljs-number">166</span><br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">08</span>  <span class="hljs-number">55</span>  <span class="hljs-number">661</span> <span class="hljs-number">176</span> <span class="hljs-number">111</span> <span class="hljs-number">111</span> <span class="hljs-number">101</span> <span class="hljs-number">124</span> <span class="hljs-number">120</span><br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">02</span>-<span class="hljs-number">03</span>  <span class="hljs-number">23</span>  <span class="hljs-number">661</span> <span class="hljs-number">176</span> <span class="hljs-number">134</span> <span class="hljs-number">134</span> <span class="hljs-number">78</span>  <span class="hljs-number">120</span> <span class="hljs-number">65</span><br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">06</span>  <span class="hljs-number">42</span>  <span class="hljs-number">661</span> <span class="hljs-number">176</span> <span class="hljs-number">176</span> <span class="hljs-number">176</span> <span class="hljs-number">65</span>  <span class="hljs-number">65</span>  <span class="hljs-number">42</span><br><span class="hljs-attribute">mart</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">08</span>  <span class="hljs-number">62</span>  <span class="hljs-number">661</span> <span class="hljs-number">299</span> <span class="hljs-number">62</span>  <span class="hljs-number">62</span>  <span class="hljs-number">62</span>  <span class="hljs-number">130</span> <span class="hljs-number">299</span><br><span class="hljs-attribute">mart</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">09</span>  <span class="hljs-number">68</span>  <span class="hljs-number">661</span> <span class="hljs-number">299</span> <span class="hljs-number">130</span> <span class="hljs-number">130</span> <span class="hljs-number">130</span> <span class="hljs-number">205</span> <span class="hljs-number">237</span><br><span class="hljs-attribute">mart</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">11</span>  <span class="hljs-number">75</span>  <span class="hljs-number">661</span> <span class="hljs-number">299</span> <span class="hljs-number">205</span> <span class="hljs-number">205</span> <span class="hljs-number">143</span> <span class="hljs-number">237</span> <span class="hljs-number">169</span><br><span class="hljs-attribute">mart</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">13</span>  <span class="hljs-number">94</span>  <span class="hljs-number">661</span> <span class="hljs-number">299</span> <span class="hljs-number">299</span> <span class="hljs-number">299</span> <span class="hljs-number">169</span> <span class="hljs-number">169</span> <span class="hljs-number">94</span><br><span class="hljs-attribute">neil</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">05</span>-<span class="hljs-number">10</span>  <span class="hljs-number">12</span>  <span class="hljs-number">661</span> <span class="hljs-number">92</span>  <span class="hljs-number">12</span>  <span class="hljs-number">12</span>  <span class="hljs-number">12</span>  <span class="hljs-number">92</span>  <span class="hljs-number">92</span><br><span class="hljs-attribute">neil</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">06</span>-<span class="hljs-number">12</span>  <span class="hljs-number">80</span>  <span class="hljs-number">661</span> <span class="hljs-number">92</span>  <span class="hljs-number">92</span>  <span class="hljs-number">92</span>  <span class="hljs-number">92</span>  <span class="hljs-number">92</span>  <span class="hljs-number">80</span><br><span class="hljs-attribute">tony</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">02</span>  <span class="hljs-number">15</span>  <span class="hljs-number">661</span> <span class="hljs-number">94</span>  <span class="hljs-number">15</span>  <span class="hljs-number">15</span>  <span class="hljs-number">15</span>  <span class="hljs-number">44</span>  <span class="hljs-number">94</span><br><span class="hljs-attribute">tony</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">04</span>  <span class="hljs-number">29</span>  <span class="hljs-number">661</span> <span class="hljs-number">94</span>  <span class="hljs-number">44</span>  <span class="hljs-number">44</span>  <span class="hljs-number">44</span>  <span class="hljs-number">94</span>  <span class="hljs-number">79</span><br><span class="hljs-attribute">tony</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">07</span>  <span class="hljs-number">50</span>  <span class="hljs-number">661</span> <span class="hljs-number">94</span>  <span class="hljs-number">94</span>  <span class="hljs-number">94</span>  <span class="hljs-number">79</span>  <span class="hljs-number">79</span>  <span class="hljs-number">50</span><br></code></pre></td></tr></table></figure>



<h2 id="窗口函数中的序列函数"><a href="#窗口函数中的序列函数" class="headerlink" title="窗口函数中的序列函数"></a>窗口函数中的序列函数</h2><ul>
<li>Hive中常用的序列函数有下面几个：</li>
</ul>
<h3 id="NTILE"><a href="#NTILE" class="headerlink" title="NTILE"></a>NTILE</h3><ul>
<li><p>NTILE(n),用于将分组数据按照顺序切分成n片，返回当前切片值</p>
</li>
<li><p>NTILE 不支持ROWS BETWEEN</p>
</li>
<li><p>如果切片步均匀，默认增加第一个切片的分布</p>
</li>
<li><p>案例：假如我们想要给每位顾客购买金额前1&#x2F;3的交易记录，我们便可以使用这个函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">select name,orderdate,cost,<br>       ntile(3) over() as sample1 , --全局数据切片<br>       ntile(3) over(partition by name), -- 按照name进行分组,在分组内将数据切成3份<br>       ntile(3) over(order by cost),--全局按照cost升序排列,数据切成3份<br>       ntile(3) over(partition by name order by cost ) --按照name分组，在分组内按照cost升序排列,数据切成3份<br>from t_window<br></code></pre></td></tr></table></figure>
</li>
<li><pre><code>name    orderdate   cost    sample1 sample2 sample3 sample4
jack    2015-01-01  10  3   1   1   1
jack    2015-02-03  23  3   1   1   1
jack    2015-04-06  42  2   2   2   2
jack    2015-01-05  46  2   2   2   2
jack    2015-01-08  55  2   3   2   3
mart    2015-04-08  62  2   1   2   1
mart    2015-04-09  68  1   2   3   1
mart    2015-04-11  75  1   3   3   2
mart    2015-04-13  94  1   1   3   3
neil    2015-05-10  12  1   2   1   1
neil    2015-06-12  80  1   1   3   2
tony    2015-01-02  15  3   2   1   1
tony    2015-01-04  29  3   3   1   2
tony    2015-01-07  50  2   1   2   3
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><br>* <br><br>### row_number,rank,dense_rank<br><br>* row_number()  取直接的数字排名，比较值相同也不会重复<br>* rank()  类似于高考排名，比较值相同则排名相同，下一个排名数字会跳跃<br>* dense_rank  比较值相同则排名相同，但是排名的数字不会跳跃<br><br>### LAG和LEAD函数<br><br>* LAG(col,n,default_val) ：往前第n行数据,没有数据则返回default_val<br><br>* LEAD(col,n,default_val)：往后第n行数据,没有数据则返回default_val<br><br>* 案例：我们要查看顾客上次的购买时间<br><br>  ````sqlite<br>  <span class="hljs-keyword">select</span> <span class="hljs-type">name</span>,orderdate,<span class="hljs-keyword">cost</span>,<br>  lag(orderdate,<span class="hljs-number">1</span>,<span class="hljs-string">&#x27;1900-01-01&#x27;</span>) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-type">name</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate ) <span class="hljs-keyword">as</span> time1,<br>  lag(orderdate,<span class="hljs-number">2</span>) <span class="hljs-keyword">over</span> (<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> <span class="hljs-type">name</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> orderdate) <span class="hljs-keyword">as</span> time2<br>  <span class="hljs-keyword">from</span> t_window;<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>结果：</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">name</span>    orderdate   cost    time1   time2<br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span>  <span class="hljs-number">10</span>  <span class="hljs-number">1900</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span>  NULL<br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">05</span>  <span class="hljs-number">46</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span>  NULL<br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">08</span>  <span class="hljs-number">55</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">05</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span><br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">02</span>-<span class="hljs-number">03</span>  <span class="hljs-number">23</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">08</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">05</span><br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">06</span>  <span class="hljs-number">42</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">02</span>-<span class="hljs-number">03</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">08</span><br><span class="hljs-attribute">mart</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">08</span>  <span class="hljs-number">62</span>  <span class="hljs-number">1900</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span>  NULL<br><span class="hljs-attribute">mart</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">09</span>  <span class="hljs-number">68</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">08</span>  NULL<br><span class="hljs-attribute">mart</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">11</span>  <span class="hljs-number">75</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">09</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">08</span><br><span class="hljs-attribute">mart</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">13</span>  <span class="hljs-number">94</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">11</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">09</span><br><span class="hljs-attribute">neil</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">05</span>-<span class="hljs-number">10</span>  <span class="hljs-number">12</span>  <span class="hljs-number">1900</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span>  NULL<br><span class="hljs-attribute">neil</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">06</span>-<span class="hljs-number">12</span>  <span class="hljs-number">80</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">05</span>-<span class="hljs-number">10</span>  NULL<br><span class="hljs-attribute">tony</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">02</span>  <span class="hljs-number">15</span>  <span class="hljs-number">1900</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span>  NULL<br><span class="hljs-attribute">tony</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">04</span>  <span class="hljs-number">29</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">02</span>  NULL<br><span class="hljs-attribute">tony</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">07</span>  <span class="hljs-number">50</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">04</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">02</span><br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="first-value和last-value"><a href="#first-value和last-value" class="headerlink" title="first_value和last_value"></a>first_value和last_value</h3><ul>
<li><p>first_value取分组内排序后，截止到当前行，第一个值</p>
</li>
<li><p>last_value取分组内排序后，截止到当前行，最后一个值</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">select name,orderdate,cost,<br>first_value(orderdate) over(partition by name order by orderdate) as time1,<br>last_value(orderdate) over(partition by name order by orderdate) as time2<br>from t_window<br></code></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">name</span>    orderdate   cost    time1   time2<br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span>  <span class="hljs-number">10</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span><br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">05</span>  <span class="hljs-number">46</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">05</span><br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">08</span>  <span class="hljs-number">55</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">08</span><br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">02</span>-<span class="hljs-number">03</span>  <span class="hljs-number">23</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">02</span>-<span class="hljs-number">03</span><br><span class="hljs-attribute">jack</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">06</span>  <span class="hljs-number">42</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">01</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">06</span><br><span class="hljs-attribute">mart</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">08</span>  <span class="hljs-number">62</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">08</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">08</span><br><span class="hljs-attribute">mart</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">09</span>  <span class="hljs-number">68</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">08</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">09</span><br><span class="hljs-attribute">mart</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">11</span>  <span class="hljs-number">75</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">08</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">11</span><br><span class="hljs-attribute">mart</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">13</span>  <span class="hljs-number">94</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">08</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">04</span>-<span class="hljs-number">13</span><br><span class="hljs-attribute">neil</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">05</span>-<span class="hljs-number">10</span>  <span class="hljs-number">12</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">05</span>-<span class="hljs-number">10</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">05</span>-<span class="hljs-number">10</span><br><span class="hljs-attribute">neil</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">06</span>-<span class="hljs-number">12</span>  <span class="hljs-number">80</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">05</span>-<span class="hljs-number">10</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">06</span>-<span class="hljs-number">12</span><br><span class="hljs-attribute">tony</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">02</span>  <span class="hljs-number">15</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">02</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">02</span><br><span class="hljs-attribute">tony</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">04</span>  <span class="hljs-number">29</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">02</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">04</span><br><span class="hljs-attribute">tony</span>    <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">07</span>  <span class="hljs-number">50</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">02</span>  <span class="hljs-number">2015</span>-<span class="hljs-number">01</span>-<span class="hljs-number">07</span><br><br></code></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>窗口函数</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop重要知识点</title>
    <url>/2021/05/26/Hadoop/</url>
    <content><![CDATA[<h1 id="Hadoop重要知识点理解"><a href="#Hadoop重要知识点理解" class="headerlink" title="Hadoop重要知识点理解"></a>Hadoop重要知识点理解</h1><h2 id="Hadoop核心组件"><a href="#Hadoop核心组件" class="headerlink" title="Hadoop核心组件"></a>Hadoop核心组件</h2><ul>
<li><p>MapReduce（计算框架）</p>
</li>
<li><p>yarn （资源调度）</p>
</li>
<li><p>HDFS（hadoop的分布式文件系统，主要用于数据存储）</p>
</li>
<li><p>Common(辅助工具，包含一些依赖，jar包)</p>
</li>
<li><p>常用端口号</p>
<p>分类	                         应用	                               端口<br>namenode	            rpc-address	                8020<br>namenode	            http-address	              9870<br>namenode	            https-address	            9871<br>datanode	              address	                       9866<br>datanode	              http-address	              9864<br>datanode	               https-address	           9865<br>resourcemanager	http-address	             8088</p>
</li>
</ul>
<h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><ul>
<li><p>定义</p>
<ul>
<li>HDFS（Hadoop Distributed File System）是一个分布式文件系统</li>
</ul>
</li>
<li><p>优点</p>
<ul>
<li>高容错性:多副本机制</li>
<li>适合处理大数据，可构建在廉价机器上</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>不适合低延时数据访问</li>
<li>无法高效的对大量小文件进行存储</li>
<li>只支持数据的追加，不支持文件的随机修改</li>
<li>一个文件只能有一个写，不允许多个线程同时写</li>
</ul>
</li>
<li><p>HDFS组成架构</p>
<ul>
<li><a href="https://imgtu.com/i/g2T94P"><img src="https://z3.ax1x.com/2021/05/17/g2T94P.png" alt="g2T94P.png"></a></li>
</ul>
</li>
</ul>
<h3 id="HDFS文件块大小"><a href="#HDFS文件块大小" class="headerlink" title="HDFS文件块大小"></a>HDFS文件块大小</h3><ul>
<li>hdfs中的文件在物理上是分块（BLOCK) 存储的</li>
<li>问题：为什么块大小不能设置太大，也不能设置太小？<ul>
<li>设置太大：增加数据的传输时间。从磁盘传输数据的时间会明显大于定位这个块开始位置所用的时间。导致程序处理这块数据时非常慢。</li>
<li>设置太小：分块太多，查找第一个块时会消耗大量时间。寻址时间增加。造成元数据增多，而元数据存储在namenode中，会消耗更多的namenode内存。</li>
</ul>
</li>
</ul>
<h2 id="HDFS的Shell操作"><a href="#HDFS的Shell操作" class="headerlink" title="HDFS的Shell操作"></a>HDFS的Shell操作</h2><ul>
<li>hadoop fs + (-命令)</li>
<li>hdfs dfs + (-命令)</li>
<li>dfs是fs的实现类</li>
</ul>
<ul>
<li><p>Hadoop fs 命令分类</p>
</li>
<li><p>本地-&gt;HDFS<br>put #上传文件到HDFS<br>copyFromLocal#与put相似，支持多线程<br>moveFromLocal将本地文件移动到HDFS上<br>appendToFile#追加文件信息,只能追加本地文件信息</p>
</li>
<li><p>HDFS-&gt;HDFS(命令与linux类似)</p>
<p>​    cp<br>​    mv<br>​    chown<br>​    chgrp<br>​    chmod<br>​    mkdir<br>​    du<br>​    df<br>​    cat<br>​    rm</p>
</li>
<li><p>HDFS-&gt;本地<br>get#从HDFS上下载文件到本地<br>getmerge#合并下载，符合条件的全部下载到本地<br>copyToLocal#与get完全一样</p>
</li>
</ul>
<h2 id="HDFS的API操作"><a href="#HDFS的API操作" class="headerlink" title="HDFS的API操作"></a>HDFS的API操作</h2><figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Test</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">testCopyFromLocalFile</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException, URISyntaxException &#123;<br><br>		<span class="hljs-comment">// 1 获取文件系统</span><br>		<span class="hljs-type">Configuration</span> <span class="hljs-variable">configuration</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Configuration</span>();<br>		configuration.set(<span class="hljs-string">&quot;dfs.replication&quot;</span>, <span class="hljs-string">&quot;2&quot;</span>);<br>		<span class="hljs-type">FileSystem</span> <span class="hljs-variable">fs</span> <span class="hljs-operator">=</span> FileSystem.get(<span class="hljs-keyword">new</span> <span class="hljs-title class_">URI</span>(<span class="hljs-string">&quot;hdfs://hadoop100:8020&quot;</span>), configuration, <span class="hljs-string">&quot;zt&quot;</span>);<br><br>		<span class="hljs-comment">// 2 上传文件</span><br>		fs.copyFromLocalFile(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(<span class="hljs-string">&quot;e:/banzhang.txt&quot;</span>), <span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(<span class="hljs-string">&quot;/banzhang.txt&quot;</span>));<br><br>		<span class="hljs-comment">// 3 关闭资源</span><br>		fs.close();<br><br>		System.out.println(<span class="hljs-string">&quot;over&quot;</span>);<br></code></pre></td></tr></table></figure>

<h2 id="HDFS的数据读写流程"><a href="#HDFS的数据读写流程" class="headerlink" title="HDFS的数据读写流程"></a>HDFS的数据读写流程</h2><ul>
<li><p>HDFS写数据流程</p>
<p><a href="https://imgtu.com/i/gWkJxS"><img src="https://z3.ax1x.com/2021/05/17/gWkJxS.md.png" alt="gWkJxS.md.png"></a></p>
</li>
</ul>
<ul>
<li>步骤：<ol>
<li>客户端通过DistributedFileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已经存在，父目录是否存在。</li>
<li>NameNode返回是否可以上传的一个响应。</li>
<li>客户端向NameNode请求第一个BLock应该上传到那几个DataNode节点上</li>
<li>NameNode返回副本个数个DataNode节点，例如dn1,dn2,dn3</li>
<li>客户端通过FSDataOutputStream模块请求建立Block传输通道</li>
<li>dn1,dn2,dn3，等副本逐级应答客户端</li>
<li>应答成功后，开始传输数据（以Packet（一般为64kb）的形式），dn1收到一个Packet就会传给dn2,dn2传给dn3。dn1每传一个Packet会放入一个应答队列等待应答</li>
<li>第一个Block上传完成后，客户端再请求NameNode上传第二个Block的服务器。</li>
</ol>
</li>
</ul>
<ul>
<li><p>DataNode的选择</p>
<ul>
<li>每一块数据存到哪些DataNode节点上，NameNode会进行选择。</li>
<li>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。</li>
<li>HDFS会通过机架感知技术，得到网络拓扑图，根据网络拓扑图选择最近的节点</li>
<li>那其他的副本是怎样选择的呢？：1.第一个副本再Client所处的节点上（自己距离自己最近），如果客户端在集群外面，随机选择一个。2.第二个副本和第一个副本位于相同机架上，随即节点。3.第三个副本位于不同机架，随机节点。</li>
</ul>
</li>
<li><p>HDFS读数据流程</p>
<p> <a href="https://imgtu.com/i/gWpZv9"><img src="https://z3.ax1x.com/2021/05/17/gWpZv9.md.png" alt="gWpZv9.md.png"></a></p>
</li>
</ul>
<ul>
<li>步骤：<ul>
<li>客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</li>
<li>挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据</li>
<li>DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</li>
<li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</li>
</ul>
</li>
</ul>
<h2 id="NameNode和SecondaryNameNode"><a href="#NameNode和SecondaryNameNode" class="headerlink" title="NameNode和SecondaryNameNode"></a>NameNode和SecondaryNameNode</h2><ul>
<li><p>namenode中一个元数据占150个字节的空间。</p>
</li>
<li><p>思考：NameNode中的元数据是存储在哪里的？</p>
<p>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage。</p>
<p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。</p>
<p>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。</p>
</li>
<li><p>工作机制</p>
<p><a href="https://imgtu.com/i/gW92Se"><img src="https://z3.ax1x.com/2021/05/17/gW92Se.md.png" alt="gW92Se.md.png"></a></p>
</li>
<li><p>具体步骤：</p>
<ul>
<li>第一阶段：NameNode<ul>
<li>第一次启动NameNode，格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载日志文件和镜像文件到内存中去</li>
<li>客户端对元数据的增删改请求</li>
<li>NameNode记录操作日志到Edits文件，更新滚动日志（当这一份日志已满，或者定时时间到了，重新创建一份新的日志文件（增删改记录写在新文件中），将旧的日志文件写到SecondaryNameNode中去）。</li>
<li>NameNode在内存中进行增删改操作</li>
</ul>
</li>
<li>第二阶段：SecondaryNameNode<ul>
<li>2nn询问nn是否需要CheckPoint，直接带回返回结果</li>
<li>请求执行CheckPoint（触发条件：1.2nn每隔一小时执行一次。2.当操作次数达到1百万时）</li>
<li>nn滚动正在写的Edits日志</li>
<li>将滚动前的日志文件和镜像文件拷贝到SecondaryNameNode中</li>
<li>2nn将从nn拷贝过来的日志文件和镜像文件在内存中合并</li>
<li>生成新的镜像文件fsimage.chkpoint</li>
<li>将新生成的镜像文件fsimage.chkpoint写到NameNode中</li>
<li>NameNode将fsimage.chkpoint重新命名成fsimage。</li>
</ul>
</li>
</ul>
</li>
<li><p>集群安全模式</p>
</li>
<li><p>集群在安全模式下，不能执行写操作，读操作会被延迟</p>
</li>
<li><p>在集群刚刚启动，或者集群的存储快要到达上限时会进入集群安全模式。</p>
</li>
<li><p>HDFS架构-联邦架构</p>
<ul>
<li>![image-20211220153640114](C:\Users\Hasee\AppData\Roaming\Typora\typora-user-images\image-2021122015364011 4.png)</li>
<li>解决单个namenode的压力过大的问题。</li>
<li>扩展namenode，每个NN共用一个集群里的存储资源，每个NameNode都可以单独对外提供服务。</li>
<li>集群规模特别大时，可采用HA(高可用)+Federation(联邦)的部署方案</li>
</ul>
</li>
</ul>
<h2 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h2><ul>
<li><p>DataNode工作机制</p>
<p><a href="https://imgtu.com/i/gWkiU1"><img src="https://z3.ax1x.com/2021/05/17/gWkiU1.md.png" alt="gWkiU1.md.png"></a></p>
</li>
</ul>
<ul>
<li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和（保证数据完整性），以及时间戳。</li>
<li>DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。</li>
<li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。</li>
<li>集群运行中可以安全加入和退出一些机器。（通过配置黑白名单）<ul>
<li>添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出。</li>
<li>在黑名单上面的主机都会被强制退出。（进入黑名单那后状态变为不可用10分钟+30s 后退役）</li>
<li>不允许白名单和黑名单中同时出现同一个主机名称</li>
</ul>
</li>
</ul>
<h2 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h2><ul>
<li><p>MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。</p>
<p>MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。</p>
</li>
<li><p>一个完整的MapReduce程序在分布式运行时有三类实例进程：</p>
<p>（1）<strong>MrAppMaster</strong>：负责整个程序的过程调度及状态协调。</p>
<p>（2）<strong>MapTask</strong>：负责Map阶段的整个数据处理流程。</p>
<p>（3）<strong>ReduceTask</strong>：负责Reduce阶段的整个数据处理流程。</p>
</li>
<li><p>WC实例：</p>
<p>Mapper代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.zt3019.wordcount;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.LongWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Mapper;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><br><span class="hljs-comment">//（输入类型：框架会将数据分成一行一行的，LongWritable表示这一行的第一个字符的索引,Text表示这一行内容）KEYIN,VALUEIN,(输出类型：单词，1)KEYOUT,VALUEOUT</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCountMapper</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Mapper</span>&lt;LongWritable,Text,Text, IntWritable&gt; &#123;<br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 框架将数据拆成一行一行输入进来，我们把数据变成（单词，1）的形式</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> key 行号</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> value 行内容</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> context 任务本身</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> IOException</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> InterruptedException</span><br><span class="hljs-comment">     */</span><br>    Text word=<span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>();<br>    IntWritable one=<span class="hljs-keyword">new</span> <span class="hljs-title class_">IntWritable</span>(<span class="hljs-number">1</span>);<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">map</span><span class="hljs-params">(LongWritable key, Text value, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException &#123;<br>        <span class="hljs-comment">//拿到一行数据</span><br>        String line=value.toString();<br>        <span class="hljs-comment">//将一行拆成很多单词</span><br>        String[] words = line.split(<span class="hljs-string">&quot; &quot;</span>);<br><br>        <span class="hljs-comment">//将（单词，1）写回框架</span><br>        <span class="hljs-keyword">for</span> (String word : words) &#123;<br>            <span class="hljs-built_in">this</span>.word.set(word);<br>            context.write(<span class="hljs-built_in">this</span>.word,<span class="hljs-built_in">this</span>.one);<br>        &#125;<br>    &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure>
</li>
<li><p>Redecer</p>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.zt3019.wordcount;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Reducer;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><br><span class="hljs-comment">//输入为map的输出，输出为（单词，单词出现的次数）</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCountReducer</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Reducer</span>&lt;Text, IntWritable,Text,IntWritable&gt; &#123;<br>    <span class="hljs-type">int</span> sum=<span class="hljs-number">0</span>;<br>    IntWritable value=<span class="hljs-keyword">new</span> <span class="hljs-title class_">IntWritable</span>();<br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 框架把数据按照单词分好组输入给我们，我们将同一个单词的次数相加</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> key 单词</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> values 这个单词所有的1</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> context 任务本身</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> IOException</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> InterruptedException</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Override</span><br><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">reduce</span><span class="hljs-params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException &#123;<br>        <span class="hljs-comment">//累加单词出现的次数</span><br>        <span class="hljs-keyword">for</span> (IntWritable value : values) &#123;<br>            sum+=value.get();<br>        &#125;<br>        <span class="hljs-comment">//封装结果</span><br>        value.set(sum);<br>        <span class="hljs-comment">//将结果写回框架</span><br>        context.write(key,value);<br>    &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure>


</li>
<li><p>Driver(本地集群模式)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.zt3019.wordcount;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.conf.Configuration;<br><span class="hljs-keyword">import</span> org.apache.hadoop.fs.Path;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.IntWritable;<br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Text;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.Job;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br><span class="hljs-keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br><br><span class="hljs-keyword">import</span> java.io.IOException;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCountDriver</span>&#123;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> IOException, ClassNotFoundException, InterruptedException &#123;<br>        <span class="hljs-type">Configuration</span> <span class="hljs-variable">configuration</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Configuration</span>();<br>        <span class="hljs-comment">//配置yarn集群运行</span><br>        <span class="hljs-comment">//core-site.xml配置文件</span><br>        configuration.set(<span class="hljs-string">&quot;fs.defaultFS&quot;</span>, <span class="hljs-string">&quot;hdfs://hadoop100:8020&quot;</span>);<br>        <span class="hljs-comment">//MapRedece-site.xml配置文件</span><br>        configuration.set(<span class="hljs-string">&quot;mapreduce.framework.name&quot;</span>,<span class="hljs-string">&quot;yarn&quot;</span>);<br>        <span class="hljs-comment">//是否允许向linux提交任务</span><br>        configuration.set(<span class="hljs-string">&quot;mapreduce.app-submission.cross-platform&quot;</span>,<span class="hljs-string">&quot;true&quot;</span>);<br>        configuration.set(<span class="hljs-string">&quot;yarn.resourcemanager.hostname&quot;</span>,<span class="hljs-string">&quot;hadoop102&quot;</span>);<br>        <span class="hljs-comment">//1.获取job实例</span><br>        Job job= Job.getInstance(configuration);<br>        <span class="hljs-comment">//2.设置jar包</span><br>        job.setJarByClass(WordCountDriver.class);<br>        <span class="hljs-comment">//3.设置Mapper和Reducer</span><br>        job.setMapperClass(WordCountMapper.class);<br>        job.setReducerClass(WordCountReducer.class);<br>        <span class="hljs-comment">//4.设置Map和Reduce的输出类型</span><br>        <span class="hljs-comment">//设置Map的输出类型</span><br>        job.setMapOutputKeyClass(Text.class);<br>        job.setMapOutputValueClass(IntWritable.class);<br>        <span class="hljs-comment">//设置Reduce的输出类型</span><br>        job.setOutputKeyClass(Text.class);<br>        job.setOutputValueClass(IntWritable.class);<br>        <span class="hljs-comment">//5.设置输入输出文件</span><br>        FileInputFormat.setInputPaths(job,<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(args[<span class="hljs-number">0</span>]));<br>        FileOutputFormat.setOutputPath(job,<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(args[<span class="hljs-number">1</span>]));<br>        <span class="hljs-comment">//6.提交job</span><br>        <span class="hljs-comment">//提交任务得到运行结果，成功或者失败</span><br>        <span class="hljs-comment">//提交流程都在这个方法中，一些job的配置都可以在上面设置。</span><br>        <span class="hljs-type">boolean</span> <span class="hljs-variable">b</span> <span class="hljs-operator">=</span> job.waitForCompletion(<span class="hljs-literal">true</span>);<br>        System.exit(b ? <span class="hljs-number">0</span>:<span class="hljs-number">1</span>);<br>    &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure>
</li>
<li><p>提交到集群上运行，先maven打包，</p>
<figure class="highlight scss"><table><tr><td class="code"><pre><code class="hljs scss">yarn jar 包名 主类引用(reference) args<span class="hljs-selector-attr">[0]</span>(输入路径) args<span class="hljs-selector-attr">[1]</span>(输出路径)<br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="Hadoop压缩"><a href="#Hadoop压缩" class="headerlink" title="Hadoop压缩"></a>Hadoop压缩</h3><ul>
<li><table>
<thead>
<tr>
<th>压缩格式</th>
<th>hadoop自带？</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
<th>换成压缩格式后，原来的程序是否需要修改</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>Gzip</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>bzip2</td>
<td>是，直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>LZO</td>
<td>否，需要安装</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
<td>需要建索引，还需要指定输入格式</td>
</tr>
<tr>
<td>Snappy</td>
<td>否，需要安装</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
</tbody></table>
</li>
</ul>
<h3 id="Hadoop序列化"><a href="#Hadoop序列化" class="headerlink" title="Hadoop序列化"></a>Hadoop序列化</h3><ul>
<li><p>序列化就是把内存中的对象，转换成字节序列（或其它数据传输协议）以便于存储到磁盘（持久化）和网络传输。</p>
</li>
<li><p>反序列化就是将收到的字节序列（或其它数据传输协议）或者是磁盘中持久化的数据，转换成内存中的对象。</p>
</li>
<li><p>为什么要序列化：一般来说，“活的”对象只存储在内存中，关机断电就没有了，而且“活的”对象只能由本地进程使用，不能被发送到网络上的另外一台计算机。而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。</p>
</li>
<li><p>自定义的数据类型要实现Writable接口实现序列化，反序列功能</p>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.zt3019.flow;<br><br><span class="hljs-keyword">import</span> org.apache.hadoop.io.Writable;<br><br><span class="hljs-keyword">import</span> java.io.DataInput;<br><span class="hljs-keyword">import</span> java.io.DataOutput;<br><span class="hljs-keyword">import</span> java.io.IOException;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">FlowBean</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">Writable</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">long</span> upFlow;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">long</span> downFlow;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">long</span> sumFlow;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">set</span><span class="hljs-params">(<span class="hljs-type">long</span> upFlow,<span class="hljs-type">long</span> downFlow)</span>&#123;<br>        <span class="hljs-built_in">this</span>.upFlow=upFlow;<br>        <span class="hljs-built_in">this</span>.downFlow=downFlow;<br>        <span class="hljs-built_in">this</span>.sumFlow=upFlow+downFlow;<br>    &#125;<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">toString</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;FlowBean&#123;&quot;</span> +<br>                <span class="hljs-string">&quot;upFlow=&quot;</span> + upFlow +<br>                <span class="hljs-string">&quot;, downFlow=&quot;</span> + downFlow +<br>                <span class="hljs-string">&quot;, sumFlow=&quot;</span> + sumFlow +<br>                <span class="hljs-string">&#x27;&#125;&#x27;</span>;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setUpFlow</span><span class="hljs-params">(<span class="hljs-type">long</span> upFlow)</span> &#123;<br>        <span class="hljs-built_in">this</span>.upFlow = upFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setDownFlow</span><span class="hljs-params">(<span class="hljs-type">long</span> downFlow)</span> &#123;<br>        <span class="hljs-built_in">this</span>.downFlow = downFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setSumFlow</span><span class="hljs-params">(<span class="hljs-type">long</span> sumFlow)</span> &#123;<br>        <span class="hljs-built_in">this</span>.sumFlow = sumFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">long</span> <span class="hljs-title function_">getDownFlow</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> downFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">long</span> <span class="hljs-title function_">getSumFlow</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> sumFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">long</span> <span class="hljs-title function_">getUpFlow</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> upFlow;<br>    &#125;<br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 序列化</span><br><span class="hljs-comment">     * 将对象数据写出到框架指定的地方</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> dataOutput 数据的容器</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> IOException</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">write</span><span class="hljs-params">(DataOutput dataOutput)</span> <span class="hljs-keyword">throws</span> IOException &#123;<br>        dataOutput.writeLong(upFlow);<br>        dataOutput.writeLong(downFlow);<br>        dataOutput.writeLong(sumFlow);<br><br>    &#125;<br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 反序列化</span><br><span class="hljs-comment">     *从框架指定地方读取数填充对象</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> dataInput 数据的容器</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@throws</span> IOException</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">readFields</span><span class="hljs-params">(DataInput dataInput)</span> <span class="hljs-keyword">throws</span> IOException &#123;<br>        <span class="hljs-built_in">this</span>.upFlow=dataInput.readLong();<br>        <span class="hljs-built_in">this</span>.downFlow=dataInput.readLong();<br>        <span class="hljs-built_in">this</span>.sumFlow=dataInput.readLong();<br>    &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="MapReduce框架原理"><a href="#MapReduce框架原理" class="headerlink" title="MapReduce框架原理"></a>MapReduce框架原理</h2><h3 id="MapReduce的数据流"><a href="#MapReduce的数据流" class="headerlink" title="MapReduce的数据流"></a>MapReduce的数据流</h3><p><a href="https://imgtu.com/i/g72oxx"><img src="https://z3.ax1x.com/2021/05/21/g72oxx.md.png" alt="g72oxx.md.png"></a></p>
<h3 id="InputFormat"><a href="#InputFormat" class="headerlink" title="InputFormat"></a>InputFormat</h3><ul>
<li><p>InputFormat实现数据变成K,V值</p>
</li>
<li><p>数据切片与MapTask并行度决定机制</p>
</li>
</ul>
<p><a href="https://imgtu.com/i/gHPDHK"><img src="https://z3.ax1x.com/2021/05/21/gHPDHK.md.png" alt="gHPDHK.md.png"></a></p>
<p>* </p>
<ul>
<li><p>job提交流程源码重要流程（客户端向集群提交的作业）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java">waitForCompletion()<span class="hljs-comment">//等待提交任务</span><br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">submit</span><span class="hljs-params">()</span>&#123;<br>    ensureState()<span class="hljs-comment">//1.确认job状态是定义的</span><br>    setUserNewAPI()<span class="hljs-comment">//2.设置新的API</span><br>    connect()<span class="hljs-comment">//获取连接</span><br>&#125;<br><br><span class="hljs-comment">// 1建立连接</span><br>	connect();	<br>		<span class="hljs-comment">// 1）创建提交Job的代理</span><br>		<span class="hljs-keyword">new</span> <span class="hljs-title class_">Cluster</span>(getConfiguration());<br>			<span class="hljs-comment">// （1）判断是本地yarn还是远程</span><br>			initialize(jobTrackAddr, conf); <br><br><span class="hljs-comment">// 2 提交job</span><br>submitter.submitJobInternal(Job.<span class="hljs-built_in">this</span>, cluster)<br>	<span class="hljs-comment">// 1）创建给集群提交数据的Stag路径</span><br>	<span class="hljs-type">Path</span> <span class="hljs-variable">jobStagingArea</span> <span class="hljs-operator">=</span> JobSubmissionFiles.getStagingDir(cluster, conf);<br><br>	<span class="hljs-comment">// 2）获取jobid ，并创建Job路径</span><br>	<span class="hljs-type">JobID</span> <span class="hljs-variable">jobId</span> <span class="hljs-operator">=</span> submitClient.getNewJobID();<br><br>	<span class="hljs-comment">// 3）拷贝jar包到集群</span><br>copyAndConfigureFiles(job, submitJobDir);	<br>	rUploader.uploadFiles(job, jobSubmitDir);<br><br><span class="hljs-comment">// 4）计算切片，生成切片信息，数据那一块给哪一个MapTask处理</span><br>writeSplits(job, submitJobDir);<br>		maps = writeNewSplits(job, jobSubmitDir);<br>		input.getSplits(job);<br><br><span class="hljs-comment">// 5）向Stag路径写XML配置文件</span><br>writeConf(conf, submitJobFile);<br>	conf.writeXml(out);<br><br><span class="hljs-comment">// 6）提交Job,返回提交状态</span><br>status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());<br></code></pre></td></tr></table></figure>
</li>
<li><p>InputFormat两个重要过程</p>
<ul>
<li>切片：将文件切片，逻辑上的划分。默认是FileInputFormat，默认就是分块大小。在客户端完成，切几个片就有几个MapTask</li>
<li>RecordReader:对于给定的一个切片得到一个RecordReader，将数据划分成指定的K,V值，传到Map中，作为Map的输入。发生在MapTask中。</li>
</ul>
</li>
<li><p>自定义InputFormat方法</p>
<ul>
<li>自定义一个类继承FileInputFormat</li>
<li>Split：重写isSplitable()，将文件切片，可以自己定义切片规则，返回false 表示不切割</li>
<li>RecordReader：重写createRecordReader()，实现自定义输入到Map的K,V值</li>
</ul>
</li>
<li><p>Map阶段</p>
<ul>
<li>MapTask: Map阶段实际执行MapTask.run()方法，MapTask是一个Map的实现类</li>
<li>Mapper：在MapTask.run方法中会调用Mapper对象的map方法</li>
<li>map: 定义在Mapper中的方法</li>
</ul>
</li>
</ul>
<h3 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h3><ul>
<li><p>shuffle负责整理数据</p>
</li>
<li><p>shuffle流程</p>
<ul>
<li><a href="https://imgtu.com/i/gLfma6"><img src="https://z3.ax1x.com/2021/05/22/gLfma6.md.png" alt="gLfma6.md.png"></a><ol>
<li>MapTask收集map()方法输出的KV对，放到环形内存缓冲区中</li>
<li>在内存缓冲区中会进行快速排序，不断溢写到本地磁盘文件</li>
<li>多个溢出的文件会被合成大的溢出文件(溢写到磁盘耗时间，耗资源)</li>
<li>在溢出过程和合并过程中，都调用Partitiner进行分区和针对Key 进行排序，在环形缓冲区中溢出时进行快速排序，在合并时进行归并排序。</li>
<li>ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</li>
<li>ReduceTask 会取到来自不同MapTask 的同一个分区结果文件，ReduceTask会将这些文件再进行合并（归并排序）</li>
<li>合并成大文件后，shuffle过程结束，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Gruop，调用用户自定义的reduce()方法）</li>
</ol>
</li>
</ul>
</li>
<li><p>Partition分区</p>
<ul>
<li><p>当设置多个ReduceTask时，需要对MapTask输出来的数据进行分区。若ReduceTask数量为1，不执行分区过程。ReduceTask数量默认为1</p>
</li>
<li><p>适应于将将结果按照条件输出到不同分区文件中（分区）。例如：词频统计中分连个区A-M一个分区，M-Z一个分区</p>
</li>
<li><p>当数据量太大时，将多个MapTask得到的结果放到一个Reduce中合并时会非常慢，为了提高效率，需要并行处理，即设置多个ReduceTask处理。ReduceTask需要设置，估算数据业务的量</p>
</li>
<li><p>分区在环形缓冲区时就已经开始进行了</p>
</li>
<li><p>默认分区是根据key的hashCode对ReduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">HashPartitioner</span>&lt;K, V&gt; <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Partitioner</span>&lt;K, V&gt; &#123;<br><br>  <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">getPartition</span><span class="hljs-params">(K key, V value, <span class="hljs-type">int</span> numReduceTasks)</span> &#123;<br>    <span class="hljs-keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;<br>  &#125;<br><span class="hljs-comment">//&amp;的目的是去负号</span><br>&#125;<br></code></pre></td></tr></table></figure>

<p><a href="https://imgtu.com/i/gzRSkn"><img src="https://z3.ax1x.com/2021/05/25/gzRSkn.md.png" alt="gzRSkn.md.png"></a></p>
</li>
</ul>
</li>
<li><p><a href="https://imgtu.com/i/gzRmkR"><img src="https://z3.ax1x.com/2021/05/25/gzRmkR.md.png" alt="gzRmkR.md.png"></a></p>
</li>
<li><p>三次排序</p>
<ul>
<li>一次快排，两次归并排序</li>
<li>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，<em><strong>当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序</strong></em>，并将这些有序数据溢写到磁盘上，<em><strong>而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。</strong></em></li>
<li>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。<em><strong>当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。</strong></em></li>
</ul>
</li>
<li><p>三个比较器（一个排序比较器，两个分组比较器）</p>
<ul>
<li>排序比较器</li>
<li>Reducer分组比较器</li>
<li>Combiner分组比较器</li>
</ul>
</li>
<li><p>Hadoop中所有比较器默认都是WritableComparator（默认调用key的compareTo()方法）</p>
</li>
<li><p>自定义排序的两种方法</p>
<ul>
<li>实现接口WritableComparable（该接口继承Writable,Comparable）</li>
<li>自定义一个类的专用比较器，在Job中覆盖Comparator，自定义的比较器要继承WritableComparator</li>
</ul>
</li>
<li><p>Combiner合并</p>
<ul>
<li>Combiner与Reducer的区别：Combiner是在每一个MapTask所在节点运行，Reducer是接受全局所有Mapper的输出结果。</li>
<li>Combiner对每一个MapTask的输出进行局部汇总，以减少IO（网络IO，和本地IO）。</li>
<li>Combiner默认不开启，要根据情况使用。能够应用的前提是不影响最终的业务逻辑。</li>
</ul>
</li>
<li><p>GroupComparator分组</p>
<ul>
<li>分组比较器，根据排好顺序的数据，根据Key值进行分组</li>
<li>对Reduce阶段的数据根据某一个或几个字段进行分组。</li>
<li>分组比较器默认也是WritableComparator（默认调用key的compareTo()方法），当排序和分组比较方案不同时，需要自定义其中的一个比较器。</li>
<li>自定义分组比较器步骤：（1）自定义类继承WritableComparator（2）重写compare()方法  （3）创建一个构造将比较对象的类传给父类</li>
</ul>
</li>
<li><p>MapReduce传输数据的方式</p>
<ul>
<li>为了提高传输效率，传输的是用Writable序列化好的序列化数据</li>
<li>数据在从Map出来进入到到环形缓冲区时进行序列化，进入环形缓冲区的数据已经完成了序列化。之后的数据流动都是序列化的数据的流动</li>
<li>之后的排序先反序列化再实现排序</li>
</ul>
</li>
<li><p>环形缓冲区</p>
<ul>
<li>一边写索引，一边写数据</li>
<li>在环形缓冲区进行排序时，不改变数据的位置，改变索引的位置</li>
</ul>
</li>
</ul>
<h3 id="OutputFormat"><a href="#OutputFormat" class="headerlink" title="OutputFormat"></a>OutputFormat</h3><ul>
<li><p>将Reduce处理完的K，V值持久化到文件</p>
</li>
<li><p>几种实现</p>
<ul>
<li>默认的文本输出TextOutputFormat，把每条记录写为文本行，利用K，V的toString()方法将其转为字符串。</li>
<li>SequenceFileOutputFormat:将K，V值序列化成文件放到磁盘上</li>
<li>自定义OutputFormat</li>
</ul>
</li>
<li><p>源码重要方法</p>
<ul>
<li>checkOutputSpecs()检查输出参数</li>
<li>getOutputCommitter()获取outputformat提交器（保证output被正确提交）</li>
<li>自定义outputformat主要实现RecordWriter()。实现接收K,V值并将K,V值处理</li>
</ul>
</li>
</ul>
<h3 id="MapReduce工作机制"><a href="#MapReduce工作机制" class="headerlink" title="MapReduce工作机制"></a>MapReduce工作机制</h3><ul>
<li><p>MapReduce哪些阶段可以进行压缩</p>
<ul>
<li>map的输入端（主要看数据大小和切片，lzo,bzip2）</li>
<li>map的输出端(主要考虑速度，snappy，Lzo)</li>
<li>reduce的输出端(看需求，是要继续做mapreduce的输出，还是直接做分析的数据等)</li>
</ul>
</li>
<li><p>MapTask工作机制</p>
<ul>
<li><p><a href="https://imgtu.com/i/gzWpHH"><img src="https://z3.ax1x.com/2021/05/25/gzWpHH.md.png" alt="gzWpHH.md.png"></a></p>
</li>
<li></li>
<li><p>（1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个</p>
</li>
<li><p>（2）Map阶段：该节点主要是将解析出的key&#x2F;value交给用户编写map()函数处理，并产生一系列新的key&#x2F;value。</p>
</li>
<li><p>（3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key&#x2F;value分区（调用Partitioner），并写入一个环形内存缓冲区中。</p>
</li>
<li><p>（4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>
<p>个key&#x2F;value。</p>
<p>​	溢写阶段详情：</p>
<p>​	步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</p>
<p>​	步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output&#x2F;spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p>
<p>​	步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output&#x2F;spillN.out.index中。</p>
<p>​	（5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p>
<p>​	当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output&#x2F;file.out中，同时生成相应的索引文件output&#x2F;file.out.index。</p>
<p>​	在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p>
<p>​	让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p>
<ul>
<li><p>ReduceTask阶段</p>
<p><a href="https://imgtu.com/i/gzWxGq"><img src="https://z3.ax1x.com/2021/05/25/gzWxGq.md.png" alt="gzWxGq.md.png"></a></p>
<p>（1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>
<p>​	（2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。</p>
<p>​	（3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。</p>
<p>​	（4）Reduce阶段：reduce()函数将计算结果写到HDFS上。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>​    </p>
<pre><code>### Join多种应用

* Reducejoin：

  - Map段主要工作：为来自不同表或文件的key/value对，***打标签以区别不同来源的记录***。然后用连接字段作为key ,其余部分和新加的标志作为value，最后进行输出。
  - Reduce端主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组中将***来源不同文件的记录（在Map阶段已经打标签）分开***，最后进行合并。
  - 缺点：这种方式中，合并操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载很低，资源利用率不高，且在Reduce阶段容易产生数据倾斜。

* Mapjoin

  1．使用场景

  Map Join适用于一张表十分小、一张表很大的场景。

  2．优点

  思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？

  在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。

  3．具体办法：采用DistributedCache 

  ​	（1）在Mapper的setup阶段，将文件读取到缓存集合中。

  ​	（2）在驱动函数中加载缓存。

  // 缓存普通文件到Task运行节点。

  job.addCacheFile(new URI(&quot;file://e:/cache/pd.txt&quot;));
</code></pre>
<p>​      </p>
<p>​    </p>
<h2 id="Yarn资源调度器"><a href="#Yarn资源调度器" class="headerlink" title="Yarn资源调度器"></a>Yarn资源调度器</h2><h3 id="Yarn基本架构和工作机制"><a href="#Yarn基本架构和工作机制" class="headerlink" title="Yarn基本架构和工作机制"></a>Yarn基本架构和工作机制</h3><ul>
<li><p>基本架构</p>
</li>
<li><p><a href="https://imgtu.com/i/2iAWPf"><img src="https://z3.ax1x.com/2021/05/27/2iAWPf.png" alt="2iAWPf.png"></a></p>
</li>
<li><p>Yarn工作流程</p>
<ul>
<li><a href="https://imgtu.com/i/2ieAJ0"><img src="https://z3.ax1x.com/2021/05/27/2ieAJ0.png" alt="2ieAJ0.png"></a></li>
</ul>
</li>
<li><p>Yarn工作步骤</p>
<ul>
<li><ol start="0">
<li>客户端执行程序job.waitForCompletion</li>
</ol>
</li>
<li><ol>
<li>申请一个Application</li>
</ol>
</li>
<li><ol start="2">
<li>返回Application资源提交临时路径和job_Id</li>
</ol>
</li>
<li><ol start="3">
<li>客户端提交job运行必要资源Job.split(分片信息)，Job.xml（配置信息），Wc.jar(所需的必要Jar包)。提交到HDFS</li>
</ol>
</li>
<li><ol start="4">
<li>资源提交完毕，申请运行AppMaster</li>
<li>ResourceManager将用户的请求初始化成一个Task资源申请,<strong>放入资源调度器等待调度</strong></li>
<li>资源调度器中的任务会分配到一个NodeManager中</li>
<li>NodeManager会根据任务创建一个容器，在容器中运行对应的AppMaster</li>
<li>AppMaster获取必要资源到本地</li>
<li>AppMaster再向ResourceManager请求运行MapTask资源申请</li>
<li>ResourceManager指定一个NodeManager领取MapTask任务，创建容器，运行MapTask</li>
<li>AppMaster向执行MapTask的NodeManager发送程序启动脚本</li>
<li>同理AppMaster向RM请求运行ReduceTask资源申请，创建容器运行ReduceTask</li>
<li>Reduce向Map获取相应的分区数据</li>
</ol>
</li>
</ul>
</li>
<li><p>资源调度器：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop3.1.3默认的资源调度器是Capacity Scheduler。</p>
<ul>
<li><p>FIFO，队列形式。基本不用</p>
</li>
<li><p>Capacity Scheduler，几个并行的FIFO。当集群资源不紧张时，采用此方式</p>
<p><a href="https://imgtu.com/i/2iGrQA"><img src="https://z3.ax1x.com/2021/05/27/2iGrQA.png" alt="2iGrQA.png"></a></p>
</li>
<li><p>Fair Scheduler　公平调度器。能最大程度利用集群的资源</p>
<p><a href="https://imgtu.com/i/2ilY4J"><img src="https://z3.ax1x.com/2021/05/27/2ilY4J.png" alt="2ilY4J.png"></a></p>
</li>
</ul>
</li>
</ul>
<h2 id="Hadoop常见优化"><a href="#Hadoop常见优化" class="headerlink" title="Hadoop常见优化"></a>Hadoop常见优化</h2><ul>
<li><p>数据倾斜问题:某个区域的数据量远远大于其它区域例如Map端和Reduce端的数据倾斜</p>
<ul>
<li>自定义分区，基于对原始数据的认知进行自定义分区。</li>
<li>Combine:Combine可以大量地减少数据倾斜。</li>
<li>采用Mapjoin，因为Reducejoin对Reduce端的压力非常大</li>
</ul>
</li>
<li><p>参数优化提升MapReduce 性能</p>
<ul>
<li><p>容错相关参数(MapReduce性能优化)</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.maxattempts</td>
<td>每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。</td>
</tr>
<tr>
<td>mapreduce.reduce.maxattempts</td>
<td>每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。</td>
</tr>
<tr>
<td>mapreduce.task.timeout</td>
<td>Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>小文件优化方法</p>
<ul>
<li>小文件的缺点：1.会占用namenode大量的内存资源。2.索引文件过大使得检索速度变慢。</li>
<li>优化：1.Sequence File:Sequence File由一系列的二进制key&#x2F;value组成，Input如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件。<ol start="2">
<li>CombineFileInputFormat:用于将多个文件合并成一个单独的split。</li>
</ol>
</li>
</ul>
</li>
<li><p>MapReduce 优化</p>
<ul>
<li>InputFormat阶段 ：1. 合并小文件</li>
<li>Map阶段：1.减少溢写（Spill）次数。2.减少合并（Merge）次数</li>
<li>Reduce阶段<ol>
<li>合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map,Reduce任务竞争资源，造成处理超时等待。</li>
<li>设置Map,Reduce共存，调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间。</li>
<li>规避使用Reduce，因为Reduce在用于连接数据集的时候会产生大量的网络消耗。</li>
<li>合理设置Reduce端的Buffer</li>
</ol>
</li>
<li>OutputFormat阶段<ol>
<li>采用数据压缩</li>
<li>使用SequenceFile二进制文件</li>
</ol>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>JVM</title>
    <url>/2021/05/02/JVM/</url>
    <content><![CDATA[<h1 id="JVM"><a href="#JVM" class="headerlink" title="JVM"></a>JVM</h1><h2 id="JVM定义"><a href="#JVM定义" class="headerlink" title="JVM定义"></a>JVM定义</h2><blockquote>
<ul>
<li>JVM是运行在操作系统之上的，它与硬件没有直接的交互</li>
<li>通过编译器将 Java 程序转换成该虚拟机所能识别的指令序列，也称 Java 字节码。Java虚拟机会将字节码，即class文件加载到JVM中。由JVM进行解释和执行</li>
</ul>
</blockquote>
<h2 id="类加载器"><a href="#类加载器" class="headerlink" title="类加载器"></a>类加载器</h2><blockquote>
<ul>
<li>类加载器，即ClassLoader,它负责加载class文件，class文件在文件开头有特定的文件标示，并且ClassLoader只负责class文件的加载，至于它是否可以运行，则由Execution Engine决定</li>
</ul>
</blockquote>
<h3 id="类加载器分类"><a href="#类加载器分类" class="headerlink" title="类加载器分类"></a>类加载器分类</h3><h4 id="虚拟机自带的类加载器"><a href="#虚拟机自带的类加载器" class="headerlink" title="虚拟机自带的类加载器"></a>虚拟机自带的类加载器</h4><ol>
<li>启动类加载器：主要负责加载jre中的最为基础、最为重要的类。如$JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;rt.jar等，以及由虚拟机参数 -Xbootclasspath 指定的类。由于它由C++代码实现，没有对应的java对象，因此在java中，尝试获取此类时，只能使用null来指代。</li>
<li>扩展类加载器：由Java代码实现，用于加载相对次要、但又通用的类，比如存放在 JRE 的 lib&#x2F;ext 目录下 jar 包中的类，以及由系统变量 java.ext.dirs 指定的类。如$JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;ext&#x2F;*.jar。</li>
<li>应用程序类加载器：由Java代码实现， 它负责加载应用程序路径下的类。（这里的应用程序路径，便是指虚拟机参数 -cp&#x2F;-classpath、系统变量 java.class.path 或环境变量 CLASSPATH 所指定的路径。）默认情况下，应用程序中包含的类便是由应用类加载器加载的。</li>
</ol>
<h4 id="用户自定义的加载器"><a href="#用户自定义的加载器" class="headerlink" title="用户自定义的加载器"></a>用户自定义的加载器</h4><blockquote>
<ul>
<li>Java.lang.ClassLoader的子类，用户可以定制类的加载方式。例如可以对 class 文件进行加密，加载时再利用自定义的类加载器对其解密。</li>
</ul>
</blockquote>
<h3 id="双亲委派机制"><a href="#双亲委派机制" class="headerlink" title="双亲委派机制"></a>双亲委派机制</h3><blockquote>
<ul>
<li>双亲委派模型：每当一个类加载器接收到加载请求时，它会先将请求转发给父类加载器。在父类加载器没有找到所请求的类的情况下，该类加载器才会尝试去加载。</li>
<li>应用程序类加载器的父类是扩展类加载器，扩展类加载器的父类是启动类加载器</li>
<li>优点：1.避免类的重复加载。2.防止核心API中定义的类型不会被用户恶意替换和篡改</li>
</ul>
</blockquote>
<h2 id="JVM的内存模型"><a href="#JVM的内存模型" class="headerlink" title="JVM的内存模型"></a>JVM的内存模型</h2><p><a href="https://imgtu.com/i/gZ2GDO" title="内存模型图片"><img src="https://z3.ax1x.com/2021/05/02/gZ2GDO.png" alt="gZ2GDO.png"></a></p>
<ul>
<li>Java虚拟机将运行时内存区域划分为五个部分，分别为<font color=red>方法区，堆，PC寄存器，Java方法栈和本地方法栈</font>。</li>
<li>执行Java代码首先需要使用类加载器将它编译成而成的class文件加载到Java虚拟机中。加载后的Java类会被存放于方法区（Method Area）中。实际运行时，虚拟机会执行方法区内的代码。</li>
<li>在虚拟机中，方法区和堆为线程共享，也是垃圾回收的重点照顾区域。栈空间为线程私有，基本不会出现垃圾回收。</li>
<li>Java虚拟机将栈细分为面向Java方法的Java方法栈，面向本地方法（c++写的native方法）的本地方法栈，以及存放各个执行线程执行位置的PC寄存器（程序计数器）</li>
<li>在运行过程中，每当调用进入一个Java方法，Java虚拟机会在当前线程的Java方法栈中生成一个栈帧（栈的一片区域），用以存放局部变量以及字节码的操作数。这个栈帧的大小是提前计算好的，而且Java虚拟机不要求栈帧在内存空间里连续分布。当推出当前执行的方法时，不管是正常返回还是异常返回，Java虚拟机均会弹出当前线程的当前栈帧，并将之舍弃。</li>
</ul>
<blockquote>
<ul>
<li>Execution Engine执行引擎负责解释命令，提交操作系统执行</li>
<li>Native Method Stack:定义了很多调用本地操作系统的方法，也称之为本地方法接口</li>
<li>每个线程都有一个程序计数器，是线程私有的,就是一个指针</li>
<li>方法区：所有定义的方法的信息都保存在该区域，此区属于共享区间。<br>静态变量+常量+类信息(构造方法&#x2F;接口定义)+运行时常量池存在方法区中。</li>
<li>JDK1.7之前通过永久代实现方法区，1.7之前字符串常量池放到方法区中</li>
<li>JDK1.8之后，通过元空间实现方法区，1.7之后字符串常量放到堆中</li>
</ul>
</blockquote>
<h3 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h3><blockquote>
<ul>
<li>栈我们也叫内存，是线程私有的，生命周期随线程的生命周期，线程结束栈内存释放</li>
<li>栈：8种基本类型的变量+对象的引用变量+实例方法都是在栈内存中分配</li>
<li>在栈区域规定了两种异常状态：如果线程请求的栈深度大于虚拟机所允许的深度，则抛出StackOverflowError异常；如果虚拟机栈可以动态扩展，在扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。</li>
<li>栈帧：一个线程的每个方法在调用时都会在栈上划分一块区域，用于存储方法所需要的变量等信息，这块区域称之为栈帧（stack frame）。栈由多个栈帧构成，好比一部电影由多个帧的画面构成。</li>
</ul>
</blockquote>
<h3 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h3><ul>
<li>堆内存:存储的是数组和对象（其实数组就是对象），凡是new建立的都是在堆中，堆中存放的都是实体（对象），实体用于封装数据，而且是封装多个（实体的多个属性），如果一个数据消失，这个实体也没有消失，还可以用，所以堆是不会随时释放的，但是栈不一样，栈里存放的都是单个变量，变量被释放了，那就没有了。堆里的实体虽然不会被释放，但是会被当成垃圾，Java有垃圾回收机制不定时的收取。</li>
</ul>
<blockquote>
<ul>
<li>逻辑上分为三部分：1.新生区。2.养老区。3.永久区。（1.8后改为元空间）</li>
<li>新生区进一步分为：<blockquote>
<ul>
<li>伊甸园区</li>
<li>幸存区<blockquote>
<ul>
<li>幸存from区</li>
<li>幸存to区</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
</li>
<li>在物理上划分，堆被划分成两个不同的区域：新生代 ( Young )、老年代 ( Old )。新生代 ( Young ) 又被划分为三个区域：Eden、From Survivor、To Survivor</li>
<li>创建对象的过程<blockquote>
<ul>
<li>新new的对象会放在伊甸园区（大对象直接进入老年代），伊甸园区的对象存活率非常低，当伊甸园区快满时会触发轻量级的垃圾回收机制（MinorGC）MinorGC会回收伊甸园区和幸存from区，会将伊甸园的幸存者标记复制到幸存to区，from区中的幸存者会根据它的年龄判断它的去向：默认情况下，如果年龄小于15则被标记到to区，如果年龄大于15则被标记复制到养老区，然后from区和to区交换角色（to区又为空了，下次又是回收from区的)</li>
<li>当养老区内存不足时会触发重量级垃圾回收机制（MajorGC&#x2F;fullGC），如果养老区无法回收内存则会出现OOM异常</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h3 id="JVM常见参数设置"><a href="#JVM常见参数设置" class="headerlink" title="JVM常见参数设置"></a>JVM常见参数设置</h3><blockquote>
<ul>
<li>-Xms:堆初始值（默认为物理内存的1&#x2F;64）</li>
<li>-Xmx:堆最大可用值（默认为物理内存的1&#x2F;4）</li>
<li>-Xss:每个线程的栈大小，默认为1M，此值不能设置过大，否则会减少线程并发数。</li>
</ul>
</blockquote>
<h3 id="常见异常"><a href="#常见异常" class="headerlink" title="常见异常"></a>常见异常</h3><blockquote>
<ul>
<li>错误原因: java.lang.OutOfMemoryError: Java heap space 堆内存溢出</li>
<li>解决办法:调大堆内存大小 </li>
<li>错误原因: java.lang.StackOverflowError表示为栈溢出，一般产生于递归调用。</li>
<li>解决办法:设置线程最大调用深度，默认是1m</li>
</ul>
</blockquote>
<h2 id="GC"><a href="#GC" class="headerlink" title="GC"></a>GC</h2><blockquote>
<ul>
<li>GC:JVM中的Garbage Collection，简称GC，它会不定时去堆内存中清理不可达对象。</li>
<li>分类：两种类型，一种是普通GC（minor GC），一种是全局GC（major GC or Full GC）</li>
<li>　新生代GC（minor GC）：只针对新生代区域的GC。</li>
<li>老年代GC（major GC or Full GC）：针对老年代的GC，偶尔伴随对新生代的GC以及对永久代的GC。</li>
<li>Minor GC触发机制：当年轻代满时就会触发Minor GC，这里的年轻代满指的是Eden区满，Survivor满不会引发GC。</li>
<li>Full GC触发机制：当年老代满时会引发Full GC，Full GC将会同时回收年轻代、年老代，当永久代满时也会引发Full GC，会导致Class、Method元信息的卸载</li>
<li>工作特点：理论上GC过程中会频繁收集Young区，很少收集Old区，基本不动Perm区（元空间&#x2F;方法区）</li>
</ul>
</blockquote>
<h3 id="标记不可达对象"><a href="#标记不可达对象" class="headerlink" title="标记不可达对象"></a>标记不可达对象</h3><blockquote>
<ul>
<li>引用计数法：引用计数法就是如果一个对象没有被任何引用指向，则可视之为垃圾。这种方法的缺点就是不能检测到循环指向的存在。</li>
<li>可达性分析（GC ROOTS算法）简单理解，可以理解为堆外指向堆内的引用</li>
</ul>
</blockquote>
<h3 id="垃圾回收的三种方式"><a href="#垃圾回收的三种方式" class="headerlink" title="垃圾回收的三种方式"></a>垃圾回收的三种方式</h3><ol>
<li>清除：第一种是清除（sweep），即把死亡对象所占据的内存标记为空闲内存，并记录在一个空闲列表（free list）之中</li>
<li>压缩：第二种是压缩（compact），即把存活的对象聚集到内存区域的起始位置，从而留下一段连续的内存空间</li>
<li>复制第三种则是复制（copy），即把内存区域分为两等分，分别用两个指针 from 和 to 来维护，并且只是用 from 指针指向的内存区域来分配内<blockquote>
<ul>
<li>总结：回收死亡对象的内存共有三种方式，分别为：会造成内存碎片的清除、性能开销较大的压缩、以及堆使用效率较低的复制。</li>
</ul>
</blockquote>
</li>
</ol>
<h3 id="垃圾回收算法"><a href="#垃圾回收算法" class="headerlink" title="垃圾回收算法"></a>垃圾回收算法</h3><ol>
<li>标记复制算法 因此Minor GC使用的则是标记-复制算法，理性情况下：，Eden 区中的对象基本都死亡了，那么需要复制的数据将非常少，因此采用这种标记 - 复制算法的效果极好。<blockquote>
<ul>
<li>优点：不会产生内存碎片 </li>
<li>缺点：需要双倍空间，浪费内存</li>
</ul>
</blockquote>
</li>
<li>标记清除算法 老年代一般是由标记清除或者是标记清除与标记压缩的混合实现<blockquote>
<ul>
<li>优点：不需要双倍空间</li>
<li>缺点：1.会产生内存碎片 2.需要停止整个应用程序 3.需要维护内存碎片的地址列表</li>
</ul>
</blockquote>
</li>
<li>标记压缩算法<blockquote>
<ul>
<li>优点：1.不需要双倍空间，也不会产生内存碎片 2.不需要维护内存碎片的列表，只需要记录内存的起始地址即可 </li>
<li>缺点：开销大，需要更新对象的地址</li>
</ul>
</blockquote>
</li>
<li>标记清除压缩算法 标记清除压缩(Mark-Sweep-Compact)算法是标记清除算法和标记压缩算法的结合算法。其原理和标记清除算法一致，只不过会在多次GC后，进行一次Compact压缩操作！</li>
</ol>
<h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="JVM内存分几个区，每个区的作用是什么？"><a href="#JVM内存分几个区，每个区的作用是什么？" class="headerlink" title="JVM内存分几个区，每个区的作用是什么？"></a>JVM内存分几个区，每个区的作用是什么？</h3><ul>
<li>方法区<ul>
<li>有时候也称为永久代，该区很少发生垃圾回收，在这里进行GC主要是对方法区里的常量池和对类型的卸载。</li>
<li>方法区用来存放已经被虚拟机加载的类信息，常量，静态变量和即时编译器编译后的代码等数据。</li>
<li>该区域是被线程共享的。</li>
<li>静态变量+常量+类信息(构造方法&#x2F;接口定义)+运行时常量池存在方法区中</li>
</ul>
</li>
<li>Java栈<ul>
<li>8种基本类型的变量+对象的引用变量+实例方法都是在栈内存中分配</li>
<li>是线程私有的，它的生命周期与线程相同</li>
<li>在栈区域规定了两种异常状态：如果线程请求的栈深度大于虚拟机所允许的深度，则抛出StackOverflowError异常；如果虚拟机栈可以动态扩展，在扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。</li>
</ul>
</li>
<li>本地方法栈<ul>
<li>本地方法栈和Java栈类似，只不过本地方法栈为Native方法服务</li>
</ul>
</li>
<li>堆<ul>
<li>堆是所有线程所共享的一块内存，在虚拟机启动时创建，几乎所有的对象实例都在这里创建，因此该区域经常发生垃圾回收操作。</li>
<li>堆区被所有线程共享</li>
</ul>
</li>
<li>程序计数器<ul>
<li>内存空间小，字节码解释器工作时通过改变这个计数值可以选取下一条需要执行的字节码指令。该内存区域是唯一一个Java虚拟机规范没有规定任何OOM情况的区域。</li>
<li>线程私有的</li>
</ul>
</li>
</ul>
<h3 id="Java类加载过程"><a href="#Java类加载过程" class="headerlink" title="Java类加载过程"></a>Java类加载过程</h3><ul>
<li>加载</li>
<li>验证</li>
<li>解析</li>
<li>初始化</li>
</ul>
<h3 id="如何判断一个对象是否存活"><a href="#如何判断一个对象是否存活" class="headerlink" title="如何判断一个对象是否存活"></a>如何判断一个对象是否存活</h3><ul>
<li>引用计数法<ul>
<li>引用计数法就是如果一个对象没有被任何引用指向，则视之为垃圾。这种方法的缺点就是不能检测到循环指向的存在。</li>
</ul>
</li>
<li>可达性算法（引用链法）</li>
<li>根据搜索算法的基本思路就是通过一系列名为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索走过的路径称为引用链（Reference Chain），当一个对象到GC Roots没有任何引用链时，则证明此对象时不可用的。</li>
</ul>
<h3 id="Java中的垃圾收集的方法有哪些"><a href="#Java中的垃圾收集的方法有哪些" class="headerlink" title="Java中的垃圾收集的方法有哪些"></a>Java中的垃圾收集的方法有哪些</h3><ul>
<li>在上面</li>
</ul>
<h3 id="什么是类加载器，类加载器有哪些"><a href="#什么是类加载器，类加载器有哪些" class="headerlink" title="什么是类加载器，类加载器有哪些"></a>什么是类加载器，类加载器有哪些</h3><ul>
<li><p>类加载器，即ClassLoader,它负责加载class文件，class文件在文件开头有特定的文件标示，并且ClassLoader只负责class文件的加载，至于它是否可以运行，则由Execution Engine决定</p>
</li>
<li><p>在上面</p>
</li>
</ul>
<h3 id="简述Java内存分配以及垃圾回收策略Minor-GC，Major-GC（Full-GC）"><a href="#简述Java内存分配以及垃圾回收策略Minor-GC，Major-GC（Full-GC）" class="headerlink" title="简述Java内存分配以及垃圾回收策略Minor GC，Major GC（Full GC）"></a>简述Java内存分配以及垃圾回收策略Minor GC，Major GC（Full GC）</h3><ul>
<li>在上面</li>
</ul>
<h2 id="JUC"><a href="#JUC" class="headerlink" title="JUC"></a>JUC</h2><ul>
<li><p>线程和进程</p>
<ul>
<li>程序(program)是为完成特定任务，用某种语言编写的一组指令的集合。即指一段静态的代码。</li>
<li>进程(process)是程序的一次执行过程，或是正在运行的一个程序。进程是一个动态的过程，即有它自身的产生，存在和消亡的过程。每个Java程序都有一个隐含的主程序，即main 方法</li>
<li>线程（thread）是进程内部的一条具体的执行路径。若一个程序可同一时间执行多个线程，就是支持多线程的。</li>
<li>总结：程序是静态的，程序运行后变为一个进程，一个进程内部可以有多个线程同时执行。进程是所有线程的集合，每一个线程是进程中的一条路径。线程是直接去竞争cpu资源的。</li>
</ul>
</li>
<li><p>线程安全</p>
<ul>
<li>当多个线程同时共享同一个全局变量或静态变量，<font color=red>做写的操作时</font>，可能会发生数据冲突问题，也就是线程安全问题。</li>
</ul>
</li>
<li><p>线程安全的解决方式</p>
<ul>
<li><p>使用多线程之间同步或使用锁（lock）可以解决线程安全问题。</p>
</li>
<li><p>使用同步代码块</p>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/*</span><br><span class="hljs-comment">synchronized(同一个对象)&#123;</span><br><span class="hljs-comment">	可能会发生线程冲突问题</span><br><span class="hljs-comment">&#125;</span><br><span class="hljs-comment">*/</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Ticket</span><br>&#123;<br>	<span class="hljs-keyword">private</span> <span class="hljs-type">int</span> <span class="hljs-variable">number</span> <span class="hljs-operator">=</span> <span class="hljs-number">30</span>;<br>	<br>	<br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">sale</span><span class="hljs-params">()</span> &#123;<br>		<br>		<span class="hljs-keyword">if</span> (number &gt; <span class="hljs-number">0</span>) &#123;<br>			System.out.println(Thread.currentThread().getName() + <span class="hljs-string">&quot;卖出第：\t&quot;</span> + (number--) + <span class="hljs-string">&quot;\t 还剩下：&quot;</span> + number);<br>		&#125;<br>	&#125;<br><br>&#125;<br><br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">SaleTicket</span> <br>&#123;<br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span><br>	&#123;<br>		<span class="hljs-comment">//创建资源对象</span><br>		<span class="hljs-type">Ticket</span> <span class="hljs-variable">tc</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Ticket</span>();<br>		<span class="hljs-comment">//创建AA线程、</span><br>		<span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Runnable</span>() &#123;<br>			<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">run</span><span class="hljs-params">()</span> &#123;<br>				<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">40</span>; i++) &#123;<br>					<span class="hljs-comment">//卖票</span><br>					tc.sale();<br>				&#125;<br>			&#125;<br>		&#125;, <span class="hljs-string">&quot;AA&quot;</span>).start();<br>		<span class="hljs-comment">//创建BB线程、</span><br>		<span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Runnable</span>() &#123;<br>			<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">run</span><span class="hljs-params">()</span> &#123;<br>				<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">40</span>; i++) &#123;<br>					<span class="hljs-comment">//卖票</span><br>					tc.sale();<br>				&#125;<br>			&#125;<br>		&#125;, <span class="hljs-string">&quot;BB&quot;</span>).start();<br>		<span class="hljs-comment">//创建CC线程、</span><br>		<span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Runnable</span>() &#123;<br>			<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">run</span><span class="hljs-params">()</span> &#123;<br>				<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">40</span>; i++) &#123;<br>					<span class="hljs-comment">//卖票</span><br>					tc.sale();<br>				&#125;<br>			&#125;<br>		&#125;, <span class="hljs-string">&quot;CC&quot;</span>).start();<br>	&#125;<br>&#125;<br><br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>使用Lock解决线程安全</p>
<ul>
<li><p>java.util.concurrent.locks.Lock接口是控制多个线程对共享资源进行访问的工具。Lock实现提供更广泛的锁定操作可以比使用 synchronized获得方法和声明更好。</p>
</li>
<li><p>相比于synchronized的有系统获取锁和释放锁，Lock需要自己动手实现加锁和释放锁，因此会更加灵活。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Ticket</span><br>&#123;<br>	<span class="hljs-keyword">private</span> <span class="hljs-type">int</span> <span class="hljs-variable">number</span> <span class="hljs-operator">=</span> <span class="hljs-number">30</span>;<br>	<br>	<span class="hljs-comment">//创建锁</span><br>	<span class="hljs-type">Lock</span> <span class="hljs-variable">lock</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ReentrantLock</span>();<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">sale</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-comment">//上锁</span><br>		lock.lock();<br>		<span class="hljs-keyword">try</span> &#123;<br>			<span class="hljs-keyword">if</span> (number &gt; <span class="hljs-number">0</span>) &#123;<br>				System.out.println(Thread.currentThread().getName() + <span class="hljs-string">&quot;卖出第：\t&quot;</span> + (number--) + <span class="hljs-string">&quot;\t 还剩下：&quot;</span> + number);<br>			&#125;<br>		&#125; <span class="hljs-keyword">finally</span> &#123;<br>			<span class="hljs-comment">//解锁</span><br>			lock.unlock();<br>		&#125;<br>		<br>	&#125;<br>&#125;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">SaleTicket</span> <br>&#123;<br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span><span class="hljs-comment">//main所有程序的入口</span><br>	&#123;<br>		<span class="hljs-comment">//创建资源对象</span><br>		<span class="hljs-type">Ticket</span> <span class="hljs-variable">tc</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Ticket</span>();<br>		<span class="hljs-comment">//创建AA线程、</span><br>		<span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Runnable</span>() &#123;<br>			<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">run</span><span class="hljs-params">()</span> &#123;<br>				<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">40</span>; i++) &#123;<br>					<span class="hljs-comment">//卖票</span><br>					tc.sale();<br>				&#125;<br>			&#125;<br>		&#125;, <span class="hljs-string">&quot;AA&quot;</span>).start();<br>		<span class="hljs-comment">//创建BB线程、</span><br>		<span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Runnable</span>() &#123;<br>			<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">run</span><span class="hljs-params">()</span> &#123;<br>				<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">40</span>; i++) &#123;<br>					<span class="hljs-comment">//卖票</span><br>					tc.sale();<br>				&#125;<br>			&#125;<br>		&#125;, <span class="hljs-string">&quot;BB&quot;</span>).start();<br>		<span class="hljs-comment">//创建CC线程、</span><br>		<span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Runnable</span>() &#123;<br>			<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">run</span><span class="hljs-params">()</span> &#123;<br>				<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">40</span>; i++) &#123;<br>					<span class="hljs-comment">//卖票</span><br>					tc.sale();<br>				&#125;<br>			&#125;<br>		&#125;, <span class="hljs-string">&quot;CC&quot;</span>).start();<br>	&#125;<br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="多线程的创建"><a href="#多线程的创建" class="headerlink" title="多线程的创建"></a>多线程的创建</h3><ul>
<li>继承Thread类<ul>
<li>定义子类继承Thread类</li>
<li>子类重写Thread类中的run方法</li>
<li>创建Thread子类对象，即创建了线程对象</li>
<li>调用线程对象start方法启动线程，默认调用run方法<ul>
<li>注意：如果只是调用run方法，则此时会在调用该方法的线程中来执行，而不是另启动一个线程</li>
</ul>
</li>
</ul>
</li>
<li>实现Runnable接口<ul>
<li>定义子类，实现Runnable接口</li>
<li>子类中重写Runnable接口中的run 方法</li>
<li>通过Thread类含参构造器创建线程对象，将Runnable接口的子类对象作为实际参数传递给Thread类的构造方法中。</li>
<li>调用Thread类的start方法启动线程，其最终调用Runnable子类接口的run方法。</li>
</ul>
</li>
<li>实现Runnable接口避免了单继承的局限性，多个线程可以共享同一个接口子类的对象，非常适合多个相同线程来处理同一份资源。</li>
<li>使用Callable 接口</li>
<li>与Runnable比较：<ul>
<li>相比于run方法可以有返回值</li>
<li>可以抛出异常</li>
<li>支持泛型的返回值</li>
<li>落地方法是call方法</li>
</ul>
</li>
<li>分布式锁<ul>
<li><font color=red>当在分布式模型下，数据只有一份（或有限制），此时需要利用锁的技术控制某一时刻修改数据的进程数。</font>分布式锁可以将标记存在内存，只是该内存不是某个进程分配的内存而是公共内存，如 Redis，通过set (key,value,nx,px,timeout)方法添加分布式锁。</li>
</ul>
</li>
<li>分布式事务<ul>
<li>分布式事务指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>Java网络编程基础</title>
    <url>/2021/04/23/Java-network/</url>
    <content><![CDATA[<h1 id="Java网络编程基础"><a href="#Java网络编程基础" class="headerlink" title="Java网络编程基础"></a>Java网络编程基础</h1><h2 id="实例代码"><a href="#实例代码" class="headerlink" title="实例代码"></a>实例代码</h2><figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">package</span> network;<br><br><span class="hljs-keyword">import</span> java.io.BufferedReader;<br><span class="hljs-keyword">import</span> java.io.BufferedWriter;<br><span class="hljs-keyword">import</span> java.io.FileInputStream;<br><span class="hljs-keyword">import</span> java.io.FileOutputStream;<br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-keyword">import</span> java.io.InputStream;<br><span class="hljs-keyword">import</span> java.io.InputStreamReader;<br><span class="hljs-keyword">import</span> java.io.OutputStream;<br><span class="hljs-keyword">import</span> java.io.OutputStreamWriter;<br><span class="hljs-keyword">import</span> java.net.ServerSocket;<br><span class="hljs-keyword">import</span> java.net.Socket;<br><span class="hljs-keyword">import</span> java.time.LocalDateTime;<br><br><span class="hljs-keyword">import</span> org.junit.Test;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">NetTest</span> &#123;<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">server3</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-type">ServerSocket</span> <span class="hljs-variable">server</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>		<span class="hljs-keyword">try</span> &#123;<br>			server = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ServerSocket</span>(<span class="hljs-number">8888</span>); <span class="hljs-comment">// 绑定端口</span><br>			<span class="hljs-keyword">while</span> (<span class="hljs-literal">true</span>) &#123;<br>				System.out.println(<span class="hljs-string">&quot;服务器在8888端口监听中.....&quot;</span>);<br>				<span class="hljs-keyword">final</span> <span class="hljs-type">Socket</span> <span class="hljs-variable">socket1</span> <span class="hljs-operator">=</span> server.accept();<br>				;<br>				<span class="hljs-type">Runnable</span> <span class="hljs-variable">runner</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Runnable</span>() &#123;<br>					<span class="hljs-meta">@Override</span><br>					<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">run</span><span class="hljs-params">()</span> &#123;<br>						<span class="hljs-type">BufferedWriter</span> <span class="hljs-variable">bufferedWriter</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>						<span class="hljs-keyword">try</span> &#123;<br>							bufferedWriter = <span class="hljs-keyword">new</span> <span class="hljs-title class_">BufferedWriter</span>(<span class="hljs-keyword">new</span> <span class="hljs-title class_">OutputStreamWriter</span>(socket1.getOutputStream()));<br>							bufferedWriter.write(<span class="hljs-string">&quot;我是服务器, 现在时间 : &quot;</span> + LocalDateTime.now());<br>							bufferedWriter.newLine();<br>							bufferedWriter.flush(); <span class="hljs-comment">// 把数据真的刷入网线中</span><br>						&#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;<br>							e.printStackTrace();<br>						&#125; <span class="hljs-keyword">finally</span> &#123;<br>							<span class="hljs-keyword">if</span> (bufferedWriter != <span class="hljs-literal">null</span>) &#123;<br>								<span class="hljs-keyword">try</span> &#123;<br>									bufferedWriter.close();<br>								&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>								&#125;<br>							&#125;<br>							<span class="hljs-keyword">if</span> (socket1 != <span class="hljs-literal">null</span>) &#123;<br>								<span class="hljs-keyword">try</span> &#123;<br>									socket1.close();<br>								&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>								&#125;<br>							&#125;<br>						&#125;<br>						<span class="hljs-keyword">try</span> &#123;<br>							Thread.sleep(<span class="hljs-number">5000</span>);<br>						&#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;<br>							e.printStackTrace();<br>						&#125;<br>					&#125;<br>				&#125;;<br>				<span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(runner).start();<br>			&#125;<br>		&#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">finally</span> &#123;<br><br>			<span class="hljs-keyword">if</span> (server != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					System.out.println(<span class="hljs-string">&quot;服务器关闭....&quot;</span>);<br>					server.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br>		&#125;<br>	&#125;<br><br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">client3</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-type">Socket</span> <span class="hljs-variable">socket2</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>		<span class="hljs-type">BufferedReader</span> <span class="hljs-variable">bufferedReader</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>		<span class="hljs-keyword">try</span> &#123;<br>			socket2 = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Socket</span>(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">8888</span>);<br>			bufferedReader = <span class="hljs-keyword">new</span> <span class="hljs-title class_">BufferedReader</span>(<span class="hljs-keyword">new</span> <span class="hljs-title class_">InputStreamReader</span>(socket2.getInputStream()));<br>			<span class="hljs-type">String</span> <span class="hljs-variable">readLine</span> <span class="hljs-operator">=</span> bufferedReader.readLine();<br>			System.out.println(readLine);<br>		&#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">finally</span> &#123;<br>			<span class="hljs-keyword">if</span> (bufferedReader != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					bufferedReader.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br>			<span class="hljs-keyword">if</span> (socket2 != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					socket2.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br>		&#125;<br>	&#125;<br><br>	<span class="hljs-comment">// 从客户端发送文件给服务端，服务端保存到本地。并返回“发送成功”给客户端。并关闭相应的连接。</span><br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">server2</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-type">ServerSocket</span> <span class="hljs-variable">server</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>		<span class="hljs-type">Socket</span> <span class="hljs-variable">socket1</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<span class="hljs-comment">// 网络套接字</span><br><br>		<span class="hljs-type">InputStream</span> <span class="hljs-variable">nis</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>		<span class="hljs-type">FileOutputStream</span> <span class="hljs-variable">fos</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>		<span class="hljs-type">BufferedWriter</span> <span class="hljs-variable">netWriter</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br><br>		<span class="hljs-keyword">try</span> &#123;<br>			server = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ServerSocket</span>(<span class="hljs-number">7777</span>);<span class="hljs-comment">// 指定服务器接口</span><br>			socket1 = server.accept();<br>			nis = socket1.getInputStream();<br>			fos = <span class="hljs-keyword">new</span> <span class="hljs-title class_">FileOutputStream</span>(<span class="hljs-string">&quot;朴树 - 猎户星座2.mp3&quot;</span>);<br>			netWriter = <span class="hljs-keyword">new</span> <span class="hljs-title class_">BufferedWriter</span>(<span class="hljs-keyword">new</span> <span class="hljs-title class_">OutputStreamWriter</span>(socket1.getOutputStream()));<br><br>			<span class="hljs-type">byte</span>[] buf = <span class="hljs-keyword">new</span> <span class="hljs-title class_">byte</span>[<span class="hljs-number">8192</span>];<br>			<span class="hljs-type">int</span> <span class="hljs-variable">realCount</span> <span class="hljs-operator">=</span> nis.read(buf);<br>			<span class="hljs-keyword">while</span> (realCount != -<span class="hljs-number">1</span>) &#123;<br>				fos.write(buf, <span class="hljs-number">0</span>, realCount);<br>				realCount = nis.read(buf);<br>			&#125;<br>			netWriter.write(<span class="hljs-string">&quot;发送成功&quot;</span>);<br>			netWriter.newLine();<br>			netWriter.flush();<br><br>		&#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">finally</span> &#123;<br>			<span class="hljs-keyword">if</span> (netWriter != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					netWriter.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br>			<span class="hljs-keyword">if</span> (fos != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					fos.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br>			<span class="hljs-keyword">if</span> (nis != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					nis.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br>			<span class="hljs-keyword">if</span> (socket1 != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					socket1.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br>			<span class="hljs-keyword">if</span> (server != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					server.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br>		&#125;<br>	&#125;<br><br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">client2</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-type">Socket</span> <span class="hljs-variable">socket2</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<span class="hljs-comment">// 网络套接字</span><br><br>		<span class="hljs-type">FileInputStream</span> <span class="hljs-variable">fis</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<span class="hljs-comment">// 文件输入流</span><br>		<span class="hljs-type">OutputStream</span> <span class="hljs-variable">nos</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<span class="hljs-comment">// 网络输出流</span><br>		<span class="hljs-type">BufferedReader</span> <span class="hljs-variable">netReader</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<span class="hljs-comment">// 网络输入流</span><br>		<span class="hljs-keyword">try</span> &#123;<br>			socket2 = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Socket</span>(<span class="hljs-string">&quot;127.0.0.1&quot;</span>, <span class="hljs-number">7777</span>);<br>			fis = <span class="hljs-keyword">new</span> <span class="hljs-title class_">FileInputStream</span>(<span class="hljs-string">&quot;src/朴树 - 猎户星座.mp3&quot;</span>);<span class="hljs-comment">// 要传到服务器端的文件，先读入到客户端</span><br>			nos = socket2.getOutputStream();<span class="hljs-comment">// 传输到服务器端的文件输出流</span><br>			netReader = <span class="hljs-keyword">new</span> <span class="hljs-title class_">BufferedReader</span>(<span class="hljs-keyword">new</span> <span class="hljs-title class_">InputStreamReader</span>(socket2.getInputStream()));<span class="hljs-comment">// 读取服务器端返回的数据</span><br><br>			<span class="hljs-type">byte</span>[] buf = <span class="hljs-keyword">new</span> <span class="hljs-title class_">byte</span>[<span class="hljs-number">8192</span>];<br>			<span class="hljs-comment">// 读取数据</span><br>			<span class="hljs-type">int</span> <span class="hljs-variable">realCount</span> <span class="hljs-operator">=</span> fis.read(buf);<br>			<span class="hljs-keyword">while</span> (realCount != -<span class="hljs-number">1</span>) &#123;<br>				<span class="hljs-comment">// 1) 处理已经读的数据</span><br>				nos.write(buf, <span class="hljs-number">0</span>, realCount);<br>				<span class="hljs-comment">// 2) 继续读</span><br>				realCount = fis.read(buf);<br>			&#125;<br>			nos.flush();<span class="hljs-comment">// 将在缓存中的数据全部输出去</span><br>			socket2.shutdownOutput();<span class="hljs-comment">// 关闭输出流</span><br><br>			<span class="hljs-type">String</span> <span class="hljs-variable">readLine</span> <span class="hljs-operator">=</span> netReader.readLine();<span class="hljs-comment">// 接受服务器端的数据</span><br>			System.out.println(readLine);<br><br>		&#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">finally</span> &#123;<span class="hljs-comment">// 关闭相应的资源</span><br>			<span class="hljs-keyword">if</span> (netReader != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					netReader.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br>			<span class="hljs-keyword">if</span> (nos != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					nos.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br>			<span class="hljs-keyword">if</span> (fis != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					fis.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br>			<span class="hljs-keyword">if</span> (socket2 != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					socket2.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br>		&#125;<br>	&#125;<br><br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">server</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-type">ServerSocket</span> <span class="hljs-variable">server</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>		<span class="hljs-type">Socket</span> <span class="hljs-variable">socket1</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>		<span class="hljs-type">BufferedReader</span> <span class="hljs-variable">bufferedReader</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>		<span class="hljs-keyword">try</span> &#123;<br>			server = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ServerSocket</span>(<span class="hljs-number">9999</span>); <span class="hljs-comment">// 绑定9999端口.</span><br>			socket1 = server.accept(); <span class="hljs-comment">// 接受客户端的连接请求, 此方法会引起阻塞.</span><br>			System.out.println(socket1);<br>			<span class="hljs-comment">// 服务器端的socket1和客户端的socket2就建立了双向的网络通道</span><br>			<span class="hljs-type">InputStream</span> <span class="hljs-variable">inputStream</span> <span class="hljs-operator">=</span> socket1.getInputStream();<br>			<span class="hljs-type">InputStreamReader</span> <span class="hljs-variable">isr</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">InputStreamReader</span>(inputStream);<br><br>			bufferedReader = <span class="hljs-keyword">new</span> <span class="hljs-title class_">BufferedReader</span>(isr);<br>			<span class="hljs-type">String</span> <span class="hljs-variable">readLine</span> <span class="hljs-operator">=</span> bufferedReader.readLine();<br>			System.out.println(readLine);<br>		&#125; <span class="hljs-keyword">catch</span> (IOException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">finally</span> &#123;<br>			<span class="hljs-keyword">if</span> (bufferedReader != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					bufferedReader.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br><br>			<span class="hljs-keyword">if</span> (socket1 != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					socket1.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br><br>			<span class="hljs-keyword">if</span> (server != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					server.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br><br>			&#125;<br>		&#125;<br>	&#125;<br><br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">client</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-comment">// 连接服务器, 必须知道ip和端口.</span><br>		<span class="hljs-type">Socket</span> <span class="hljs-variable">socket2</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>		<span class="hljs-type">BufferedWriter</span> <span class="hljs-variable">bufferedWriter</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>		<span class="hljs-keyword">try</span> &#123;<br>			socket2 = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Socket</span>(<span class="hljs-string">&quot;127.0.0.1&quot;</span>, <span class="hljs-number">9999</span>);<br>			System.out.println(socket2);<br>			<span class="hljs-comment">// 客户端的socket2和服务器端的socket1就建立了双向的网络通道</span><br>			<span class="hljs-type">OutputStream</span> <span class="hljs-variable">outputStream</span> <span class="hljs-operator">=</span> socket2.getOutputStream();<br>			<span class="hljs-type">OutputStreamWriter</span> <span class="hljs-variable">osw</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">OutputStreamWriter</span>(outputStream);<br>			bufferedWriter = <span class="hljs-keyword">new</span> <span class="hljs-title class_">BufferedWriter</span>(osw);<br>			bufferedWriter.write(<span class="hljs-string">&quot;你好, 服务器, 俺是客户端.....&quot;</span>);<br>			bufferedWriter.newLine();<span class="hljs-comment">// 必须要有换行</span><br>			bufferedWriter.flush();<br>		&#125; <span class="hljs-keyword">catch</span> (IOException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">finally</span> &#123;<br>			<span class="hljs-keyword">if</span> (bufferedWriter != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					bufferedWriter.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br><br>			<span class="hljs-keyword">if</span> (socket2 != <span class="hljs-literal">null</span>) &#123;<br>				<span class="hljs-keyword">try</span> &#123;<br>					socket2.close();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e2) &#123;<br>				&#125;<br>			&#125;<br>		&#125;<br>	&#125;<br>&#125;<br><br></code></pre></td></tr></table></figure>
<h2 id="杀死本机被占用的端口号"><a href="#杀死本机被占用的端口号" class="headerlink" title="杀死本机被占用的端口号"></a>杀死本机被占用的端口号</h2><figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">netstat</span> -ano   查看操作系统所有占用端口的进程<br><span class="hljs-attribute">netstat</span> -ano | findstr <span class="hljs-string">&quot;9999&quot;</span> 获取占用了<span class="hljs-number">9999</span>端口的进程<br><span class="hljs-attribute">taskkill</span> /F /pid <span class="hljs-number">1234</span> 关闭进程号为<span class="hljs-number">1234</span>的进程<br></code></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title>Java反射</title>
    <url>/2021/04/22/Java-reflect/</url>
    <content><![CDATA[<h1 id="反射"><a href="#反射" class="headerlink" title="反射"></a>反射</h1><h2 id="单元测试类"><a href="#单元测试类" class="headerlink" title="单元测试类"></a>单元测试类</h2><blockquote>
<ul>
<li>单元测试方法的要求：类必须是公共的，必需不能有任何构造器</li>
<li>方法必修是公共非静态无返回值参数</li>
<li>执行：在方法名中右击，点击run as -junit</li>
<li>包装类：把基本数据类型包装成对象</li>
</ul>
</blockquote>
<h2 id="反射概述"><a href="#反射概述" class="headerlink" title="反射概述"></a>反射概述</h2><ul>
<li><p>JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。</p>
<p>要想解剖一个类,必须先要获取到该类的字节码文件对象。而解剖使用的就是Class类中的方法.所以先要获取到每一个字节码文件对应的Class类型的对象.</p>
</li>
<li><p>反射就是把Java类中的各种成分映射成一个个的Java对象</p>
<p>例如：一个类有：成员变量、方法、构造方法、包等等信息，利用反射技术可以对一个类进行解剖，把个个组成部分映射成一个个对象。</p>
<p>   （其实：一个类中这些成员方法、构造方法、在加入类中都有一个类来描述）</p>
</li>
<li><p>参考链接：CSDN博客 <a href="https://blog.csdn.net/qq_36226453/article/details/82790375">链接</a></p>
</li>
</ul>
<h2 id="反射基础实例代码"><a href="#反射基础实例代码" class="headerlink" title="反射基础实例代码"></a>反射基础实例代码</h2><figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">package</span> reflect;<br><br><span class="hljs-keyword">import</span> <span class="hljs-keyword">static</span> org.junit.Assert.*;<br><br><span class="hljs-keyword">import</span> java.io.FileInputStream;<br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-keyword">import</span> java.io.InputStream;<br><span class="hljs-keyword">import</span> java.io.Serializable;<br><span class="hljs-keyword">import</span> java.lang.annotation.Annotation;<br><span class="hljs-keyword">import</span> java.lang.annotation.ElementType;<br><span class="hljs-keyword">import</span> java.lang.annotation.Retention;<br><span class="hljs-keyword">import</span> java.lang.annotation.RetentionPolicy;<br><span class="hljs-keyword">import</span> java.lang.annotation.Target;<br><span class="hljs-keyword">import</span> java.lang.reflect.Constructor;<br><span class="hljs-keyword">import</span> java.lang.reflect.Field;<br><span class="hljs-keyword">import</span> java.lang.reflect.InvocationTargetException;<br><span class="hljs-keyword">import</span> java.lang.reflect.Method;<br><span class="hljs-keyword">import</span> java.util.ArrayList;<br><span class="hljs-keyword">import</span> java.util.HashMap;<br><span class="hljs-keyword">import</span> java.util.Map.Entry;<br><span class="hljs-keyword">import</span> java.util.Properties;<br><span class="hljs-keyword">import</span> java.util.Set;<br><br><span class="hljs-keyword">import</span> org.junit.Test;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 反射 : java的动态处理技术</span><br><span class="hljs-comment"> */</span><br><span class="hljs-meta">@Target(&#123;ElementType.TYPE, ElementType.FIELD&#125;)</span><br><span class="hljs-meta">@Retention(RetentionPolicy.RUNTIME)</span><br><span class="hljs-meta">@interface</span> HelloAnnotation &#123;<br>	String <span class="hljs-title function_">name</span><span class="hljs-params">()</span> <span class="hljs-keyword">default</span> <span class="hljs-string">&quot;缺省名字&quot;</span>;<br>	<span class="hljs-type">int</span> <span class="hljs-title function_">id</span><span class="hljs-params">()</span> <span class="hljs-keyword">default</span> <span class="hljs-number">10</span>;<br>&#125;<br><br><span class="hljs-meta">@HelloAnnotation(name=&quot;abc&quot;)</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Teacher</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">ArrayList</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">Serializable</span>, Comparable, Runnable &#123;<br>	<br>	<span class="hljs-meta">@HelloAnnotation</span><br>	<span class="hljs-keyword">private</span> String name;<br>	<span class="hljs-keyword">private</span> <span class="hljs-type">int</span> age;<br>	<span class="hljs-keyword">private</span> String gender;<br>	<br>	<span class="hljs-comment">//@HelloAnnotation</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-title function_">Teacher</span><span class="hljs-params">()</span> &#123;<br>	&#125;<br>	<span class="hljs-comment">//构造方法，Ctrl+Alt+S+O</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-title function_">Teacher</span><span class="hljs-params">(String name)</span> &#123;<br>		<span class="hljs-built_in">this</span>.name = name;<br>	&#125;<br>	<br>	<span class="hljs-keyword">public</span> <span class="hljs-title function_">Teacher</span><span class="hljs-params">(<span class="hljs-type">int</span> age)</span> &#123;<br>		<span class="hljs-built_in">this</span>.age = age;<br>	&#125;<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-title function_">Teacher</span><span class="hljs-params">(String name,String gender)</span> &#123;<br>		<span class="hljs-built_in">this</span>.name = name;<br>		<span class="hljs-built_in">this</span>.gender = gender;<br>	&#125;<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-title function_">Teacher</span><span class="hljs-params">(String name, <span class="hljs-type">int</span> age, String gender)</span> &#123;<br>		<span class="hljs-built_in">this</span>.name = name;<br>		<span class="hljs-built_in">this</span>.age = age;<br>		<span class="hljs-built_in">this</span>.gender = gender;<br>	&#125;<br>    <span class="hljs-comment">//set,get方法 快捷键Ctrl+Alt+S+R</span><br>	<span class="hljs-comment">//@HelloAnnotation</span><br>	<span class="hljs-keyword">public</span> String <span class="hljs-title function_">getName</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-keyword">return</span> name;<br>	&#125;<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setName</span><span class="hljs-params">(String name)</span> &#123;<br>		<span class="hljs-built_in">this</span>.name = name;<br>	&#125;<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">getAge</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-keyword">return</span> age;<br>	&#125;<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setAge</span><span class="hljs-params">(<span class="hljs-type">int</span> age)</span> &#123;<br>		<span class="hljs-built_in">this</span>.age = age;<br>	&#125;<br><br>	<span class="hljs-keyword">public</span> String <span class="hljs-title function_">getGender</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-keyword">return</span> gender;<br>	&#125;<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setGender</span><span class="hljs-params">(String gender)</span> &#123;<br>		<span class="hljs-built_in">this</span>.gender = gender;<br>	&#125;<br>    <span class="hljs-comment">//Ctrl+Alt+S+S</span><br>	<span class="hljs-meta">@Override</span><br>	<span class="hljs-keyword">public</span> String <span class="hljs-title function_">toString</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Teacher [name=&quot;</span> + name + <span class="hljs-string">&quot;, age=&quot;</span> + age + <span class="hljs-string">&quot;, gender=&quot;</span> + gender + <span class="hljs-string">&quot;]&quot;</span>;<br>	&#125;<br><br>	<span class="hljs-meta">@Override</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">run</span><span class="hljs-params">()</span> &#123;<br>	&#125;<br><br>	<span class="hljs-meta">@Override</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">compareTo</span><span class="hljs-params">(Object o)</span> &#123;<br>		<span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>	&#125;<br>	<br>	<span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">lesson</span><span class="hljs-params">(String content, <span class="hljs-type">int</span> time)</span> &#123;<br>		System.out.println(<span class="hljs-string">&quot;老师在上[&quot;</span> + content + <span class="hljs-string">&quot;]课, 共上了[&quot;</span> + time +<span class="hljs-string">&quot;]小时&quot;</span>);<br>		<span class="hljs-comment">//return true;</span><br>		<span class="hljs-comment">//throw new RuntimeException(&quot;一个异常&quot;);</span><br>	&#125;<br>	<br>&#125;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ReflectTest</span> &#123;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">testName</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> Exception &#123;<br>		<span class="hljs-type">Class</span> <span class="hljs-variable">clazz</span> <span class="hljs-operator">=</span> Class.forName(<span class="hljs-string">&quot;reflect.Teacher&quot;</span>);<br>		Annotation[] annotations = clazz.getAnnotations();<br>		System.out.println(annotations.length);<br>		System.out.println(annotations[<span class="hljs-number">0</span>]);<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test14</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> Exception &#123;<br>		<span class="hljs-type">Class</span> <span class="hljs-variable">clazz</span> <span class="hljs-operator">=</span> Class.forName(<span class="hljs-string">&quot;reflect.Teacher&quot;</span>);<br>		System.out.println(<span class="hljs-string">&quot;父类 : &quot;</span> + clazz.getSuperclass());<br>		Constructor[] constructors = clazz.getConstructors();<br>		<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; constructors.length; i++) &#123;<br>			System.out.println(<span class="hljs-string">&quot;构造器 : &quot;</span> + constructors[i]);<br>		&#125;<br>		System.out.println(<span class="hljs-string">&quot;************************************************&quot;</span>);<br>		Field[] fields = clazz.getFields(); <span class="hljs-comment">// 所有公共属性</span><br>		<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; fields.length; i++) &#123;<br>			System.out.println(<span class="hljs-string">&quot;公共属性：&quot;</span>+fields[i]);<br>		&#125;<br>		<span class="hljs-comment">//反射尽量不要破坏封装性，容易发生严重后果，慎用getDeclared.....()方法</span><br>		Field[] declaredFields = clazz.getDeclaredFields(); <span class="hljs-comment">// 所有本类属性</span><br>		<span class="hljs-keyword">for</span> (Field field : declaredFields) &#123;<br>			System.out.println(<span class="hljs-string">&quot;本类属性：&quot;</span>+field);<br>		&#125;<br>		System.out.println(<span class="hljs-string">&quot;************************************************&quot;</span>);<br>		Method[] methods = clazz.getMethods();<span class="hljs-comment">//所有公共方法</span><br>		<span class="hljs-keyword">for</span> (Method method : methods) &#123;<br>			System.out.println(<span class="hljs-string">&quot;公共方法&quot;</span>+method);<br>		&#125;<br>		System.out.println(<span class="hljs-string">&quot;************************************************&quot;</span>);<br>		Method[] declaredMethods = clazz.getDeclaredMethods();<span class="hljs-comment">//所有本类方法</span><br>		<span class="hljs-keyword">for</span> (Method method : declaredMethods) &#123;<br>			System.out.println(<span class="hljs-string">&quot;本类方法：&quot;</span>+method);<br>		&#125;<br>	&#125;<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test13</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-keyword">try</span> &#123;<br>			<span class="hljs-type">Class</span> <span class="hljs-variable">clazz</span> <span class="hljs-operator">=</span> Class.forName(<span class="hljs-string">&quot;reflect.Teacher&quot;</span>);<br>			<span class="hljs-type">Method</span> <span class="hljs-variable">lessonMethod</span> <span class="hljs-operator">=</span> clazz.getDeclaredMethod(<span class="hljs-string">&quot;lesson&quot;</span>, String.class, <span class="hljs-type">int</span>.class);<br>			lessonMethod.setAccessible(<span class="hljs-literal">true</span>);<br>			<br>			<span class="hljs-type">Short</span> <span class="hljs-variable">n</span> <span class="hljs-operator">=</span> <span class="hljs-number">3</span>;<br>			<span class="hljs-type">Object</span> <span class="hljs-variable">retValue</span> <span class="hljs-operator">=</span> lessonMethod.invoke(<span class="hljs-literal">null</span>, <span class="hljs-string">&quot;JavaWEB&quot;</span>, n); <span class="hljs-comment">// 静态方法传null, 不需要传入对象</span><br>			System.out.println(retValue);<br>		&#125; <span class="hljs-keyword">catch</span> (ClassNotFoundException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (IllegalAccessException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (NoSuchMethodException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (SecurityException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (IllegalArgumentException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (InvocationTargetException e) &#123;<br>			e.printStackTrace();<br>		&#125;<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test12</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-keyword">try</span> &#123;<br>			<span class="hljs-type">Class</span> <span class="hljs-variable">clazz</span> <span class="hljs-operator">=</span> Class.forName(<span class="hljs-string">&quot;reflect.Teacher&quot;</span>);<br>			<span class="hljs-type">Constructor</span> <span class="hljs-variable">constructor</span> <span class="hljs-operator">=</span> clazz.getConstructor(String.class, <span class="hljs-type">int</span>.class, String.class);<br>			<span class="hljs-type">Object</span> <span class="hljs-variable">object</span> <span class="hljs-operator">=</span> constructor.newInstance(<span class="hljs-string">&quot;佟刚&quot;</span>, <span class="hljs-number">40</span>, <span class="hljs-string">&quot;男&quot;</span>);<br>			<br>			<span class="hljs-comment">// getMethod只能获取公共的方法, 包括从父类继承的. </span><br>			<span class="hljs-comment">//Method lessonMethod = clazz.getMethod(&quot;lesson&quot;, String.class, int.class); </span><br>			<br>			<span class="hljs-comment">// getDeclaredMethod可以获取本类中所有声明的方法</span><br>			<span class="hljs-type">Method</span> <span class="hljs-variable">lessonMethod</span> <span class="hljs-operator">=</span> clazz.getDeclaredMethod(<span class="hljs-string">&quot;lesson&quot;</span>, String.class, <span class="hljs-type">int</span>.class);<br>			lessonMethod.setAccessible(<span class="hljs-literal">true</span>);<br>			<br>			<span class="hljs-type">Short</span> <span class="hljs-variable">n</span> <span class="hljs-operator">=</span> <span class="hljs-number">3</span>;<br>			<span class="hljs-type">Object</span> <span class="hljs-variable">retValue</span> <span class="hljs-operator">=</span> lessonMethod.invoke(object, <span class="hljs-string">&quot;JavaWEB&quot;</span>, n); <br>			System.out.println(retValue);<br>			<span class="hljs-comment">/*</span><br><span class="hljs-comment">		    private String outOfBoundsMsg(int index) &#123;</span><br><span class="hljs-comment">		        return &quot;Index: &quot;+index+&quot;, Size: &quot;+size;</span><br><span class="hljs-comment">		    &#125;</span><br><span class="hljs-comment">		    */</span><br>			<span class="hljs-comment">/*</span><br><span class="hljs-comment">			//拿父类的方法，会有警告，最好不要暴力访问私有方法，属性</span><br><span class="hljs-comment">			Method declaredMethod = clazz.getSuperclass().getDeclaredMethod(&quot;outOfBoundsMsg&quot;, int.class);</span><br><span class="hljs-comment">			System.out.println(declaredMethod);</span><br><span class="hljs-comment">			//危险操作</span><br><span class="hljs-comment">			declaredMethod.setAccessible(true);</span><br><span class="hljs-comment">			</span><br><span class="hljs-comment">			Object invoke = declaredMethod.invoke(object, 20);</span><br><span class="hljs-comment">			System.out.println(invoke);</span><br><span class="hljs-comment">			*/</span><br>		&#125; <span class="hljs-keyword">catch</span> (ClassNotFoundException e) &#123; <span class="hljs-comment">// 类未找到</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (NoSuchMethodException e) &#123; <span class="hljs-comment">// 方法没有找到, 方法名错误或参数列表错误</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (SecurityException e) &#123; <span class="hljs-comment">// 安全异常</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (InstantiationException e) &#123; <span class="hljs-comment">// 创建对象时出现异常</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (IllegalAccessException e) &#123; <span class="hljs-comment">// 非法访问异常</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (IllegalArgumentException e) &#123; <span class="hljs-comment">// 非法实参异常, 实参和形参不匹配, 类型和顺序和数量</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (InvocationTargetException e) &#123; <span class="hljs-comment">// 调用的目标方法内部出现异常了.</span><br>			e.printStackTrace();<br>		&#125;<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test11</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-keyword">try</span> &#123;<br>			<span class="hljs-type">Class</span> <span class="hljs-variable">clazz</span> <span class="hljs-operator">=</span> Class.forName(<span class="hljs-string">&quot;reflect.Teacher&quot;</span>);<br>			<span class="hljs-type">Constructor</span> <span class="hljs-variable">constructor</span> <span class="hljs-operator">=</span> clazz.getConstructor(String.class, <span class="hljs-type">int</span>.class, String.class);<br>			<span class="hljs-type">Object</span> <span class="hljs-variable">object</span> <span class="hljs-operator">=</span> constructor.newInstance(<span class="hljs-string">&quot;佟刚&quot;</span>, <span class="hljs-number">40</span>, <span class="hljs-string">&quot;男&quot;</span>);<br>			<br>			<span class="hljs-comment">//object.lesson(&quot;JavaWEB&quot;, 3);</span><br>			<span class="hljs-comment">//先获取方法</span><br>			<br>			<span class="hljs-type">Method</span> <span class="hljs-variable">lessonMethod</span> <span class="hljs-operator">=</span> clazz.getMethod(<span class="hljs-string">&quot;lesson&quot;</span>, String.class, <span class="hljs-type">int</span>.class); <span class="hljs-comment">// 后面是方法参数类型列表</span><br>			<span class="hljs-type">short</span> <span class="hljs-variable">n</span> <span class="hljs-operator">=</span> <span class="hljs-number">3</span>;<br>			<span class="hljs-type">Object</span> <span class="hljs-variable">retValue</span> <span class="hljs-operator">=</span> lessonMethod.invoke(object, <span class="hljs-string">&quot;JavaWEB&quot;</span>, n); <span class="hljs-comment">// 后面是实参列表, 如果方法没有返回值, 它的返回值是null</span><br>			System.out.println(retValue);<br>		&#125; <span class="hljs-keyword">catch</span> (ClassNotFoundException e) &#123; <span class="hljs-comment">// 类未找到</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (NoSuchMethodException e) &#123; <span class="hljs-comment">// 方法没有找到, 方法名错误或参数列表错误</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (SecurityException e) &#123; <span class="hljs-comment">// 安全异常</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (InstantiationException e) &#123; <span class="hljs-comment">// 创建对象时出现异常</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (IllegalAccessException e) &#123; <span class="hljs-comment">// 非法访问异常</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (IllegalArgumentException e) &#123; <span class="hljs-comment">// 非法实参异常, 实参和形参不匹配, 类型和顺序和数量</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (InvocationTargetException e) &#123; <span class="hljs-comment">// 调用的目标方法内部出现异常了.</span><br>			e.printStackTrace();<br>		&#125;<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test10</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-keyword">try</span> &#123;<br>			<span class="hljs-type">Class</span> <span class="hljs-variable">clazz</span> <span class="hljs-operator">=</span> Class.forName(<span class="hljs-string">&quot;reflect.Teacher&quot;</span>);<br>			<span class="hljs-comment">//Object object = clazz.newInstance(); 没有无参构造器时出问题</span><br>			<br>			<span class="hljs-comment">//public Teacher(String name, int age, String gender) 要想定位这个构造器, 必须让参数列表一致.</span><br>			<br>			<span class="hljs-comment">// 提供形式参数类型列表, 是类模板对象的列表</span><br>			<span class="hljs-type">Constructor</span> <span class="hljs-variable">constructor</span> <span class="hljs-operator">=</span> clazz.getConstructor(String.class, <span class="hljs-type">int</span>.class, String.class); <span class="hljs-comment">// 定位合适的构造器</span><br>			<span class="hljs-comment">// 调用时必须给定实参列表</span><br>			<span class="hljs-type">Object</span> <span class="hljs-variable">object</span> <span class="hljs-operator">=</span> constructor.newInstance(<span class="hljs-string">&quot;程程&quot;</span>, <span class="hljs-number">20</span>, <span class="hljs-string">&quot;女&quot;</span>); <span class="hljs-comment">// new Teacher(&quot;程程&quot;, 20, &quot;女&quot;);</span><br>			System.out.println(object);<br>			<br>		&#125; <span class="hljs-keyword">catch</span> (ClassNotFoundException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (InstantiationException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (IllegalAccessException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (NoSuchMethodException e) &#123; <span class="hljs-comment">// 参数列表出错, 或者方法名出错</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (SecurityException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (IllegalArgumentException e) &#123; <span class="hljs-comment">// 方法调用时实参和形参不匹配</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (InvocationTargetException e) &#123;<br>			e.printStackTrace();<br>		&#125;<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test9</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> IOException  &#123;<br>		<span class="hljs-comment">//FileInputStream fis = new FileInputStream(&quot;只能读当前目录下的文件&quot;);</span><br>		<span class="hljs-type">ClassLoader</span> <span class="hljs-variable">classLoader</span> <span class="hljs-operator">=</span> <span class="hljs-built_in">this</span>.getClass().getClassLoader();<br>		<span class="hljs-comment">// 只能加载build-path和src下的文件</span><br>		<span class="hljs-comment">//InputStream inputStream = classLoader.getResourceAsStream(&quot;com/sun/corba/se/impl/logging/LogStrings.properties&quot;); // 读取资源文件, 只要是Build-Path(classpath)中的文件都可以</span><br>		<span class="hljs-type">InputStream</span> <span class="hljs-variable">inputStream</span> <span class="hljs-operator">=</span> classLoader.getResourceAsStream(<span class="hljs-string">&quot;s2&quot;</span>);<br>		<span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>		properties.load(inputStream);<br>		<br>		Set&lt;Entry&lt;Object, Object&gt;&gt; entrySet = properties.entrySet();<br>		<span class="hljs-keyword">for</span> (Entry&lt;Object, Object&gt; entry : entrySet) &#123;<br>			System.out.println(entry);<br>		&#125;<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test8</span><span class="hljs-params">()</span>  &#123;<br>		<span class="hljs-type">ClassLoader</span> <span class="hljs-variable">classLoader1</span> <span class="hljs-operator">=</span> ClassLoader.getSystemClassLoader(); <span class="hljs-comment">// 获取系统类加载器</span><br>		System.out.println(classLoader1);<br>		<span class="hljs-type">ClassLoader</span> <span class="hljs-variable">classLoader2</span> <span class="hljs-operator">=</span> <span class="hljs-built_in">this</span>.getClass().getClassLoader(); <span class="hljs-comment">// 使用最多的, 获取当前类的类加载器</span><br>		System.out.println(classLoader2);<br>		<br>		<span class="hljs-type">ClassLoader</span> <span class="hljs-variable">classLoader3</span> <span class="hljs-operator">=</span> classLoader1.getParent(); <span class="hljs-comment">// 获取父 &quot;类加载器&quot;,  是 扩展 &quot;类加载器&quot;</span><br>		System.out.println(classLoader3);<br>		<br>		<span class="hljs-type">ClassLoader</span> <span class="hljs-variable">classLoader4</span> <span class="hljs-operator">=</span> classLoader3.getParent(); <span class="hljs-comment">// 获取到的是引导类加载器(Bootstrap )</span><br>		System.out.println(classLoader4); <span class="hljs-comment">// 这个类加载器无法获取, 无法使用</span><br>		<br>		<span class="hljs-comment">/*</span><br><span class="hljs-comment">		双亲委派机制</span><br><span class="hljs-comment">		用户类加载器加载类时, 必须把此加载请求转发给父类加载器, 父类加载器再继续向父类加载器委派, 直到Bootstrap类加载器</span><br><span class="hljs-comment">		从Bootstrap类加载器开始真正加载, 各司其职. </span><br><span class="hljs-comment">		*/</span><br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test7</span><span class="hljs-params">()</span>  &#123;<br>		<span class="hljs-type">Class</span> <span class="hljs-variable">strClazz</span> <span class="hljs-operator">=</span> String.class;<br>		System.out.println(strClazz);<br>		<br>		<span class="hljs-comment">// 基本数据类型的类模板只能用第一种方式获取.</span><br>		<span class="hljs-type">Class</span> <span class="hljs-variable">clazz1</span> <span class="hljs-operator">=</span> <span class="hljs-type">int</span>.class; <span class="hljs-comment">// 基本数据类型也有相应的类模板对象, 但是不能获取属性和方法, 只能作为一个标记来使用.</span><br>		<span class="hljs-type">Class</span> <span class="hljs-variable">clazz2</span> <span class="hljs-operator">=</span> Integer.class; <span class="hljs-comment">// 这是一个普通类模板.</span><br>		System.out.println(clazz1 == clazz2);<br>		<br>		<span class="hljs-comment">// 判断类模板类型</span><br>		System.out.println(<span class="hljs-string">&quot;是否是基本型 : &quot;</span> + clazz1.isPrimitive());<br>		System.out.println(<span class="hljs-string">&quot;是否是基本型 : &quot;</span> + clazz2.isPrimitive());<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test6</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> ClassNotFoundException &#123;<br>		<span class="hljs-comment">// 获取类模板对象的方法 ，有４种</span><br>		<span class="hljs-comment">// 1) 直接通过类.class, 效率最高, 最安全.</span><br>		<span class="hljs-type">Class</span> <span class="hljs-variable">clazz1</span> <span class="hljs-operator">=</span> Teacher.class;<br>		<br>		<span class="hljs-comment">// 2) 根据对象, 调用它的getClass()方法获取, 此方法也很常用.</span><br>		<span class="hljs-type">Teacher</span> <span class="hljs-variable">teacher</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Teacher</span>(<span class="hljs-string">&quot;佟刚&quot;</span>, <span class="hljs-number">40</span>, <span class="hljs-string">&quot;男&quot;</span>);<br>		<span class="hljs-type">Class</span> <span class="hljs-variable">clazz2</span> <span class="hljs-operator">=</span> teacher.getClass();<br>		<br>		System.out.println(clazz1 == clazz2);<br>		<br>		<span class="hljs-comment">// 3) 反射中最常用的 Class.forName(&quot;全限定类名&quot;);</span><br>		<span class="hljs-type">Class</span> <span class="hljs-variable">clazz3</span> <span class="hljs-operator">=</span> Class.forName(<span class="hljs-string">&quot;reflect.Teacher&quot;</span>);<br>		System.out.println(clazz2 == clazz3);<br>		<br>		<span class="hljs-comment">// 4) 通过类加载器对象动态加载类</span><br>		<span class="hljs-type">ClassLoader</span> <span class="hljs-variable">classLoader</span> <span class="hljs-operator">=</span> <span class="hljs-built_in">this</span>.getClass().getClassLoader();<br>		<span class="hljs-type">Class</span> <span class="hljs-variable">clazz4</span> <span class="hljs-operator">=</span> classLoader.loadClass(<span class="hljs-string">&quot;reflect.Teacher&quot;</span>);<br>		System.out.println(clazz3 == clazz4);<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test5</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> ClassNotFoundException &#123;<br>		<span class="hljs-type">Class</span> <span class="hljs-variable">clazz</span> <span class="hljs-operator">=</span> Class.forName(<span class="hljs-string">&quot;reflect.Teacher&quot;</span>);<br>		<span class="hljs-type">String</span> <span class="hljs-variable">name</span> <span class="hljs-operator">=</span> clazz.getName();<br>		System.out.println(<span class="hljs-string">&quot;类名 : &quot;</span> + name);<br>		System.out.println(<span class="hljs-string">&quot;简单类名 :  &quot;</span> + clazz.getSimpleName());<br>		<span class="hljs-type">Class</span> <span class="hljs-variable">superclass</span> <span class="hljs-operator">=</span> clazz.getSuperclass();<br>		System.out.println(<span class="hljs-string">&quot;父类 : &quot;</span> + superclass);<br>		Class[] interfaces = clazz.getInterfaces();<br>		System.out.println(<span class="hljs-string">&quot;接口列表 &quot;</span>);<br>		<span class="hljs-keyword">for</span> (Class class1 : interfaces) &#123;<br>			System.out.println(class1);<br>		&#125;<br>		<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test4</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-keyword">try</span> &#123;<br>			<span class="hljs-type">Class</span> <span class="hljs-variable">clazz</span> <span class="hljs-operator">=</span> Class.forName(<span class="hljs-string">&quot;reflect.Teacher&quot;</span>);<br>			<span class="hljs-type">Object</span> <span class="hljs-variable">object</span> <span class="hljs-operator">=</span> clazz.newInstance();<br>			System.out.println(object);<br>			<br>			<span class="hljs-comment">//Field ageField = clazz.getField(&quot;age&quot;); // getField方法只能获取公共的属性, 也包括从父类继承的属性, 不可以获取私有属性</span><br>			<span class="hljs-type">Field</span> <span class="hljs-variable">ageField</span> <span class="hljs-operator">=</span> clazz.getDeclaredField(<span class="hljs-string">&quot;age&quot;</span>); <span class="hljs-comment">// 获取本类中声明的任意属性 </span><br>			<span class="hljs-comment">// 暴力反射!!! 不推荐使用!!</span><br>			ageField.setAccessible(<span class="hljs-literal">true</span>); <span class="hljs-comment">// 设置此属性为可访问的.</span><br>			ageField.set(object, <span class="hljs-number">40</span>); <br>			System.out.println(ageField.get(object)); <br>			<br>			<span class="hljs-type">Field</span> <span class="hljs-variable">nameField</span> <span class="hljs-operator">=</span> clazz.getDeclaredField(<span class="hljs-string">&quot;name&quot;</span>);<br>			nameField.setAccessible(<span class="hljs-literal">true</span>);<br>			nameField.set(object, <span class="hljs-string">&quot;佟刚&quot;</span>);<br>			<br>			System.out.println(object);<br>		&#125; <span class="hljs-keyword">catch</span> (ClassNotFoundException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (InstantiationException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (IllegalAccessException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (NoSuchFieldException e) &#123; <span class="hljs-comment">// 查找属性时, 有可能属性名不对, 可能是属性不存在</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (SecurityException e) &#123; <span class="hljs-comment">// 如果有安全检查.</span><br>			e.printStackTrace();<br>		&#125;<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test3</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-keyword">try</span> &#123;<br>			<span class="hljs-type">Class</span> <span class="hljs-variable">clazz</span> <span class="hljs-operator">=</span> Class.forName(<span class="hljs-string">&quot;reflect.Teacher&quot;</span>);<br>			<span class="hljs-type">Object</span> <span class="hljs-variable">object</span> <span class="hljs-operator">=</span> clazz.newInstance();<br>			System.out.println(object);<br>			<br>			<span class="hljs-comment">// 要想使用属性, 1)先获取属性定义对象 2) 配合目标this对象完成对象属性的间接访问.</span><br>			<span class="hljs-type">Field</span> <span class="hljs-variable">ageField</span> <span class="hljs-operator">=</span> clazz.getField(<span class="hljs-string">&quot;age&quot;</span>); <span class="hljs-comment">// 根据属性名获取属性的定义对象</span><br>			ageField.set(object, <span class="hljs-number">40</span>); <span class="hljs-comment">// 设置属性值, 相当于 object.age = 40; </span><br>			System.out.println(ageField.get(object)); <span class="hljs-comment">// 获取属性值,  相当于 System.out.println(object.age)</span><br>			<br>			<span class="hljs-type">Field</span> <span class="hljs-variable">nameField</span> <span class="hljs-operator">=</span> clazz.getField(<span class="hljs-string">&quot;name&quot;</span>);<br>			nameField.set(object, <span class="hljs-string">&quot;佟刚&quot;</span>); <span class="hljs-comment">// object.name = &quot;佟刚&quot;</span><br>			<br>			<span class="hljs-type">Field</span> <span class="hljs-variable">genderField</span> <span class="hljs-operator">=</span> clazz.getField(<span class="hljs-string">&quot;gender&quot;</span>);<br>			genderField.set(object, <span class="hljs-string">&quot;男&quot;</span>);<br>			<br>			System.out.println(object);<br>		&#125; <span class="hljs-keyword">catch</span> (ClassNotFoundException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (InstantiationException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (IllegalAccessException e) &#123;<br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (NoSuchFieldException e) &#123; <span class="hljs-comment">// 查找属性时, 有可能属性名不对, 可能是属性不存在</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (SecurityException e) &#123; <span class="hljs-comment">// 如果有安全检查.</span><br>			e.printStackTrace();<br>		&#125;<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test2</span><span class="hljs-params">()</span> &#123; <span class="hljs-comment">// 软编码, 灵活, 把问题延迟到运行时.</span><br>		<span class="hljs-keyword">try</span> &#123;<br>			<span class="hljs-comment">// 干预类的加载, 直接获取类模板对象.</span><br>			<span class="hljs-type">Class</span> <span class="hljs-variable">clazz</span> <span class="hljs-operator">=</span> Class.forName(<span class="hljs-string">&quot;reflect.Teacher&quot;</span>); <span class="hljs-comment">// 类名必须全限定!!!</span><br>			<span class="hljs-comment">// 通过类模板对象.newInstance创建实体对象</span><br>			<span class="hljs-type">Object</span> <span class="hljs-variable">object</span> <span class="hljs-operator">=</span> clazz.newInstance(); <span class="hljs-comment">// 调用无参构造器创建对象</span><br>			System.out.println(object);<br>		&#125; <span class="hljs-keyword">catch</span> (ClassNotFoundException e) &#123; <span class="hljs-comment">// 在运行时动态加载类时, 发现没有找到类</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (InstantiationException e) &#123; <span class="hljs-comment">// 在创建对象时出现异常, 可能是构造器不存在</span><br>			e.printStackTrace();<br>		&#125; <span class="hljs-keyword">catch</span> (IllegalAccessException e) &#123; <span class="hljs-comment">// 非法访问, 访问权限不够时出现</span><br>			e.printStackTrace();<br>		&#125;<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test1</span><span class="hljs-params">()</span> &#123; <span class="hljs-comment">// 编译时必须依赖类, 硬编码</span><br>		<br>		<span class="hljs-comment">//Teacher t1 = new Teacher(); // 强烈依赖类</span><br>		<span class="hljs-comment">//t1.name = &quot;佟刚&quot;;</span><br>		<span class="hljs-comment">//t1.age = 40;</span><br>		<span class="hljs-comment">//t1.gender = &quot;男&quot;;</span><br>		<br>		<span class="hljs-comment">//System.out.println(t1.name);</span><br>		<span class="hljs-comment">//System.out.println(t1.age);</span><br>		<span class="hljs-comment">//System.out.println(t1.gender);</span><br>		<br>		<span class="hljs-comment">//System.out.println(t1);</span><br>		<br>		<span class="hljs-type">Teacher</span> <span class="hljs-variable">t2</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Teacher</span>(<span class="hljs-string">&quot;程程&quot;</span>, <span class="hljs-number">20</span>, <span class="hljs-string">&quot;女&quot;</span>);<br>		System.out.println(t2);<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test0</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-comment">// 测试方法所在的线程永远是守护线程</span><br>	&#125;<br>	<br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>		<span class="hljs-comment">// 守护线程, setDaemon(true); 此方法必须在start()以前调用.</span><br>		<span class="hljs-comment">// 主线程永远是用户线程</span><br>	&#125;<br>&#125;<br><br></code></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>反射</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux</title>
    <url>/2021/05/11/Linux/</url>
    <content><![CDATA[<h1 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h1><h2 id="vim的三种模式"><a href="#vim的三种模式" class="headerlink" title="vim的三种模式"></a>vim的三种模式</h2><h3 id="一般模式"><a href="#一般模式" class="headerlink" title="一般模式"></a>一般模式</h3><ul>
<li>以vi&#x2F;vim编辑器打开一个文件就进入了一般默认，是默认的默认</li>
<li>在一般模式中可以对文件进行复制、粘贴、删除、撤销</li>
<li>常用的命令<ul>
<li>yy<ul>
<li>复制一行</li>
</ul>
</li>
<li>y数字y<ul>
<li>复制多行</li>
</ul>
</li>
<li>dd<ul>
<li>删除一行</li>
</ul>
</li>
<li>d数字d<ul>
<li>删除多行</li>
</ul>
</li>
<li>p<ul>
<li>粘贴</li>
</ul>
</li>
<li>u<ul>
<li>撤销</li>
</ul>
</li>
<li>^<ul>
<li>回到行头</li>
</ul>
</li>
<li>$<ul>
<li>回到行尾</li>
</ul>
</li>
<li>gg或1+G<ul>
<li>回到页头</li>
</ul>
</li>
<li>G<ul>
<li>回到页尾</li>
</ul>
</li>
<li>数字+G<ul>
<li>回到某一行</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="编辑模式"><a href="#编辑模式" class="headerlink" title="编辑模式"></a>编辑模式</h3><ul>
<li>在一般模式中输入i、o、a或I、O、A时就进入了编辑模式</li>
<li>在编辑模式中按Esc又回到一般模式</li>
<li>常用的命令<ul>
<li>i<ul>
<li>在光标前插入</li>
</ul>
</li>
<li>o<ul>
<li>在下一行插入</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="命令模式"><a href="#命令模式" class="headerlink" title="命令模式"></a>命令模式</h3><ul>
<li>在一般模式中输入:、&#x2F;或？时就进入了命令模式</li>
<li>在命令模式中按Esc又回到一般模式</li>
<li>常用的命令<ul>
<li>:w<ul>
<li>保存</li>
</ul>
</li>
<li>:q<ul>
<li>推出</li>
</ul>
</li>
<li>:!<ul>
<li>强制执行</li>
</ul>
</li>
<li>:wq！<ul>
<li>保存并强制退出</li>
</ul>
</li>
<li>ZZ<ul>
<li>如果没有修改直接退出，修改了保存后退出</li>
</ul>
</li>
<li>:%s&#x2F;old 字符&#x2F;new 字符<ul>
<li>批量替换字符</li>
</ul>
</li>
<li>:nohl<ul>
<li>取消高亮显示</li>
</ul>
</li>
<li>&#x2F;要查找的词或?要查找的词<ul>
<li>通过n或N进行向上或向下的查找</li>
</ul>
</li>
<li>:set nu<ul>
<li>设置显示行号</li>
</ul>
</li>
<li>:set nonu<ul>
<li>取消显示行号</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="网络相关命令"><a href="#网络相关命令" class="headerlink" title="网络相关命令"></a>网络相关命令</h2><ul>
<li>ifconfig<ul>
<li>查询ip地址</li>
</ul>
</li>
<li>hostname<ul>
<li>查看主机名</li>
<li>修改主机名<ul>
<li>vim &#x2F;etc&#x2F;sysconfig&#x2F;network</li>
</ul>
</li>
<li>配置ip地址与主机名的映射关系<ul>
<li>vim &#x2F;etc&#x2F;hosts</li>
</ul>
</li>
</ul>
</li>
<li>service 管理后台服务<ul>
<li>service 服务名 start<ul>
<li>开启某个服务</li>
</ul>
</li>
<li>service 服务名 stop<ul>
<li>关闭某个服务</li>
</ul>
</li>
<li>service 服务名 status<ul>
<li>查看服务的状态</li>
</ul>
</li>
</ul>
</li>
<li>chkconfig 查看开机启动状态<ul>
<li>chkconfig 服务名 on<ul>
<li>开启某个服务开机自启</li>
</ul>
</li>
<li>chkconfig 服务名 off<ul>
<li>关闭某个服务开机自启</li>
</ul>
</li>
<li>chkconfig 服务名 –list<ul>
<li>查看某个服务的开机自启状态</li>
<li>一共有7个运行级别（0到6）<ul>
<li>如果2、3、4、5这四个运行级别是开启的则当前服务就是开机自启的</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="帮助相关命令"><a href="#帮助相关命令" class="headerlink" title="帮助相关命令"></a>帮助相关命令</h2><ul>
<li>man和help<ul>
<li>查询命令的帮助信息</li>
</ul>
</li>
<li>常用的快捷键<ul>
<li>ctrl + l<ul>
<li>清屏</li>
</ul>
</li>
<li>ctrl + c<ul>
<li>停止进程</li>
</ul>
</li>
<li>一定要善用Tab键<ul>
<li>可以帮助我们自动补全路径，防止出错</li>
</ul>
</li>
<li>通过上下键查询最近输入过的命令</li>
<li>ctrl + alt<ul>
<li>鼠标在虚拟机之间和主机之间切换</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="文件相关命令"><a href="#文件相关命令" class="headerlink" title="文件相关命令"></a>文件相关命令</h2><ul>
<li>pwd<ul>
<li>查看当前所在的工作目录</li>
</ul>
</li>
<li>cd<ul>
<li>进入某个目录<ul>
<li>通过绝对路径和相对路径都可以进入某个目录</li>
</ul>
</li>
<li>cd -<ul>
<li>返回上一次所在的目录</li>
</ul>
</li>
<li>cd或cd ~<ul>
<li>回到自己的家目录</li>
</ul>
</li>
</ul>
</li>
<li>rm -rf <ul>
<li>强制删除文件或目录，不提示</li>
</ul>
</li>
<li>touch 文件名<ul>
<li>创建一个空文件</li>
</ul>
</li>
<li>mkdir<ul>
<li>创建目录</li>
</ul>
</li>
<li>echo “要输出的内容”<ul>
<li>输出内容到控制台</li>
</ul>
</li>
<li>cat 文件<ul>
<li>查看小文件</li>
</ul>
</li>
<li>less 文件<ul>
<li>查看大文件<ul>
<li>通过pageup和pagedown键翻页</li>
</ul>
</li>
</ul>
</li>
<li>cp 源文件 目录<ul>
<li>将某个文件复制到某个目录下</li>
</ul>
</li>
<li>mv<ul>
<li>mv 老名字 新名字<ul>
<li>给文件重命名</li>
</ul>
</li>
<li>mv 源文件 目录<ul>
<li>剪切文件到某个目录下</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="搜索查找类命令"><a href="#搜索查找类命令" class="headerlink" title="搜索查找类命令"></a>搜索查找类命令</h2><ul>
<li>find 某个目录 [选项] 内容<ul>
<li>在某个目录下查找相关内容</li>
<li>选项<ul>
<li>-name <ul>
<li>根据文件名称查找</li>
</ul>
</li>
<li>-user<ul>
<li>根据文件所属的用户查找</li>
</ul>
</li>
<li>-size<ul>
<li>根据文件的大小查找</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>grep<ul>
<li>通常结合管道符 | 进行过滤查找</li>
<li>ll &#x2F;root | grep -n atguigu.txt<ul>
<li>查找&#x2F;root目录下是否包含atguigu.txt文件，并且会显示出行号</li>
</ul>
</li>
</ul>
</li>
<li>which<ul>
<li>查询某个命令在那个目录下</li>
</ul>
</li>
</ul>
<h2 id="压缩解压缩类命令"><a href="#压缩解压缩类命令" class="headerlink" title="压缩解压缩类命令"></a>压缩解压缩类命令</h2><ul>
<li>tar -zcvf  xxx.tar.gz 要压缩的内容<ul>
<li>压缩文件</li>
</ul>
</li>
<li>tar -zxvf xxx.tar.gz<ul>
<li>解压缩tar包</li>
</ul>
</li>
</ul>
<h2 id="进程线程类命令"><a href="#进程线程类命令" class="headerlink" title="进程线程类命令"></a>进程线程类命令</h2><ul>
<li>ps -aux | grep xxx<ul>
<li>查看内存和CPU的占用率</li>
</ul>
</li>
<li>ps -ef | grep xxx<ul>
<li>查看进程和父进程的ID</li>
</ul>
</li>
<li>kill -9 进程ID<ul>
<li>通过进程ID杀死进程</li>
</ul>
</li>
<li>killall 进程名<ul>
<li>通过进程名杀死进程</li>
</ul>
</li>
<li>netstat -nlp | grep 端口号<ul>
<li>查看端口号是否被占用</li>
</ul>
</li>
</ul>
<h2 id="RPM软件包和YUM仓库"><a href="#RPM软件包和YUM仓库" class="headerlink" title="RPM软件包和YUM仓库"></a>RPM软件包和YUM仓库</h2><ul>
<li>rpm -qa<ul>
<li>查询安装的所有的rpm软件包</li>
</ul>
</li>
<li>rpm -qa | grep 软件名<ul>
<li>查询安装的某个软件的rpm软件包名</li>
</ul>
</li>
<li>rpm -e –nodeps rpm软件包名<ul>
<li>不检查依赖协助某个rpm软件包</li>
</ul>
</li>
<li>rpm -ivh rpm软件包名<ul>
<li>安装rpm软件包</li>
</ul>
</li>
<li>yum -y install<ul>
<li>安装rpm软件包</li>
</ul>
</li>
<li>yum -y update<ul>
<li>更新rpm软件包</li>
</ul>
</li>
<li>yum -y check-update<ul>
<li>检查某个rpm软件包是否有更新</li>
</ul>
</li>
<li>yum -y remove<ul>
<li>卸载rpm软件包</li>
</ul>
</li>
<li>yum list<ul>
<li>查询所有缓存的rpm软件包</li>
</ul>
</li>
<li>yum clean all<ul>
<li>清除缓存</li>
</ul>
</li>
<li>yum makecache<ul>
<li>建立缓存</li>
</ul>
</li>
<li>yum deplist<ul>
<li>显示yum软件包的所有依赖关系</li>
</ul>
</li>
</ul>
<h2 id="文件权限管理"><a href="#文件权限管理" class="headerlink" title="文件权限管理"></a>文件权限管理</h2><ul>
<li>文件属性<ul>
<li>Linux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定。在Linux中我们可以使用ll或者ls -l命令来显示一个文件的属性以及文件所属的用户和组。</li>
<li><a href="https://imgtu.com/i/ga5mY6"><img src="https://z3.ax1x.com/2021/05/11/ga5mY6.md.png" alt="ga5mY6.md.png"></a></li>
<li>首位表示类型<ul>
<li>符号 - 代表文件</li>
<li>符号 d 代表目录</li>
<li>符号 l 代表链接文档</li>
</ul>
</li>
<li>第1-3位确定属主（该文件的所有者）拥有该文件的权限。—User </li>
<li>第4-6位确定属组（所有者的同组用户）拥有该文件的权限，—Group </li>
<li>第7-9位确定其他用户拥有该文件的权限 —Other</li>
</ul>
</li>
<li>chmod修改权限<ul>
<li>方式一：<ul>
<li>chmod  [{ugoa}{+-&#x3D;}{rwx}] 文件或目录 </li>
<li>chmod g+x houge.txt</li>
<li>chmod u-x,o+x houge.txt</li>
</ul>
</li>
<li>方式二：<ul>
<li>chmod  [mode&#x3D;421 ]  [文件或目录]</li>
<li>u:所有者  g:所有组  o:其他人  a:所有人(u、g、o的总和)</li>
<li>r&#x3D;4 w&#x3D;2 x&#x3D;1        rwx&#x3D;4+2+1&#x3D;7 </li>
<li>chmod 777 houge.txt</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Crontab调度"><a href="#Crontab调度" class="headerlink" title="Crontab调度"></a>Crontab调度</h2><ul>
<li><p>基本语法</p>
<ul>
<li>crontab -e：修改crontab文件。如果文件不存在会自动创建</li>
<li>crontab -l：显示crontab文件</li>
<li>crontab -r：删除crontab文件</li>
<li>crontab -ir：删除crontab文件前提醒用户</li>
</ul>
</li>
<li><p>脚本加执行权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">sudo chmod +x my.sh<br></code></pre></td></tr></table></figure>
</li>
<li><p>时间显示</p>
</li>
<li><pre><code>例子：
    # 每月的最后1天
    0 0 L * * *

    说明：
    Linux
    *    *    *    *    *
    -    -    -    -    -
    |    |    |    |    |
    |    |    |    |    +----- day of week (0 - 7) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
    |    |    |    +---------- month (1 - 12) OR jan,feb,mar,apr ...
    |    |    +--------------- day of month (1 - 31)
    |    +-------------------- hour (0 - 23)
    +------------------------- minute (0 - 59)
</code></pre>
</li>
<li></li>
<li><table>
<thead>
<tr>
<th align="left">字段</th>
<th align="left">是否必填</th>
<th align="left">允许值</th>
<th align="left">允许特殊字符</th>
<th align="left">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Seconds</td>
<td align="left">是</td>
<td align="left">0–59</td>
<td align="left"><code>*,-</code></td>
<td align="left">标准实现不支持此字段。</td>
</tr>
<tr>
<td align="left">Minutes</td>
<td align="left">是</td>
<td align="left">0–59</td>
<td align="left"><code>*,-</code></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Hours</td>
<td align="left">是</td>
<td align="left">0–23</td>
<td align="left"><code>*,-</code></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Day of month</td>
<td align="left">是</td>
<td align="left">1–31</td>
<td align="left"><code>*,-?LW</code></td>
<td align="left"><code>?LW</code>只有部分软件实现了</td>
</tr>
<tr>
<td align="left">Month</td>
<td align="left">是</td>
<td align="left">1–12 or JAN–DEC</td>
<td align="left"><code>*,-</code></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Day of week</td>
<td align="left">是</td>
<td align="left">0–7 or SUN–SAT</td>
<td align="left"><code>*,-?L#</code></td>
<td align="left"><code>?L#</code>只有部分软件实现了 Linux和Spring的允许值为0-7，0和7为周日 Quartz的允许值为1-7，1为周日</td>
</tr>
<tr>
<td align="left">Year</td>
<td align="left">否</td>
<td align="left">1970–2099</td>
<td align="left"><code>*,-</code></td>
<td align="left">标准实现不支持此字段。</td>
</tr>
</tbody></table>
</li>
<li><p>标准字段</p>
<ul>
<li><code>,</code>用于分隔列表。例如，在第5个字段（星期几）中使用<code>MON,WED,FRI</code>表示周一，周三和周五</li>
<li><code>-</code>字符定义范围。例如，<code>2000-2010</code>，表示2000年至2010年期间的每年，包括2000年和2010年</li>
</ul>
</li>
<li><p>非标准字段</p>
<ul>
<li><strong>“L”</strong>代表“Last”</li>
<li>“day of month”字段可以使用<strong>“W”</strong>字符。指定最接近给定日期的工作日（星期一-星期五）</li>
<li>分钟字段设置 <code>*/5</code>表示每5分钟一次，注意：这里指的是能被5整除的分钟数。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>JavaWeb</title>
    <url>/2021/05/26/JavaWeb/</url>
    <content><![CDATA[<h1 id="JavaWeb"><a href="#JavaWeb" class="headerlink" title="JavaWeb"></a>JavaWeb</h1><h2 id="网页的组成"><a href="#网页的组成" class="headerlink" title="网页的组成"></a>网页的组成</h2><ul>
<li>结构（HTML)<ul>
<li>超文本标记语言</li>
<li>网页的主要内容通过html来实现</li>
<li>用来写网页的语言</li>
</ul>
</li>
<li>表现（CSS)<ul>
<li>层叠样式表</li>
<li>网页的字体颜色、背景色、背景图片等通过它来实现</li>
<li>用来美化网页</li>
</ul>
</li>
<li>行为（JavaScript&#x2F;jQuery）<ul>
<li>用来实现网页上的一下动态的效果</li>
</ul>
</li>
<li>一个良好的网页要求结构、表现、行为三者分离</li>
</ul>
<h2 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h2><ul>
<li><p>常用的标签</p>
<ul>
<li><p>标题标签</p>
<ul>
<li>一共六个（h1到h6）</li>
</ul>
<figure class="highlight html"><table><tr><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">h1</span>&gt;</span><br>    一级标题<br><span class="hljs-tag">&lt;/<span class="hljs-name">h1</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">h2</span>&gt;</span><br>    二级标题<br><span class="hljs-tag">&lt;/<span class="hljs-name">h2</span>&gt;</span><br>...<br><span class="hljs-tag">&lt;<span class="hljs-name">h6</span>&gt;</span><br>    六级标题<br><span class="hljs-tag">&lt;/<span class="hljs-name">h6</span>&gt;</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>超链接</p>
<ul>
<li><p>通过a标签创建一个超链接</p>
<ul>
<li>通过a标签中的href属性指定要跳转的页面的地址</li>
</ul>
<figure class="highlight html"><table><tr><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;设置要跳转的页面的地址&quot;</span>&gt;</span>我是超链接<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>表格</p>
<ul>
<li>通过table标签创建一个表格<ul>
<li>表格中的行通过tr标签来表示<ul>
<li>表格中的表头通过th标签来表示</li>
<li>表格中的列（单元格）通过td标签来标签<ul>
<li>通过rowspan属性跨行合并单元格</li>
<li>通过colspan属性跨列合并单元格</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight html"><table><tr><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">table</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">tr</span>&gt;</span><br>    	<span class="hljs-tag">&lt;<span class="hljs-name">th</span>&gt;</span>姓名<span class="hljs-tag">&lt;/<span class="hljs-name">th</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">th</span>&gt;</span>性别<span class="hljs-tag">&lt;/<span class="hljs-name">th</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">th</span>&gt;</span>年龄<span class="hljs-tag">&lt;/<span class="hljs-name">th</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">tr</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">tr</span>&gt;</span><br>    	<span class="hljs-tag">&lt;<span class="hljs-name">td</span>&gt;</span>盖伦<span class="hljs-tag">&lt;/<span class="hljs-name">td</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">td</span>&gt;</span>男<span class="hljs-tag">&lt;/<span class="hljs-name">td</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">td</span>&gt;</span>19<span class="hljs-tag">&lt;/<span class="hljs-name">td</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">tr</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">table</span>&gt;</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>表单</p>
<ul>
<li>使用form标签创建一个表单<ul>
<li>通过action属性指定要提交的服务器的地址</li>
<li>通过method属性指定提交的请求方式</li>
<li>通过input标签创建表单项<ul>
<li>type值是text的是文本框</li>
<li>type值是password的是密码框</li>
<li>type值是submit的是提交按钮</li>
<li>type值不可用任意指定，通过alt+&#x2F;根据提示选择</li>
<li>必须给input指定name属性值<ul>
<li>name属性值可用任意指定</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight html"><table><tr><td class="code"><pre><code class="hljs html"><span class="hljs-comment">&lt;!-- 通过form标签创建一个表单 </span><br><span class="hljs-comment">		action属性：设置要提交的服务器的地址</span><br><span class="hljs-comment">		method属性：设置请求方式</span><br><span class="hljs-comment">			get：将发送一个GET请求，此时用户输入的数据是通过浏览器的地址栏进行传输</span><br><span class="hljs-comment">			post：将发送一个POST请求，此时用户输入的数据通过HTTP协议中请求报文中的请求体进行传输</span><br><span class="hljs-comment">	--&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">form</span> <span class="hljs-attr">action</span>=<span class="hljs-string">&quot;success.html&quot;</span> <span class="hljs-attr">method</span>=<span class="hljs-string">&quot;post&quot;</span>&gt;</span><br>		<span class="hljs-comment">&lt;!-- </span><br><span class="hljs-comment">			表单中的表单项通过input标签来创建，表单项的类型通过type属性来指定；</span><br><span class="hljs-comment">			必须给表单项指定name属性值，用户输入的数据通过name属性值进行携带，并以键值对的形式发送到</span><br><span class="hljs-comment">			服务器，多个键值对之间使用 &amp;符号分隔，例如：username=admin&amp;password=123456</span><br><span class="hljs-comment">		 --&gt;</span><br>		用户名称：<span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;username&quot;</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">br</span>&gt;</span><br>		用户密码：<span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;password&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;password&quot;</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">br</span>&gt;</span><br>		<span class="hljs-comment">&lt;!-- 提交按钮上显示的文字通过value指定 --&gt;</span><br>		<span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;submit&quot;</span> <span class="hljs-attr">value</span>=<span class="hljs-string">&quot;登录&quot;</span>&gt;</span><br>	<span class="hljs-tag">&lt;/<span class="hljs-name">form</span>&gt;</span><br></code></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="CSS"><a href="#CSS" class="headerlink" title="CSS"></a>CSS</h2><ul>
<li><p>CSS样式可用书写的位置</p>
<ul>
<li><p>1）写在标签的style属性中</p>
<ul>
<li>结构与表现相耦合，不建议使用</li>
</ul>
<figure class="highlight html"><table><tr><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;color: red;font-size: &quot;</span>&gt;</span>我是一个段落<span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br></code></pre></td></tr></table></figure>


</li>
<li><p>2）写在style标签中，style标签放在head标签中</p>
<ul>
<li>开发测试阶段使用这种方式</li>
</ul>
<figure class="highlight html"><table><tr><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">style</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text/css&quot;</span>&gt;</span><span class="language-css"></span><br><span class="language-css">	<span class="hljs-selector-id">#rd</span>&#123;</span><br><span class="language-css">		<span class="hljs-attribute">color</span>: <span class="hljs-number">#FBA</span></span><br><span class="language-css">	&#125;</span><br><span class="language-css"></span><span class="hljs-tag">&lt;/<span class="hljs-name">style</span>&gt;</span><br></code></pre></td></tr></table></figure>


</li>
<li><p>3）引入外部的css文件</p>
<ul>
<li>项目上线后使用这种方式</li>
</ul>
<figure class="highlight html"><table><tr><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">link</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;stylesheet&quot;</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text/css&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;style.css&quot;</span> /&gt;</span><br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>CSS种的基本选择器</p>
<ul>
<li>标签选择器</li>
</ul>
<figure class="highlight css"><table><tr><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">h1</span> &#123;<br>	<span class="hljs-attribute">color</span>: red<br>&#125;<br></code></pre></td></tr></table></figure>



<ul>
<li>ID选择器<ul>
<li>格式：#id属性值</li>
</ul>
</li>
</ul>
<figure class="highlight css"><table><tr><td class="code"><pre><code class="hljs css"><span class="hljs-selector-id">#p1</span> &#123;<br>	<span class="hljs-attribute">color</span>: green<br>&#125;<br></code></pre></td></tr></table></figure>



<ul>
<li>类选择器<ul>
<li>格式：.class属性值</li>
</ul>
</li>
</ul>
<figure class="highlight css"><table><tr><td class="code"><pre><code class="hljs css"><span class="hljs-selector-class">.p2</span> &#123;<br>	<span class="hljs-attribute">color</span>: blue<br>&#125;<br></code></pre></td></tr></table></figure>



<ul>
<li>分组选择器</li>
</ul>
<figure class="highlight css"><table><tr><td class="code"><pre><code class="hljs css"><span class="hljs-selector-id">#p1</span>, <span class="hljs-selector-class">.p2</span> &#123;<br>	<span class="hljs-attribute">font-size</span>: <span class="hljs-number">20px</span><br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="JavaScript"><a href="#JavaScript" class="headerlink" title="JavaScript"></a>JavaScript</h2><ul>
<li><p>JavaScript可用书写的位置跟CSS类似，一共三种方式</p>
</li>
<li><p>变量</p>
<ul>
<li>通过var关键字声明一个变量</li>
<li>在使用变量的过程种可用给它赋任意值</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><code class="hljs javascript"><span class="hljs-keyword">var</span> a; a=<span class="hljs-number">123</span>; a=<span class="hljs-string">&quot;hello&quot;</span>;a=函数；a=对象；<br><span class="hljs-keyword">var</span> a = <span class="hljs-string">&quot;javascript&quot;</span>;<br></code></pre></td></tr></table></figure>
</li>
<li><p>函数</p>
<ul>
<li>通过function关键字声明一个函数</li>
<li>在声明函数时不需要指定返回值的类型及形参的类型</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><code class="hljs javascript"><span class="hljs-comment">//方式一：</span><br><span class="hljs-comment">//不带参数的函数</span><br><span class="hljs-keyword">function</span> <span class="hljs-title function_">fun</span>(<span class="hljs-params"></span>)&#123;<br>    <span class="hljs-comment">//函数体</span><br>&#125;<br><span class="hljs-keyword">function</span> <span class="hljs-title function_">sum</span>(<span class="hljs-params">a,b</span>)&#123;<br>    <span class="hljs-keyword">return</span> a + b;<br>&#125;<br><span class="hljs-comment">//方式二：</span><br><span class="hljs-keyword">var</span> sum2 = <span class="hljs-keyword">function</span>(<span class="hljs-params">a,b,c</span>)&#123;<br>    <span class="hljs-keyword">return</span> a + b + c;<br>&#125;<br><span class="hljs-comment">//我们通常是通过方式二这种方式将一个函数赋给对象的事件属性</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>DOM</p>
<ul>
<li>全称：Document Object Model，文档对象模型</li>
<li>DOM种常用的属性和方法<ul>
<li>document.getElementById(“id属性值”)<ul>
<li>根据标签的id属性值获取一个具体的标签对象</li>
</ul>
</li>
<li>对象.innerHTML<ul>
<li>获取或设置成对出现的标签中的文本内容<ul>
<li>对象.innerHTML<ul>
<li>获取文本内容</li>
</ul>
</li>
<li>对象.innerHTML&#x3D;”new valule”<ul>
<li>设置文本内容</li>
</ul>
</li>
</ul>
</li>
<li>在jQuery中与之对应的是text()&#x2F;html()方法</li>
</ul>
</li>
<li>对象.onclick<ul>
<li>给对象绑定单击事件</li>
<li>在jQuery中与之对应的是click()方法</li>
</ul>
</li>
<li>对象.onfocus<ul>
<li>给对象绑定获取焦点事件</li>
<li>在jQuery中与之对应的是focus()方法</li>
</ul>
</li>
<li>对象.onblur<ul>
<li>给对象绑定失去焦点的事件</li>
<li>在jQuery中与之对应的是blur()方法</li>
</ul>
</li>
<li>对象.onchange<ul>
<li>给对象内容改变的事件</li>
<li>在jQuery中与之对应的是change()方法</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Servlet"><a href="#Servlet" class="headerlink" title="Servlet"></a>Servlet</h2><ul>
<li>Servlet是服务器端的一个组件，用来处理用户的请求</li>
<li>直接new一个Sersvlet，然后配置映射的请求地址即可</li>
<li>doGet和doPost方法中的两个参数request和response的作用<ul>
<li>request的作用<ul>
<li>获取请求参数</li>
<li>转发</li>
<li>它是一个域对象</li>
</ul>
</li>
<li>response的作用<ul>
<li>给浏览器响应一个字符串或一个页面</li>
<li>重定向</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-comment">//处理GET请求的方法</span><br>	<span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">doGet</span><span class="hljs-params">(HttpServletRequest request, HttpServletResponse response)</span> <span class="hljs-keyword">throws</span> ServletException, IOException &#123;<br>		System.out.println(<span class="hljs-string">&quot;doGet方法被调用&quot;</span>);<br>		<span class="hljs-comment">//request的作用</span><br>		<span class="hljs-comment">//1.获取请求参数</span><br>		<span class="hljs-comment">/*</span><br><span class="hljs-comment">		 * GET请求的请求中文乱码问题的解决方案：</span><br><span class="hljs-comment">		 * 在Tomcat的配置文件server.xml中的第一个Connector标签中添加属性URIEncoding=&quot;UTF-8&quot;</span><br><span class="hljs-comment">		 */</span><br>		<span class="hljs-type">String</span> <span class="hljs-variable">username</span> <span class="hljs-operator">=</span> request.getParameter(<span class="hljs-string">&quot;username&quot;</span>);<br>		<span class="hljs-type">String</span> <span class="hljs-variable">password</span> <span class="hljs-operator">=</span> request.getParameter(<span class="hljs-string">&quot;password&quot;</span>);<br>		System.out.println(username);<br>		System.out.println(password);<br>		<span class="hljs-comment">//2.转发</span><br>		<span class="hljs-comment">//获取转发器</span><br>		<span class="hljs-type">RequestDispatcher</span> <span class="hljs-variable">requestDispatcher</span> <span class="hljs-operator">=</span> request.getRequestDispatcher(<span class="hljs-string">&quot;WEB-INF/success.html&quot;</span>);<br>		<span class="hljs-comment">//进行请求的转发</span><br>		requestDispatcher.forward(request, response);<br>		<span class="hljs-comment">//3.request是一个域对象（下回分解）</span><br>	&#125;<br>	<span class="hljs-comment">//处理POST请求的方法</span><br>	<span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">doPost</span><span class="hljs-params">(HttpServletRequest request, HttpServletResponse response)</span> <span class="hljs-keyword">throws</span> ServletException, IOException &#123;<br>		System.out.println(<span class="hljs-string">&quot;doPost方法被调用&quot;</span>);<br>		<span class="hljs-comment">/*</span><br><span class="hljs-comment">		 * POST请求的请求中文乱码问题的解决方案：</span><br><span class="hljs-comment">		 * 在第一次获取请求参数之前，通过request设置字符集位UTF-8</span><br><span class="hljs-comment">		 */</span><br>		request.setCharacterEncoding(<span class="hljs-string">&quot;UTF-8&quot;</span>);<br>		<span class="hljs-type">String</span> <span class="hljs-variable">username</span> <span class="hljs-operator">=</span> request.getParameter(<span class="hljs-string">&quot;username&quot;</span>);<br>		<span class="hljs-type">String</span> <span class="hljs-variable">password</span> <span class="hljs-operator">=</span> request.getParameter(<span class="hljs-string">&quot;password&quot;</span>);<br>		System.out.println(username);<br>		System.out.println(password);<br>		<span class="hljs-comment">//response的作用</span><br>		<span class="hljs-comment">//1.给浏览器响应一个字符串或一个页面</span><br>		<span class="hljs-comment">/*</span><br><span class="hljs-comment">		 * 响应中文乱码的解决方案：</span><br><span class="hljs-comment">		 * 	在获取流之前设置内容的类型，内容的类型中包含UTF-8字符集</span><br><span class="hljs-comment">		 */</span><br>		response.setContentType(<span class="hljs-string">&quot;text/html;charset=UTF-8&quot;</span>);<br>		<span class="hljs-type">PrintWriter</span> <span class="hljs-variable">writer</span> <span class="hljs-operator">=</span> response.getWriter();<br><span class="hljs-comment">//		writer.write(&quot;Response Success!&quot;);</span><br><span class="hljs-comment">//		writer.write(&quot;响应成功！&quot;);</span><br><span class="hljs-comment">//		writer.write(&quot;&lt;!DOCTYPE html&gt;\r\n&quot; + </span><br><span class="hljs-comment">//				&quot;&lt;html&gt;\r\n&quot; + </span><br><span class="hljs-comment">//				&quot;&lt;head&gt;\r\n&quot; + </span><br><span class="hljs-comment">//				&quot;&lt;meta charset=\&quot;UTF-8\&quot;&gt;\r\n&quot; + </span><br><span class="hljs-comment">//				&quot;&lt;title&gt;Insert title here&lt;/title&gt;\r\n&quot; + </span><br><span class="hljs-comment">//				&quot;&lt;/head&gt;\r\n&quot; + </span><br><span class="hljs-comment">//				&quot;&lt;body&gt;\r\n&quot; + </span><br><span class="hljs-comment">//				&quot;	&lt;h1&gt;请求处理成功！&lt;/h1&gt;\r\n&quot; + </span><br><span class="hljs-comment">//				&quot;&lt;/body&gt;\r\n&quot; + </span><br><span class="hljs-comment">//				&quot;&lt;/html&gt;&quot;);</span><br>		<span class="hljs-comment">//2.重定向</span><br>		response.sendRedirect(<span class="hljs-string">&quot;WEB-INF/success.html&quot;</span>);<br>		<br>		<span class="hljs-comment">/*</span><br><span class="hljs-comment">		 * 转发与重定向的区别：</span><br><span class="hljs-comment">		 * 	1.转发浏览器发送一次请求；重定向浏览器发送两次请求</span><br><span class="hljs-comment">		 * 	2.转发浏览器地址栏地址无变化；重定向浏览器地址栏地址有变化</span><br><span class="hljs-comment">		 * 	3.转发可以访问WEB-INF目录下的资源；重定向不可以访问WEB-INF目录下的资源</span><br><span class="hljs-comment">		 * 	4.转发可以共享request域中的数据；重定向不可以共享request域中的数据</span><br><span class="hljs-comment">		 */</span><br>	&#125;<br><br></code></pre></td></tr></table></figure>

<h2 id="JSP"><a href="#JSP" class="headerlink" title="JSP"></a>JSP</h2><ul>
<li><p>JSP必须运行服务器上，它本质上是一个Servlet</p>
</li>
<li><p>HTML和Servlet能实现的功能JSP都可用实现</p>
</li>
<li><p>JSP中的基本语法</p>
<ul>
<li>JSP脚本片段<ul>
<li>格式：&lt;%   %&gt;</li>
<li>作用：在里面写Java代码</li>
</ul>
</li>
<li>JSP表达式<ul>
<li>格式：&lt;%&#x3D;  %&gt;</li>
<li>作用：用来输出对象</li>
</ul>
</li>
</ul>
<figure class="highlight jsp"><table><tr><td class="code"><pre><code class="hljs jsp">&lt;%<br>		<span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span> ; i &lt; <span class="hljs-number">10</span> ; i++)&#123;<br>			<span class="hljs-comment">//out.print(&quot;伊朗要干美国了！&quot;);</span><br>	%&gt;<br>	&lt;h1&gt;伊朗要干美国了！&lt;/h1&gt;<br>	&lt;%	<br>		&#125;<br>	%&gt;<br><br> &lt;%=<span class="hljs-string">&quot;我是通过JSP表达式输出的&quot;</span> %&gt;<br></code></pre></td></tr></table></figure>
</li>
<li><p>四个域</p>
<ul>
<li>page域<ul>
<li>范围：当前页面</li>
<li>对应的域对象：pageContext</li>
<li>域对象的类型：PageContext</li>
</ul>
</li>
<li>request域<ul>
<li>范围：当前请求（一次请求）</li>
<li>对应的域对象：request</li>
<li>域对象的类型：HttpServletRequest</li>
</ul>
</li>
<li>session<ul>
<li>范围：当前会话（一次会话）</li>
<li>对应的域对象：session</li>
<li>域对象的类型：HttpSession</li>
</ul>
</li>
<li>application域<ul>
<li>范围：当前Web应用</li>
<li>对应的域对象：application</li>
<li>域对象的类型：ServletContext</li>
</ul>
</li>
<li>四个域对象都有以下三个方法<ul>
<li>void setAttribute(String key , Object value)</li>
<li>Object getAttribute(String key)</li>
<li>void removeAttribute(String key)</li>
</ul>
</li>
</ul>
<figure class="highlight jsp"><table><tr><td class="code"><pre><code class="hljs jsp">&lt;!-- 在当前页面中分别向四个域中添加四个属性 --&gt;<br>	 &lt;%<br>	 	pageContext.setAttribute(<span class="hljs-string">&quot;pageKey&quot;</span>, <span class="hljs-string">&quot;pageValue&quot;</span>);<br>	 	request.setAttribute(<span class="hljs-string">&quot;reqKey&quot;</span>, <span class="hljs-string">&quot;reqValue&quot;</span>);<br>	 	session.setAttribute(<span class="hljs-string">&quot;sessKey&quot;</span>, <span class="hljs-string">&quot;sessValue&quot;</span>);<br>	 	application.setAttribute(<span class="hljs-string">&quot;appKey&quot;</span>, <span class="hljs-string">&quot;appValue&quot;</span>);<br>	 %&gt;<br>	 &lt;h1&gt;在当前页面中分别获取四个域中的属性值&lt;/h1&gt;<br>	 page域中的属性值是：&lt;%=pageContext.getAttribute(<span class="hljs-string">&quot;pageKey&quot;</span>) %&gt;&lt;br&gt;<br>	 request域中的属性值是：&lt;%=request.getAttribute(<span class="hljs-string">&quot;reqKey&quot;</span>) %&gt;&lt;br&gt;<br>	 session域中的属性值是：&lt;%=session.getAttribute(<span class="hljs-string">&quot;sessKey&quot;</span>) %&gt;&lt;br&gt;<br>	 application域中的属性值是：&lt;%=application.getAttribute(<span class="hljs-string">&quot;appKey&quot;</span>) %&gt;&lt;br&gt;<br></code></pre></td></tr></table></figure>
</li>
<li><p>EL</p>
<ul>
<li>EL全称是Expression Language，是JSP的内置表达式</li>
<li>格式：${表达式}</li>
<li>作用：主要用来获取域对象中的属性值，用来代替JSP的表达式</li>
<li>EL表达式获取数据时才输出，获取不到数据则什么也不输出</li>
<li>EL中的四个Scope对象<ul>
<li>pageScope<ul>
<li>获取page域中的属性值</li>
</ul>
</li>
<li>requestScope<ul>
<li>获取request域中的属性值</li>
</ul>
</li>
<li>sessionScope<ul>
<li>获取session域中的属性值</li>
</ul>
</li>
<li>applicationScope<ul>
<li>获取application域中的属性值</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight jsp"><table><tr><td class="code"><pre><code class="hljs jsp">&lt;%<br>	 	<span class="hljs-type">Date</span> <span class="hljs-variable">date</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Date</span>();<br>	 	<span class="hljs-comment">//分别向四个域中添加四个属性</span><br>	 	pageContext.setAttribute(<span class="hljs-string">&quot;date&quot;</span>, date+<span class="hljs-string">&quot;-&quot;</span>);<br>	 	request.setAttribute(<span class="hljs-string">&quot;date&quot;</span>, date+<span class="hljs-string">&quot;--&quot;</span>);<br>	 	session.setAttribute(<span class="hljs-string">&quot;date&quot;</span>, date+<span class="hljs-string">&quot;---&quot;</span>);<br>	 	application.setAttribute(<span class="hljs-string">&quot;date&quot;</span>, date+<span class="hljs-string">&quot;----&quot;</span>);<br>	 %&gt;<br>	 通过JSP表达式获取域对象中的属性值：&lt;%=pageContext.getAttribute(<span class="hljs-string">&quot;date&quot;</span>) %&gt;&lt;br&gt;<br>	 通过EL表达式获取域对象中的属性值：$&#123;date &#125;&lt;br&gt;<br>	 通过EL表达式获取request域中的属性值：$&#123;requestScope.date &#125;<br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="Ajax"><a href="#Ajax" class="headerlink" title="Ajax"></a>Ajax</h2><ul>
<li><p>全称：Asynchronous JavaScript And XML，通过JavaScript发送请求，使用XML作为响应数据，后来XML已经被另外一种数据格式JSON所替代</p>
</li>
<li><p>同步请求和异步请求的区别</p>
<ul>
<li>同步请求<ul>
<li>发送请求之后必须等待服务器的响应成功之后才能发送其他请求，有一个等待的过程</li>
<li>响应成功之后会刷新整个页面</li>
</ul>
</li>
<li>异步请求<ul>
<li>发送请求之后无需等待服务器的响应即可发送其他请求</li>
<li>响应成功之后不会刷新整个页面，可用局部更新页面中的内容</li>
</ul>
</li>
</ul>
</li>
<li><p>如果通过jQuery发送ajax请求</p>
<ul>
<li><p>使用$.ajax()方法发送Ajax请求</p>
<ul>
<li>ajax()方法中的常用选项<ul>
<li>url<ul>
<li>必须的。用来设置请求地址，值是一个字符串</li>
</ul>
</li>
<li>type<ul>
<li>可选的。用来设置请求方式。GET或POST，默认是GET，值是一个字符串</li>
</ul>
</li>
<li>data<ul>
<li>可选的。用来设置请求参数，值是一个字符串</li>
</ul>
</li>
<li>success<ul>
<li>可选的。用来设置一个回调函数，当响应成功之后系统会自动调用该函数，响应数据会以参数的形式传入到该函数中</li>
</ul>
</li>
<li>dataType<ul>
<li>可选的。用来设置响应数据的类型，如 text、json等</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><code class="hljs javascript">$.<span class="hljs-title function_">ajax</span>(&#123;<br>				<span class="hljs-attr">url</span>:<span class="hljs-string">&quot;AjaxServlet&quot;</span>,<br>				<span class="hljs-attr">type</span>:<span class="hljs-string">&quot;get&quot;</span>,<br>				<span class="hljs-attr">data</span>:<span class="hljs-string">&quot;username=admin&amp;password=123456&quot;</span>,<br>				<span class="hljs-attr">success</span>:<span class="hljs-keyword">function</span>(<span class="hljs-params">res</span>)&#123;<br>					<span class="hljs-comment">//将响应数据设置到span标签中</span><br>					$(<span class="hljs-string">&quot;#msg&quot;</span>).<span class="hljs-title function_">text</span>(res);<br>				&#125;,<br>				<span class="hljs-attr">dataType</span>:<span class="hljs-string">&quot;text&quot;</span><br>			&#125;);<br></code></pre></td></tr></table></figure>
</li>
<li><p>JSON</p>
<ul>
<li>JOSN格式<ul>
<li>JSON对象</li>
<li>JSON数组</li>
</ul>
</li>
<li>JSON中接受的数据类型<ul>
<li>字符串</li>
<li>数字</li>
<li>null</li>
<li>布尔类型</li>
<li>数组</li>
<li>对象</li>
</ul>
</li>
<li>在JS中JSON对象和JSON字符串之间的转换<ul>
<li>JSON对象转JSON字符串<ul>
<li>JSON.stringify(JSON对象)</li>
</ul>
</li>
<li>JSON字符串转JSON对象<ul>
<li>JSON.parse(JSON字符串)</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><code class="hljs javascript"><span class="hljs-comment">//JSON的格式：</span><br>	<span class="hljs-comment">//1.JSON对象</span><br>	<span class="hljs-comment">//属性名必须使用双引号括起来；属性名和属性值之间使用冒号分隔；多个属性之间使用逗号分隔</span><br>	<span class="hljs-keyword">var</span> jsonObj = &#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;孙悟空&quot;</span>,<span class="hljs-string">&quot;age&quot;</span>:<span class="hljs-number">520</span>&#125;;<br><span class="hljs-comment">// 	alert(jsonObj);</span><br>	<span class="hljs-comment">//2.JSON数组</span><br>	<span class="hljs-keyword">var</span> jsonArry = [<span class="hljs-string">&quot;猪八戒&quot;</span>,<span class="hljs-number">1500</span>,<span class="hljs-literal">true</span>,<span class="hljs-literal">null</span>,jsonObj];<br>	<span class="hljs-comment">//获取jsonArry中的jsonObj中的age属性值</span><br><span class="hljs-comment">// 	alert(jsonArry[4].age);</span><br>	<br>	<span class="hljs-comment">//在JS中将JSON对象转换为JSON字符串</span><br>	<span class="hljs-keyword">var</span> objToStr = <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(jsonObj);<br><span class="hljs-comment">// 	alert(objToStr);</span><br>	<span class="hljs-comment">//声明一个JSON字符串</span><br>	<span class="hljs-keyword">var</span> jsonStr = <span class="hljs-string">&#x27;&#123;&quot;name&quot;:&quot;唐僧&quot;,&quot;age&quot;:18&#125;&#x27;</span>;<br><span class="hljs-comment">// 	alert(jsonStr);</span><br>	<span class="hljs-comment">//在JS中将JSON字符串转换为JSON对象</span><br>	<span class="hljs-keyword">var</span> strToObj = <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">parse</span>(jsonStr);<br><span class="hljs-comment">// 	alert(strToObj.name);</span><br></code></pre></td></tr></table></figure>

<ul>
<li><p>在Java中对象与JSON字符串之间的转换</p>
<ul>
<li><p>借助于第三方工具json-lib、jackson、gson等可用将Java对象转换为JSON字符串，也可用将JSON字符串转换回Java对象</p>
</li>
<li><p>通常是前端发送一个Ajax请求，在后台查询到对象之后将对象转换为JOSN字符串响应到前端</p>
</li>
<li><p>通过发送Ajax请求接收JSON格式的响应数据</p>
<ul>
<li>前端代码</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="code"><pre><code class="hljs javascript"><span class="hljs-comment">//发送Ajax请求接收JSON格式的响应数据</span><br>			$.<span class="hljs-title function_">ajax</span>(&#123;<br>				<span class="hljs-attr">url</span>:<span class="hljs-string">&quot;JSONServlet&quot;</span>,<br>				<span class="hljs-attr">success</span>:<span class="hljs-keyword">function</span>(<span class="hljs-params">res</span>)&#123;<br>					<span class="hljs-title function_">alert</span>(res.<span class="hljs-property">id</span>);<br>				&#125;,<br>				<span class="hljs-attr">dataType</span>:<span class="hljs-string">&quot;json&quot;</span><br>			&#125;);<br></code></pre></td></tr></table></figure>



<ul>
<li>后端代码</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">doGet</span><span class="hljs-params">(HttpServletRequest request, HttpServletResponse response)</span><br>			<span class="hljs-keyword">throws</span> ServletException, IOException &#123;<br>		response.setContentType(<span class="hljs-string">&quot;text/html;charset=UTF-8&quot;</span>);<br>		<span class="hljs-comment">//假设从数据库中查询到员工的信息</span><br>		<span class="hljs-type">Employee</span> <span class="hljs-variable">employee</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Employee</span>(<span class="hljs-number">1</span>, <span class="hljs-string">&quot;张三&quot;</span>, <span class="hljs-string">&quot;zhangsan@atguigu.com&quot;</span>);<br>		<span class="hljs-comment">//创建Gson对象</span><br>		<span class="hljs-type">Gson</span> <span class="hljs-variable">gson</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Gson</span>();<br>		<span class="hljs-comment">//将Employee对象转换为JSON字符串</span><br>		<span class="hljs-type">String</span> <span class="hljs-variable">json</span> <span class="hljs-operator">=</span> gson.toJson(employee);<br>		System.out.println(json);<br>		<span class="hljs-comment">//给浏览器响应一个JSON格式的字符串</span><br>		response.getWriter().write(json);<br>	&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark</title>
    <url>/2021/08/27/Spark/</url>
    <content><![CDATA[<h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><h2 id="Spark入门"><a href="#Spark入门" class="headerlink" title="Spark入门"></a>Spark入门</h2><ul>
<li><p>spark是一种基于内存的快速，通用，可扩展的大数据分析计算引擎。</p>
</li>
<li><p>spark内置模块</p>
<p><a href="https://imgtu.com/i/h16ZB4"><img src="https://z3.ax1x.com/2021/08/28/h16ZB4.png" alt="h16ZB4.png"></a></p>
<ul>
<li>spark core：实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core中还包含了对弹性分布式数据集(Resilient Distributed DataSet，简称RDD)的API定义。 </li>
<li>Spark SQL：是Spark用来操作结构化数据的程序包。通过Spark SQL，我们可以使用 SQL或者Apache Hive版本的HQL来查询数据。Spark SQL支持多种数据源，比如Hive表、Parquet以及JSON等。</li>
<li>Spark Streaming：是Spark提供的对实时数据进行流式计算的组件。提供了用来操作数据流的API，并且与Spark Core中的 RDD API高度对应。 </li>
<li>Spark MLlib：提供常见的机器学习功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。 </li>
<li>Spark GraphX：主要用于图形并行计算和图挖掘系统的组件。</li>
<li>集群管理器：Spark设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算。为了实现这样的要求，同时获得最大灵活性，Spark支持在各种集群管理器(Cluster Manager)上运行，包括Hadoop YARN、Apache Mesos，以及Spark自带的一个简易调度器，叫作独立调度器。</li>
</ul>
</li>
</ul>
<h3 id="Spark运行模式"><a href="#Spark运行模式" class="headerlink" title="Spark运行模式"></a>Spark运行模式</h3><ul>
<li><p>运行模式:单机模式，集群模式</p>
<ul>
<li>Local模式：本地部署单个spark模式</li>
<li>Standalone：spark自带的任务调度模式</li>
<li>YARN模式：spark使用Hadoop的YARN组件进行资源与任务调度。（重点）<ul>
<li>修改&#x2F;opt&#x2F;module&#x2F;spark&#x2F;conf&#x2F;spark-env.sh，添加YARN_CONF_DIR配置:YARN_CONF_DIR&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;etc&#x2F;hadoop，保证后续运行任务的路径都变成集群路径</li>
</ul>
</li>
<li>Mesos模式：saprk使用Mesos平台进行资源与任务的调度。</li>
</ul>
</li>
<li><p>运行流程</p>
<ul>
<li>Spark有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程序的运行节点。</li>
<li>yarn-client:Driver程序运行在客户端，适用于交互，调试，希望立即看到app的输出。</li>
<li>yarn-cluster：Driver程序运行在由ResourceManager启动的APPMaster，适用于生产环境。</li>
</ul>
</li>
<li><p>端口号总结</p>
<ul>
<li>spark历史服务器端口号：18080 						hadoop历史服务器端口号：19888</li>
<li>spark Master web服务器端口号：8080             Hadoop的namenode web端口号：9870</li>
<li>spark Master内部通信服务端口号：7077           Hadoop的namenode内部通信服务端口号：8020</li>
<li>Spark查看当前Spark-shell运行任务情况端口号：4040</li>
<li>hadoop yarn任务运行情况查看端口号：8088</li>
</ul>
</li>
</ul>
<h2 id="SparkCore"><a href="#SparkCore" class="headerlink" title="SparkCore"></a>SparkCore</h2><h3 id="RDD概述"><a href="#RDD概述" class="headerlink" title="RDD概述"></a>RDD概述</h3><ul>
<li><p>RDD（Resilient Distributed Dataset）弹性分布式数据集，是Spark中最基本的数据抽象。</p>
<ul>
<li><a href="https://imgtu.com/i/hdgUNq"><img src="https://z3.ax1x.com/2021/08/31/hdgUNq.png" alt="hdgUNq.png"></a></li>
</ul>
</li>
<li><p>RDD特性</p>
<p><a href="https://imgtu.com/i/hwcbRK"><img src="https://z3.ax1x.com/2021/09/01/hwcbRK.png" alt="hwcbRK.png"></a></p>
</li>
<li><p>yarn模式下的sparkWordCount实现案例：大致流程</p>
<ul>
<li><a href="https://imgtu.com/i/hdg2U1"><img src="https://z3.ax1x.com/2021/08/31/hdg2U1.png" alt="hdg2U1.png"></a></li>
<li><a href="https://imgtu.com/i/hwccGV"><img src="https://z3.ax1x.com/2021/09/01/hwccGV.png" alt="hwccGV.png"></a></li>
<li>任务从客户端发送到ReourceManager，RM指定一个NM创建一个AppMster进程，创建spark的一个Driver线程，Driver线程将任务分配到其它的NM，其它的NodeManager申请container，container会创建spark的一个excutor线程，RDD会根据分区进行一系列transformations转换定义，也会在一些情况下有shuffer过程，程序不会立刻执行，而是直到调用action触发调用RDD的计算。</li>
</ul>
</li>
</ul>
<h3 id="RDD编程"><a href="#RDD编程" class="headerlink" title="RDD编程"></a>RDD编程</h3><ul>
<li>在spark中只有遇到action（如rdd.collect）才会执行RDD的计算（即延迟计算）</li>
</ul>
<h4 id="RDD的创建"><a href="#RDD的创建" class="headerlink" title="RDD的创建"></a>RDD的创建</h4><ul>
<li>从集合中创建</li>
<li>从外部存储（HDFS，Hbase）创建</li>
<li>通过RDD的转换算子转换</li>
</ul>
<h4 id="分区规则"><a href="#分区规则" class="headerlink" title="分区规则"></a>分区规则</h4><ul>
<li><p>分区数：</p>
<ul>
<li><p>创建RDD的方法都可以指定分区数</p>
</li>
<li><p>如果SparkConf.setMaster(local[*])，从集合创建默认是cpu核数，从外部存储创建默认是math.min(分配给应用的CPU核数,2)</p>
</li>
</ul>
</li>
<li><p>数据分区规则：</p>
<ul>
<li>通过集合创建RDD的情况：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">positions</span></span>(length: <span class="hljs-type">Long</span>, numSlices: <span class="hljs-type">Int</span>): <span class="hljs-type">Iterator</span>[(<span class="hljs-type">Int</span>, <span class="hljs-type">Int</span>)] = &#123;<br>      (<span class="hljs-number">0</span> until numSlices).iterator.map &#123; i =&gt;<br>        <span class="hljs-keyword">val</span> start = ((i * length) / numSlices).toInt<br>        <span class="hljs-keyword">val</span> end = (((i + <span class="hljs-number">1</span>) * length) / numSlices).toInt<br>        (start, end)<br>      &#125;<br>    &#125;<br><span class="hljs-comment">//通过返回这样一个左闭右开的迭代元组对象，元组里面是以一组组下标，根据下标划分集合元素</span><br></code></pre></td></tr></table></figure>

<ul>
<li><p>通过外部存储创建RDD的情况：</p>
<ul>
<li>默认分区规则math.min(分配给应用的CPU核数,2)</li>
<li>指定分区：在TextFile方法中，第二个参数minPartitions，表示最小分区数（是最小，不代表实际的分区数）</li>
<li>在实际计算分区个数时，会根据文件的总大小和最小分区数进行相除运算，如果余数为0，那最小分区数就是实际分区数，否则需要进一步计算得到分区数。</li>
<li>切片规划：调用FileInputFormat中的getSplits方法</li>
<li>注意：getSplits文件返回的是切片规划，真正读取是在compute方法中创建LineRecordReader读取的，有两个关键变量start&#x3D;split.getStart()	  end &#x3D; start + split.getLength</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala">public <span class="hljs-type">InputSplit</span>[] getSplits(<span class="hljs-type">JobConf</span> job, int numSplits)<br>  <span class="hljs-keyword">throws</span> <span class="hljs-type">IOException</span> &#123;<br>  <span class="hljs-type">FileStatus</span>[] files = listStatus(job);<br>  <span class="hljs-comment">// 输入的文件数量</span><br>  <span class="hljs-comment">// Save the number of input files for metrics/loadgen</span><br>  job.setLong(<span class="hljs-type">NUM_INPUT_FILES</span>, files.length);<br>  long totalSize = <span class="hljs-number">0</span>;                           <span class="hljs-comment">// compute total size</span><br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">FileStatus</span> file: files) &#123;                <span class="hljs-comment">// check we have valid files</span><br>    <span class="hljs-keyword">if</span> (file.isDirectory()) &#123;<br>      <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-type">IOException</span>(<span class="hljs-string">&quot;Not a file: &quot;</span>+ file.getPath());<br>    &#125;<br>    totalSize += file.getLen();<br>  &#125;<br>    <br>  long goalSize = totalSize / (numSplits == <span class="hljs-number">0</span> ? <span class="hljs-number">1</span> : numSplits);<br>  long minSize = <span class="hljs-type">Math</span>.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.<br>    <span class="hljs-type">FileInputFormat</span>.<span class="hljs-type">SPLIT_MINSIZE</span>, <span class="hljs-number">1</span>), minSplitSize);<br>    <br>  <span class="hljs-comment">// generate splits</span><br>  <span class="hljs-type">ArrayList</span>&lt;<span class="hljs-type">FileSplit</span>&gt; splits = <span class="hljs-keyword">new</span> <span class="hljs-type">ArrayList</span>&lt;<span class="hljs-type">FileSplit</span>&gt;(numSplits);<br>  <span class="hljs-type">NetworkTopology</span> clusterMap = <span class="hljs-keyword">new</span> <span class="hljs-type">NetworkTopology</span>();<br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">FileStatus</span> file: files) &#123;<br>    <span class="hljs-type">Path</span> path = file.getPath();<br>    long length = file.getLen();<br>    <span class="hljs-keyword">if</span> (length != <span class="hljs-number">0</span>) &#123;<br>      <span class="hljs-type">FileSystem</span> fs = path.getFileSystem(job);<br>      <span class="hljs-type">BlockLocation</span>[] blkLocations;<br>      <span class="hljs-keyword">if</span> (file instanceof <span class="hljs-type">LocatedFileStatus</span>) &#123;<br>        blkLocations = ((<span class="hljs-type">LocatedFileStatus</span>) file).getBlockLocations();<br>      &#125; <span class="hljs-keyword">else</span> &#123;<br>        blkLocations = fs.getFileBlockLocations(file, <span class="hljs-number">0</span>, length);<br>      &#125;<br>      <span class="hljs-keyword">if</span> (isSplitable(fs, path)) &#123;<br>        long blockSize = file.getBlockSize();<br>        long splitSize = computeSplitSize(goalSize, minSize, blockSize);<br>    <br>        long bytesRemaining = length;<br>        <span class="hljs-keyword">while</span> (((double) bytesRemaining)/splitSize &gt; <span class="hljs-type">SPLIT_SLOP</span>) &#123;<br>          <span class="hljs-type">String</span>[] splitHosts = getSplitHosts(blkLocations,<br>              length-bytesRemaining, splitSize, clusterMap);<br>          splits.add(makeSplit(path, length-bytesRemaining, splitSize,<br>              splitHosts));<br>          bytesRemaining -= splitSize;<br>        &#125;<br>    <br>        <span class="hljs-keyword">if</span> (bytesRemaining != <span class="hljs-number">0</span>) &#123;<br>          <span class="hljs-type">String</span>[] splitHosts = getSplitHosts(blkLocations, length<br>              - bytesRemaining, bytesRemaining, clusterMap);<br>          splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,<br>              splitHosts));<br>        &#125;<br>      &#125; <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-type">String</span>[] splitHosts = getSplitHosts(blkLocations,<span class="hljs-number">0</span>,length,clusterMap);<br>        splits.add(makeSplit(path, <span class="hljs-number">0</span>, length, splitHosts));<br>      &#125;<br>    &#125; <span class="hljs-keyword">else</span> &#123; <br>      <span class="hljs-comment">//Create empty hosts array for zero length files</span><br>      splits.add(makeSplit(path, <span class="hljs-number">0</span>, length, <span class="hljs-keyword">new</span> <span class="hljs-type">String</span>[<span class="hljs-number">0</span>]));<br>    &#125;<br>  &#125;<br>  <span class="hljs-type">LOG</span>.debug(<span class="hljs-string">&quot;Total # of splits: &quot;</span> + splits.size());<br>  <span class="hljs-keyword">return</span> splits.toArray(<span class="hljs-keyword">new</span> <span class="hljs-type">FileSplit</span>[splits.size()]);<br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="Transformation转换算子（重点）"><a href="#Transformation转换算子（重点）" class="headerlink" title="Transformation转换算子（重点）"></a>Transformation转换算子（重点）</h3><ul>
<li>算子：从认知心理学角度来讲，解决问题其实是将问题的初始状态，通过一系列的转换操作（operator），变成解决状态。</li>
<li>转换算子（Transformation）执行完毕之后，会创建新的RDD，并不会马上执行计算。</li>
<li>行动算子（Action）执行后，才会触发计算。</li>
</ul>
<h4 id="Value类型"><a href="#Value类型" class="headerlink" title="Value类型"></a>Value类型</h4><ul>
<li><p>map</p>
<ul>
<li>对RDD中的元素进行一个个映射</li>
<li>新RDD中的每一个元素都是原来RDD中每一个元素依次应用f函数而得到的。</li>
</ul>
</li>
<li><p>mapPartitions</p>
<ul>
<li>以分区为单位，对RDD中的元素进行映射</li>
</ul>
</li>
<li><p>mapPartitionsWithIndex</p>
<ul>
<li>以分区为单位，对RDD中的元素进行映射，并且带分区编号</li>
</ul>
</li>
<li><p>flatMap</p>
<ul>
<li>对RDD中的元素进行扁平化处理</li>
<li>在flatMap操作中，f函数的返回值是一个集合，并且会将每一个该集合的元素拆分出来放到新的RDD中。</li>
</ul>
</li>
<li><p>glom</p>
<ul>
<li>将RDD中每一个分区中的单个元素，转换为数组</li>
</ul>
</li>
<li><p>groupBy</p>
<ul>
<li>按照一定的规则，对RDD中的元素进行分组</li>
<li>按照传入函数的返回值进行分组，将相同的key对应的值放入一个迭代器</li>
</ul>
</li>
<li><p>filter</p>
<ul>
<li>按照一定的规则，对RDD中的元素进行过滤</li>
</ul>
</li>
<li><p>sample</p>
<ul>
<li><p>对RDD中的元素进行抽样</p>
</li>
<li><p>参数一：是否抽象放回   true放回，false不放回</p>
</li>
<li><p>参数二 ：参数一以为true时，放回抽样，参数二代表期望元素出现的次数   参数大于0</p>
</li>
<li><p>参数二：参数一为false时，不放回抽样，参数二代表每一个元素出现的概率[0,1]</p>
</li>
<li><p>参数三：随机算法的初始种子</p>
</li>
<li><p>takeSample（行动算子）</p>
</li>
</ul>
</li>
<li><p>distinct</p>
<ul>
<li>去重</li>
<li>底层是通过map+reduceByKey完成去重操作</li>
</ul>
</li>
<li><p>改变分区</p>
<ul>
<li><p>def coalesce(numPartitions: Int, shuffle: Boolean &#x3D; false,<br>           partitionCoalescer: Option[PartitionCoalescer] &#x3D; Option.empty)<br>          (implicit ord: Ordering[T] &#x3D; null) : RDD[T]</p>
</li>
<li><p>coalesce   一般用于缩减分区，默认不执行shuffle</p>
</li>
<li><p>reparation  一般用于扩大分区，默认执行shuffle，底层调用的就是coalesce</p>
</li>
</ul>
</li>
<li><p>sortBy</p>
<ul>
<li>按照指定规则，对RDD中的元素进行排序，默认升序</li>
<li>该操作用于排序数据，在排序之前，可以将数据通过f函数进行处理，之后按照f函数处理的结果进行排序，默认为正排序。排序后新产生的RDD分区数与原RDD的分区数一致。</li>
</ul>
</li>
<li><p>pipe()</p>
<ul>
<li>对于RDD中的每一个分区，都会执行pipe 算子中指定的脚本</li>
</ul>
</li>
</ul>
<h4 id="双Value类型"><a href="#双Value类型" class="headerlink" title="双Value类型"></a>双Value类型</h4><ul>
<li>两个RDD之间进行操作：对源RDD和参数RDD进行操作，返回一个新的RDD</li>
<li>union()<ul>
<li>并集</li>
</ul>
</li>
<li>intersection<ul>
<li>交集</li>
</ul>
</li>
<li>subtract<ul>
<li>差集</li>
</ul>
</li>
<li>zip<ul>
<li>拉链，该操作可以将两个RDD中的元素，以键值对的形式进行合并。其中，键值对中的key为第一个RDD中的元素，value为第二个RDD中的元素。</li>
<li>注意必须要保证分区数以及每一个分区中元素的个数一致</li>
</ul>
</li>
</ul>
<h4 id="Key-Value类型"><a href="#Key-Value类型" class="headerlink" title="Key-Value类型"></a>Key-Value类型</h4><ul>
<li><p>PartitionBy</p>
<ul>
<li>按照指定的分区其，通过key对RDD中的元素进行分区</li>
<li>默认分区器  HashPartitioner</li>
</ul>
</li>
<li><p>reduceByKey</p>
<ul>
<li>将相同的key放在一起，对value进行聚合操作</li>
</ul>
</li>
<li><p>groupByKey</p>
<ul>
<li>按照key对RDD中的元素进行分组。对每个key进行操作，但只生成一个seq ,并不进行聚合。</li>
</ul>
</li>
<li><p>reduceByKey和groupByKey的区别</p>
<ul>
<li>reduceByKey：按照key进行聚合，在shuffle之前有combine（预聚合）操作，返回结果是RDD[k,v]</li>
<li>groupByKey: 按照key进行分组，直接进行shuffle</li>
<li>在不影响业务逻辑的前提下，优先选用reduceByKey。求和操作不影响业务逻辑，求平均值影响业务逻辑。</li>
</ul>
</li>
<li><p>aggregateByKey[(zeroValue)(分区内计算规则，分区间计算规则)]</p>
<ul>
<li>（1）zeroValue（初始值）：给每一个分区中的每一种key一个初始值；<br>（2）seqOp（分区内）：函数用于在每一个分区中用初始值逐步迭代value；<br>（3）combOp（分区间）：函数用于合并每个分区中的结果。</li>
</ul>
</li>
<li><p>foldByKey(zeroValue)（分区间计算规则）</p>
<ul>
<li>是aggregateBykey的简化，分区内和分区间计算规则相同</li>
</ul>
</li>
<li><p>combineByKey (对当前key的value进行转换,分区内计算规则,分区间计算规则)</p>
</li>
<li><p>几种聚合算子对比</p>
<ul>
<li><p>上面四个聚合算子底层都是调用</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala">&gt;reduceByKey(_+_)<br>	combineByKeyWithClassTag[<span class="hljs-type">V</span>]((v: <span class="hljs-type">V</span>) =&gt; v, func, func)<br>    <br>&gt;aggregateByKey(zeroValue)(cleanedSeqOp,combOp)<br>	combineByKeyWithClassTag[<span class="hljs-type">U</span>]((v: <span class="hljs-type">V</span>) =&gt; cleanedSeqOp(createZero(), v),cleanedSeqOp, combOp)<br>    <br>&gt;foldByKey<br>	combineByKeyWithClassTag[<span class="hljs-type">V</span>]((v: <span class="hljs-type">V</span>) =&gt; cleanedFunc(createZero(), v),cleanedFunc, cleanedFunc)<br>    <br>&gt;combineByKey<br>	combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners)	<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>sortByKey</p>
<ul>
<li>按照RDD的中的key对元素进行排序</li>
</ul>
</li>
<li><p>mapValues</p>
<ul>
<li>只对RDD中的value进行操作</li>
</ul>
</li>
<li><p>join&amp;cogroup</p>
<ul>
<li>连接操作</li>
</ul>
</li>
</ul>
<h3 id="行动算子（Action）"><a href="#行动算子（Action）" class="headerlink" title="行动算子（Action）"></a>行动算子（Action）</h3><ul>
<li><p>行动算子执行后，才会触发计算</p>
</li>
<li><p>reduce</p>
<ul>
<li>对RDD中的元素进行聚合</li>
</ul>
</li>
<li><p>collect.foreach和foreach</p>
<ul>
<li><p>collect.foreach：将每一个Excutor中的数据收集到Driver，形成一个新的数组<br>.foreach不是一个算子，是集合的方法，是对数组中的元素进行遍历</p>
</li>
<li><p>foreach：对RDD中的元素进行遍历</p>
</li>
</ul>
</li>
<li><p>count</p>
<ul>
<li>获取RDD中元素的个数</li>
</ul>
</li>
<li><p>countByKey</p>
<ul>
<li>获取RDD中每个key对应的元素个数</li>
</ul>
</li>
<li><p>first</p>
<ul>
<li>获取RDD中的第一个元素</li>
</ul>
</li>
<li><p>take</p>
<ul>
<li>获取RDD中的前几个元素</li>
</ul>
</li>
<li><p>takeOrdered</p>
<ul>
<li>获取排序后的RDD中的前几个元素</li>
</ul>
</li>
<li><p>aggregate&amp;fold</p>
<ul>
<li>aggregateByKey  处理kv类型的RDD，并且在进行分区间聚合的时候，初始值不参与运算</li>
<li>fold 是aggregate的简化版</li>
</ul>
</li>
<li><p>save相关的算子</p>
<ul>
<li>saveAsTextFile</li>
<li>saveAsObjectFile</li>
<li>saveAsSequenceFile(只针对KV类型RDD)</li>
</ul>
</li>
</ul>
<h3 id="RDD序列化"><a href="#RDD序列化" class="headerlink" title="RDD序列化"></a>RDD序列化</h3><ul>
<li>为什么要序列化：因为在spark程序中，算子相关的操作在Excutor上执行，算子之外的代码在Driver端执行，在执行有些算子时，需要使用Driver里面定义的数据，这就涉及到了跨进程或者跨节点之间的通讯，所以这就涉及到了跨进程或者跨节点之间的通讯。所以要求传递给Excutor中的数组所属的类型必须实现Serializable接口。</li>
<li>如何判断是否实现了序列化接口：在作业job提交之前，其中有一行代码 val cleanF &#x3D; sc.clean(f)，用于进行闭包检查之所以叫闭包检查，是因为在当前函数的内部访问了外部函数的变量，属于闭包的形式。如果算子的参数是函数的形式，都会存在这种情况。</li>
</ul>
<h3 id="RDD的血缘关系以及依赖关系"><a href="#RDD的血缘关系以及依赖关系" class="headerlink" title="RDD的血缘关系以及依赖关系"></a>RDD的血缘关系以及依赖关系</h3><ul>
<li>血缘关系<ul>
<li>toDebugString</li>
</ul>
</li>
<li>依赖关系<ul>
<li>dependencies</li>
<li>窄依赖：父RDD一个分区中的数据，还是交给子RDD的一个分区处理</li>
<li>宽依赖：父RDD一个分区中的数据，交给子RDD的多个分区处理。分区中数据打乱了，进行了shuffle操作</li>
</ul>
</li>
</ul>
<h3 id="Spark的Job调度（重点）"><a href="#Spark的Job调度（重点）" class="headerlink" title="Spark的Job调度（重点）"></a>Spark的Job调度（重点）</h3><ul>
<li>集群(Standalone|Yarn)<ul>
<li>一个Spark集群可以同时运行多个spark应用</li>
</ul>
</li>
<li>应用(Application)<ul>
<li>我们所编写完成某些功能的程序</li>
<li>一个应用可以并发的运行多个Job</li>
</ul>
</li>
<li>Job<ul>
<li><strong>Job对应着应用中的行动算子，每次执行一个行动算子，都会提交一个Job</strong></li>
<li>一个Job由多个stage组成</li>
</ul>
</li>
<li>Stage<ul>
<li><strong>一个宽依赖做一次阶段的划分</strong></li>
<li>阶段的个数&#x3D;宽依赖的个数+1</li>
<li>一个Stage由多个Task组成</li>
</ul>
</li>
<li>Task<ul>
<li><strong>每一个阶段最后一个RDD的分区数，就是当前阶段的Task个数</strong></li>
</ul>
</li>
</ul>
<h3 id="数据读取与保存"><a href="#数据读取与保存" class="headerlink" title="数据读取与保存"></a>数据读取与保存</h3><h4 id="RDD的持久化"><a href="#RDD的持久化" class="headerlink" title="RDD的持久化"></a>RDD的持久化</h4><ul>
<li>cache   底层调用persist，默认存储在内存中  相当于缓存，当有多个行动算子时，前面相同的操作部分可以进行缓存，减少重复计算。</li>
<li>persist  可以通过参数指定存储级别   内存，磁盘</li>
<li>checkpoint   检查点会切断血缘关系，会把中间结果记录下来，一般存储在高可用的存储系统中（如HDFS）<ul>
<li>作用：为了避免容错执行时间过长</li>
<li>一般和缓存搭配使用，因为在切断血缘关系后，为保证中间结果的正确性，会将前面的操作再运行一遍，加上缓存后，就不用再重复计算一遍。</li>
</ul>
</li>
</ul>
<h4 id="文件保存"><a href="#文件保存" class="headerlink" title="文件保存"></a>文件保存</h4><ul>
<li>textFile</li>
<li>sequenceFile   SequenceFile文件是Hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)</li>
<li>objectFile        对象文件是将对象序列化后保存的文件，采用Java的序列化机制。</li>
<li>Json   本质还是通过textFile读取文本，对读到的内容进行处理</li>
<li>HDFS</li>
<li>MySQL<ul>
<li>map———mapPartition</li>
<li>foreach——-foreachPartion</li>
</ul>
</li>
</ul>
<h3 id="累加器-广播变量"><a href="#累加器-广播变量" class="headerlink" title="累加器 广播变量"></a>累加器 广播变量</h3><ul>
<li>Spark三大结构<ul>
<li>RDD   弹性分布式数据集</li>
<li>累加器   分布式共享只写变量</li>
<li>广播变量  分布式共享只读变量</li>
</ul>
</li>
<li>累加器    Driver端的变量会复制到Excutor端的Task中去，但是Driver端不能读到Excutor中的变量。累加器可以实现将EXcutor端数据的改变传回到Driver端去<ul>
<li>自定义累加器：继承AccumulatorV2，设定输入、输出泛型     重写方法</li>
</ul>
</li>
<li>广播变量<ul>
<li>在多个并行操作中（Executor）使用同一个变量，Spark默认会为每个任务(Task)分别发送，这样如果共享比较大的对象，会占用很大工作节点的内存。</li>
<li>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Task操作使用。</li>
<li>实现Excutor端Task之间共享变量，节省内存</li>
</ul>
</li>
</ul>
<h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><h3 id="Spark-SQL概述"><a href="#Spark-SQL概述" class="headerlink" title="Spark SQL概述"></a>Spark SQL概述</h3><ul>
<li><p>Spark SQL是Spark用于结构化数据（structured data）处理Spark模块</p>
</li>
<li><p>与Hive类似，将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快</p>
</li>
<li><p>Spark Sql提供了2个编程抽象，类似Spark Core中的RDD</p>
<ul>
<li>DataFrame</li>
<li>DataSet</li>
</ul>
</li>
<li><p>DataFrame</p>
<ul>
<li>在Spark中，<font color=red>DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据中的二维表格。</font></li>
<li>DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。</li>
<li>这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。</li>
</ul>
</li>
<li><p>DataSet</p>
<ul>
<li>DataSet是分布式数据集合。</li>
<li>是DataFrame的一个扩展。它提供了RDD的优势（强类型，使用强大的lambda函数的能力）以及Spark SQL优化执行引擎的优点。DataSet也可以使用功能性的转换（操作map，flatMap，filter等等）。</li>
<li>DataFrame是DataSet的特例</li>
</ul>
</li>
</ul>
<h3 id="Hive-on-Spark-和Spark-on-Hive"><a href="#Hive-on-Spark-和Spark-on-Hive" class="headerlink" title="Hive on Spark 和Spark on Hive"></a>Hive on Spark 和Spark on Hive</h3><ul>
<li>hive on spark<ul>
<li>就是把hive查询从MR换成spark,参考官方的版本配置，不然很容易遇到依赖冲突。</li>
<li>之后还是在hive上写hivesql，但是执行引擎是spark，任务会转化成spark rdd算子去执行。</li>
<li>优化器还是hive的优化器，而没有spark sql自带的优化器的效果好</li>
</ul>
</li>
<li>spark on hive<ul>
<li>就是同步Sparksql,加载hive的配置文件，获取到hive的元数据信息</li>
<li>Spark sql获取到hive的元数据信息之后就可以拿到hive的所有表的数据</li>
<li>接下来就可以通过spark sql来操作hive表中的数据</li>
</ul>
</li>
<li>spark beeline<ul>
<li>Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容HiveServer2。因为 <strong>Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，</strong>因此我们部署好Spark Thrift Server后，可以直接使用hive的beeline访问Spark Thrift Server执行相关语句。Spark Thrift Server 的目的也只是取代 HiveServer2，因此它依旧可以和 Hive Metastore进行交互，获取到 hive 的元数据。</li>
<li>Spark Thrift Server和HiveServer2的区别<ul>
<li><a href="https://imgse.com/i/pAZdDJg"><img src="https://s21.ax1x.com/2024/09/05/pAZdDJg.png" alt="pAZdDJg.png"></a></li>
</ul>
</li>
<li>总结：Spark Thrift Server说白了就是小小的改动了下HiveServer2，代码量也不多。虽然接口和HiveServer2完全一致，但是它以<strong>单个Application</strong>在集群运行的方式还是比较奇葩的。可能官方也是为了实现简单而没有再去做更多的优化。</li>
</ul>
</li>
</ul>
<h3 id="Spark-SQL编程"><a href="#Spark-SQL编程" class="headerlink" title="Spark SQL编程"></a>Spark SQL编程</h3><ul>
<li>SparkSession是spark最新的SQL查询起始点</li>
</ul>
<h4 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h4><ul>
<li>创建DataFrame<ul>
<li>通过spark的数据源进行创建<ul>
<li>如果从内存中读取数据，spark可以知道是什么数据类型，如果是数字，默认做Int处理；但是从文件中读取的数字，不能确定是什么类型，所以用bigint接收，可以和Long类型转换，但是和Int不能进行转换。</li>
</ul>
</li>
<li>从一个存在的RDD进行转换</li>
<li>从hive Table进行查询返回</li>
</ul>
</li>
<li>SQL风格语法：是指我们查询数据的时候使用SQL语句来查询，这种风格的查询必须要有临时视图或者全局视图来辅助。</li>
<li>DSL风格语法：DataFrame提供一个特定领域语言（domain-specific language,DSL）去管理结构化的数据，可以在Scala，Java，Python和R中使用DSL，使用DSL语法风格不必再创建视图了。</li>
</ul>
<h4 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h4><ul>
<li>DataSet是具有强类型的数据集合，需要提供对应的类型信息。它是和类对应起来的。</li>
<li>创建方式<ul>
<li>使用样例类序列创建DataSet</li>
<li>使用基本类型的序列创建DataSet</li>
<li>RDD，DataFrame转换为DataSet</li>
</ul>
</li>
</ul>
<h4 id="RDD，DataFrame，DataSet三者之间的关系"><a href="#RDD，DataFrame，DataSet三者之间的关系" class="headerlink" title="RDD，DataFrame，DataSet三者之间的关系"></a>RDD，DataFrame，DataSet三者之间的关系</h4><ul>
<li>三者的共性<ul>
<li>都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利</li>
<li>三者都有惰性，再创建，转换，等操作时不会立即执行，只有再遇到Action操作时，才会开始运算。</li>
</ul>
</li>
<li>三者的区别<ul>
<li>RDD不支持SparkSQL操作</li>
<li>DataFrame每一行的类型固定为Row,每一列的值没法直接访问，只有通过解析才能获取各个字段的值。</li>
<li>DataFrame是DataSet的一个特例  type DataFrame&#x3D;Dataset[Row]</li>
<li>DataFrame也可以叫DataSet[Row],每一行的类型Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无法得知，只能用getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。而DataSet中，每一行是什么类型是不一定的，在自定义了case  class之后可以很方便的获取每一行的信息。</li>
</ul>
</li>
<li>三者之间的转换关系图<ul>
<li><a href="https://imgtu.com/i/4Dw8YQ"><img src="https://z3.ax1x.com/2021/09/24/4Dw8YQ.png" alt="4Dw8YQ.png"></a></li>
<li>速记<ul>
<li>转换成RDD都是rdd</li>
<li>转换成DataFrame都是toDF。RDD到DF是toDF(“列名”)</li>
<li>RDD转换成DataSet是：转换样例类.toDS;DataFrame转换成是:as[类型]</li>
</ul>
</li>
<li>如果需要RDD与DF或者DS之间操作，那么需要引入<font color=red>import spark.implicits._ (spark不是包名，而是sparkSession对象的名称，所以必须先创建SparkSession再导入对象，.implicits是一个内部的object)</font></li>
</ul>
</li>
</ul>
<h3 id="Spark-sql-任务优化"><a href="#Spark-sql-任务优化" class="headerlink" title="Spark sql 任务优化"></a>Spark sql 任务优化</h3><h4 id="spark-sql-任务中小文件的优化"><a href="#spark-sql-任务中小文件的优化" class="headerlink" title="spark sql 任务中小文件的优化"></a>spark sql 任务中小文件的优化</h4><ul>
<li><p>描述：在数栈开发过程中如果任务选择的是spark sql引擎，一定要注意减少小文件的输出。spark sql中  spark.spl.shuffle.partitions&#x3D;200 ,即会在shuffle结果中产生200个文件，设置的是RDD1做shuffle处理后生成的结果RDD2的分区数。这样产生的影响有：<strong>大部分时间都花费在调度</strong>，任务执行本身花费时间较小；最终产生的小文件数过多，会对后续任务的使用造成资源浪费，<strong>且严重影响namenode的性能</strong>。如果结果数据不多且预计增量不会太大（例如缓慢变化维表，码值表等），可以将此参数数值设低。</p>
</li>
<li><p>解决：针对数据量变化较大的任务，可以通过以下参数动态调节shuffle.partition 的个数产生及小文件个数的产生。</p>
</li>
<li><p>针对数据量变化较大的任务，可以通过以下参数动态调节shuffle.partition的个数产生及小文件个数的产生。spark sql 小文件合并参数。</p>
<ul>
<li>spark.sql.adaptive.enabled&#x3D;true</li>
<li>spark.sql.adaptive.shuffle.targetPostShuffleInputSize&#x3D;256000000  #hdfs中block大小的倍数。</li>
</ul>
</li>
<li><p>以上参数只对宽依赖产生作用（join,groupby ,distribute by,order by 等），如果没有用到宽依赖的关键字，比如只用了union all,可以在sql代码最后添加distribute by 关键字，如：distribute by 1</p>
</li>
<li><p>任务中是否产生了小文件信息可以通过以下方法查看：</p>
<ul>
<li><p>通过hdfs_namenode的web页面进行查看，默认端口为50070</p>
</li>
<li><p>登录namenode节点，通过类似命令查看：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">hdfs dfs -<span class="hljs-built_in">du</span> -h /dtInsight/hive/warehouse/<span class="hljs-variable">$&#123;项目名称.db&#125;</span>/<span class="hljs-variable">$&#123;表名&#125;</span>/pt=<span class="hljs-variable">$&#123;分区&#125;</span>/<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h4 id="用户自定函数"><a href="#用户自定函数" class="headerlink" title="用户自定函数"></a>用户自定函数</h4><ul>
<li>UDF（User-Defined-Function)   用户自定义函数  （一进一出）</li>
<li>UDAF（User-Defined Aggregation Function）用户自定义聚合函数（多进一出）如 count()，countDistinct()，avg()，max()，min()<ul>
<li>实现：通过继承UserDefinedAggregateFunction来实现用户自定义聚合函数</li>
</ul>
</li>
<li>UDTF（User-Defined Table-Generating Functions）用户自定义表生成函数 （一进多出） Spark中没有UDTF，spark中使用flatMap实现UDTF的功能</li>
</ul>
<h3 id="数据加载与保存"><a href="#数据加载与保存" class="headerlink" title="数据加载与保存"></a>数据加载与保存</h3><ul>
<li>spark.read.load 是加载数据的通用方法</li>
<li>df.write.save 是保存数据的通用方法</li>
</ul>
<h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><h3 id="Spark-Streaming概述"><a href="#Spark-Streaming概述" class="headerlink" title="Spark Streaming概述"></a>Spark Streaming概述</h3><ul>
<li>离线计算：就是在计算开始前就已知所有输入数据，输入数据不会产生变化，一般数据量大，计算时间也较长。最经典的就是Hadoop的MapReduce方式。</li>
<li>实时计算：输入数据是可以以序列化的方式一个个输入并进行处理的，也就是说在开始的时候不需要知道所有数据，与离线计算相比，运行时间短，计算量级相对较小。强调计算过程的时间要短，即所查当下给出结果。</li>
<li>数据处理的方式<ul>
<li>批：处理离线数据，冷数据。单个处理数据量大，处理速度比流慢。</li>
<li>流：在线，实时产生的数据。单次处理的数据量小，但处理速度更快。</li>
</ul>
</li>
<li>Spark Streaming 用于流式数据的处理。</li>
<li>Spark Streaming使用了一个高级抽象离散化流（discretized stream），叫做DStream。</li>
<li><font color=red>DStreams是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为RDD存在，而DStream是由这些RDD所组成的序列（因此得名“离散化”）</font></li>
<li>Spark Streaming 是一种“微量批处理”架构，和其他基于”一次处理一条记录”架构的系统相比，他的延迟会相对高一些。</li>
<li>背压机制：根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。</li>
</ul>
<h3 id="DStream"><a href="#DStream" class="headerlink" title="DStream"></a>DStream</h3><ul>
<li>DataFrame创建<ul>
<li>使用ssc.queueStream(queueOfRDDs)来创建DStream，每一个推送到这个队列中的RDD，都会作为一个DStream处理。</li>
<li>自定义数据源：继承Receiver，并实现onStart,onStop方法来自定义数据源采集。</li>
<li>Kafaka数据源<ul>
<li>kafka 0-10 Direct模式<ul>
<li>Executor读取数据并计算</li>
<li>增加Executor个数来增加消费的并行度</li>
<li>offset存储<ul>
<li>_consumer_offsets系统主题中，Kafka中的内部主题</li>
<li>手动维护（有事务的存储系统）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>DStream转换<ul>
<li>与RDD类似，分为Transformations（转换）和Output Operations(输出)两种</li>
</ul>
</li>
<li>Transformations（转换）<ul>
<li>无状态转换操作就是把简单的RDD转化操作应用到每个批次上，也就是转化DStream 中的每一个RDD。不会记录历史结果</li>
<li>有状态转换操作：<ul>
<li>UpdateStateByKey  算子用于将历史结果应用到当前批次，该操作允许在使用新信息不断更新的同时能保留它的状态。保留历史结果到下一次计算。</li>
<li>window 窗口操作。 窗口时长：计算内容的时间范围。滑动时长：间隔多久触发一次计算。这两者都必须为采集周期的整数倍。</li>
</ul>
</li>
</ul>
</li>
<li>DStream输出<ul>
<li>print() 在运行流程序的驱动结点上打印DStream中每一批次数据的最开始10个元素。这用于开发和调试。</li>
<li>foreachRDD(func) 即将函数func用产生于stream的每一个RDD，其中参数传入的函数func应该实现将每一个RDD中数据推送到外部系统。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>algorithm</title>
    <url>/2021/10/04/algorithm/</url>
    <content><![CDATA[<h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><ul>
<li>首先，动态规划问题一般形式就是求最值，求解动态规划的核心问题是穷举。</li>
<li>由于动态规划这类问题基本存在<strong>重叠子问题</strong>，如果暴力求解的话效率会很低，所以需要<strong>备忘录或者DP table</strong>来优化穷举过程。</li>
<li>动态规划问题一定会具备<strong>最优子结构</strong>，这样就能通过子问题求解原问题。要符合最优子结构，子问题之间必须相互独立。</li>
<li>正确的<strong>状态转移方程</strong>才能正确的求解。</li>
<li>求解动态规划的一般办法：<ul>
<li>确定基础案例，也就是最简单的子问题。</li>
<li>确定状态，也就是原问题和子问题中会变化的量</li>
<li>确定选择，也就是导致状态变化的行为</li>
<li>明确dp 数组的含义</li>
</ul>
</li>
</ul>
<h2 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h2><ul>
<li>常见排序算法的时间复杂度，稳定性<ul>
<li><a href="https://imgtu.com/i/5KSEnO"><img src="https://z3.ax1x.com/2021/10/13/5KSEnO.png" alt="5KSEnO.png"></a></li>
</ul>
</li>
</ul>
<h2 id="大数取余算法"><a href="#大数取余算法" class="headerlink" title="大数取余算法"></a>大数取余算法</h2><ul>
<li>场景：当一个数很大时，大到基本类型都放不下，这时我们需要用String来存储数字。那么怎么进行取余操作呢？</li>
<li>我们根据一个例子来看看：<ul>
<li><a href="https://imgtu.com/i/5KlnOS"><img src="https://z3.ax1x.com/2021/10/13/5KlnOS.png" alt="5KlnOS.png"></a></li>
<li>计算过程<ul>
<li>第一位数字4： 4%3&#x3D;1；</li>
<li>第二位数字4：（1*10+4）%3&#x3D;2</li>
<li>第三位数字3：（2*10+3）%3&#x3D;2</li>
</ul>
</li>
<li>最终的结果是最后一步计算得到的余数2.</li>
</ul>
</li>
</ul>
<h2 id="KMP"><a href="#KMP" class="headerlink" title="KMP"></a>KMP</h2><p>* </p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala</title>
    <url>/2021/08/10/Scala/</url>
    <content><![CDATA[<h1 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h1><h2 id="Scala简介"><a href="#Scala简介" class="headerlink" title="Scala简介"></a>Scala简介</h2><ul>
<li>Scala是一门多范式的编程语言，Scala支持面向对象和函数式编程。（多范式，就是多种编程方法的意思。有面向过程，面向对象，泛型，函数式四种程序设计方式。）</li>
</ul>
<h3 id="class和object说明"><a href="#class和object说明" class="headerlink" title="class和object说明"></a>class和object说明</h3><ul>
<li>object：从语法的角度上讲，上面的语法表示声明了一个伴生对象，但是还会生成一个伴生类。Scala是纯面向对象的，去除了java中的static关键字，通过伴生对象模拟static效果</li>
<li>伴生对象：伴随类产生的一个对象</li>
<li>对scala源文件进行编译后，默认会生成两个字节码文件，一个是伴生类另一个是伴生对象所属类（类名+一个$符号）</li>
<li>真正的伴生对象是伴生对象所属类中创建的单例对象</li>
<li>如果不想生成伴生类，可以手动生成，要求伴生类名称和伴生对象名称一致。</li>
<li>运行原理：<ul>
<li>java运行原理：先编译，再解释。.java源文件—&gt;编译器(javac)—&gt;.class字节码文件—&gt;JVM(java 不同平台)—&gt;机器指令</li>
<li>scala运行原理：先编译，再解释。.scala源文件—&gt;编译器(scalac)—&gt;.class字节码文件—&gt;JVM(scala 不同平台)—&gt;机器指令</li>
</ul>
</li>
</ul>
<h2 id="变量和数据类型"><a href="#变量和数据类型" class="headerlink" title="变量和数据类型"></a>变量和数据类型</h2><ul>
<li><p>注释和java规则一样。</p>
</li>
<li><p>变量 var 变量名[:变量类型]&#x3D;初始值</p>
</li>
<li><p>常量 val 变量名[:变量类型]&#x3D;初始值</p>
</li>
<li><p>声明变量时必须要有初始指</p>
</li>
<li><p>标识符业余java基本一致。特殊情况：（1）以字母或者下划线开头，后接字母、数字、下划线</p>
<p>（2）以操作符开头，且只包含操作符（+ - * &#x2F; # !等）</p>
<p>（3）用反引号<code>....</code>包括的任意字符串，即使是Scala关键字（39个）也可以</p>
</li>
<li><p>键盘输入输出：</p>
</li>
<li><p>输出：（1）字符串，通过+号连接</p>
<p>（2）printf用法：字符串，通过%传值。</p>
<p>（3）字符串模板（插值字符串）：通过”$”获取变量值。</p>
<p>${}</p>
</li>
<li><p>输入：StdIn.readLine()、StdIn.readShort()、StdIn.readDouble()</p>
</li>
</ul>
<h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><ul>
<li>Any：所有类的父类</li>
<li>AnyVal:(值类型)<ul>
<li>Byte,Short,Int,Long,Float,Double,Boolean,Char</li>
<li>Unit:表示返回值类型为空，相当于Java中的 void  关键字</li>
<li>StringOps:对自负床功能的 增强</li>
</ul>
</li>
<li>AnyRef（引用类型 ）<ul>
<li>所有java语言中的类</li>
<li>Scala  语言中的类</li>
<li>集合</li>
<li>Null：表示变量声明后，没有指向任何对象，相当于java中的null关键字</li>
</ul>
</li>
<li>Nothing :所有类的子类</li>
</ul>
<h3 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h3><ul>
<li>char在java中是没有符号位的。char占两个字节。</li>
</ul>
<h3 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h3><ul>
<li><p>面向对象编程：</p>
<ul>
<li>Scala是一个完全面向对象的编程语言，万物皆对象。</li>
<li>对象的本质：对数据和行为的一个封装。</li>
</ul>
</li>
<li><p>函数式编程：</p>
<ul>
<li>将问题分解成一个一个的步骤，将每一个步骤进行封装，通调用这些封装好的步骤。</li>
<li>Scala是一个完全函数式编程语言，万物皆函数。</li>
</ul>
</li>
<li><p>函数的本质：函数可以当做一个值进行传递。</p>
</li>
</ul>
<h4 id="函数基础"><a href="#函数基础" class="headerlink" title="函数基础"></a>函数基础</h4><ul>
<li><p>函数与方法的区别</p>
<ul>
<li>为完成某一功能的程序语句的集合，称为函数。</li>
<li>类中的函数称为方法</li>
<li>函数没有重载和重写的概念；方法可以进行重载和重写</li>
<li>Scala中函数可以嵌套定义</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">TestFunction</span> </span>&#123;<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>        <span class="hljs-comment">// （1）函数定义</span><br>        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f</span></span>(arg: <span class="hljs-type">String</span>): <span class="hljs-type">Unit</span> = &#123;<br>            println(arg)<br>        &#125;<br>        <span class="hljs-comment">// （2）函数调用</span><br>        <span class="hljs-comment">// 函数名（参数）</span><br>        f(<span class="hljs-string">&quot;hello world&quot;</span>)<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
</li>
<li><p>函数至简原则</p>
<ul>
<li><p>（1）return可以省略，Scala会使用函数体的最后一行代码作为返回值</p>
<p>（2）如果函数体只有一行代码，可以省略花括号</p>
<p>（3）返回值类型如果能够推断出来，那么可以省略（:和返回值类型一起省略）</p>
<p>（4）如果有return，则不能省略返回值类型，必须指定</p>
<p>（5）如果函数明确声明unit，那么即使函数体中使用return关键字也不起作用</p>
<p>（6）Scala如果期望是无返回值类型，可以省略等号</p>
<p>（7）如果函数无参，但是声明了参数列表，那么调用时，小括号，可加可不加</p>
<p>（8）如果函数没有参数列表，那么小括号可以省略，调用时小括号必须省略</p>
<p>（9）如果不关心名称，只关心逻辑处理，那么函数名（def）可以省略</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">package</span> com.zt.spark.day01<br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">FunctionTest</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-comment">//(0)函数标准写法</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f</span></span>(s:<span class="hljs-type">String</span>): <span class="hljs-type">String</span>=&#123;<br>      <span class="hljs-keyword">return</span> s+<span class="hljs-string">&quot;hello world&quot;</span><br>    &#125;<br>    println(f(<span class="hljs-string">&quot;hi,&quot;</span>))<br>    <span class="hljs-comment">//至简原则</span><br>    <span class="hljs-comment">//(1)return可以省略，Scala会使用函数体的最后一行代码作为返回值</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f1</span></span>(s:<span class="hljs-type">String</span>):<span class="hljs-type">String</span>=&#123;<br>      s+<span class="hljs-string">&quot;hello&quot;</span><br>    &#125;<br>    println(f1(<span class="hljs-string">&quot;world &quot;</span>))<br>    <span class="hljs-comment">//(2)如果函数只有一行代码，可以省略花括号</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f2</span></span>(s:<span class="hljs-type">String</span>):<span class="hljs-type">String</span>=s+<span class="hljs-string">&quot; hello&quot;</span><br>    println(f2(<span class="hljs-string">&quot;world&quot;</span>))<br>    <span class="hljs-comment">//(3)返回值类型如果能推断出来，那么可以省略（：和返回值类型一起省略）</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f3</span></span>(s:<span class="hljs-type">String</span>)=s+<span class="hljs-string">&quot; wocao&quot;</span><br>    println(f3(<span class="hljs-string">&quot;aaaa&quot;</span>))<br>    <span class="hljs-comment">//(4)如果return,则不能省略返回值类型，必须指定。</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f4</span></span>():<span class="hljs-type">String</span>=<span class="hljs-keyword">return</span> <span class="hljs-string">&quot;hello&quot;</span><br>    println(f4)<br>    <span class="hljs-comment">//(5)如果函数明确声明unit，那么即使函数体中使用return关键字也不起作用</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f5</span></span>():<span class="hljs-type">Unit</span>=<span class="hljs-keyword">return</span> <span class="hljs-string">&quot;xxxx&quot;</span><br>    println(f5)<br>    <span class="hljs-comment">//(6)Scala如果期望是无返回值类型，可以省略等号</span><br>    <span class="hljs-comment">//将无返回值的函数称为&quot;过程&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f6</span></span>() &#123;<br>      <span class="hljs-string">&quot;dadadada&quot;</span><br>    &#125;<br>    println(f6())<br>    <span class="hljs-comment">//(7)如果函数无参，但是声明了参数列表，那么调用时，小括号可加可不加</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f7</span></span>()=<span class="hljs-string">&quot;xxx&quot;</span><br>    println(f7())<br>    println(f7)<br>    <span class="hljs-comment">//(8)如果函数没有参数列表，那么小括号可以省略，调用时小括号必须省略</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f8</span> </span>=<span class="hljs-string">&quot;xxx&quot;</span><br>    println(f8)<br>    <span class="hljs-comment">//(9)如果不关心名称，只关心逻辑处理，那么函数名(def)可以省略</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f9=</span></span>(x:<span class="hljs-type">String</span>)=&gt;&#123;println(<span class="hljs-string">&quot;wode&quot;</span>)&#125;<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f10</span></span>(f:<span class="hljs-type">String</span>=&gt;<span class="hljs-type">Unit</span>): <span class="hljs-type">Unit</span> =&#123;<br>      f(<span class="hljs-string">&quot;&quot;</span>)<br>    &#125;<br>    f10(f9)<br>    println(f10((x:<span class="hljs-type">String</span>)=&gt;&#123;println(<span class="hljs-string">&quot;dadada&quot;</span>)&#125;))<br>  &#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure></li>
</ul>
<h4 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h4><ul>
<li><p>函数可以作为值进行传递</p>
<ul>
<li>var f &#x3D; 函数名 _</li>
<li>如果明确了变量的数据类型，那么下划线可以省略</li>
</ul>
</li>
<li><p>函数可以作为参数进行传递</p>
<ul>
<li>通过匿名函数</li>
<li>扩展函数的功能</li>
<li>提高函数的灵活度</li>
</ul>
</li>
<li><p>函数可以作为返回值进行传递</p>
<ul>
<li>函数的嵌套</li>
<li>函数链式调用，通过参数传递数据，在执行过程中，函数始终占据栈内存，容易导致内存溢出</li>
<li>闭包：内层函数访问外层函数的局部变量，会自动延长外层函数局部变量的生命周期，与内层函数形成一个闭合的效果，我们称之为闭包</li>
<li>柯里化：将一个参数列表中的多个参数，拆分为多个参数列表</li>
</ul>
</li>
<li><p>匿名函数：</p>
<ul>
<li><p>(x:Int)&#x3D;&gt;{函数体}</p>
</li>
<li><p>x：表示输入参数类型；Int：表示输入参数类型；函数体：表示具体代码逻辑</p>
</li>
<li><p>参数的类型可以省略，会根据形参进行自动的推导</p>
</li>
<li><p>类型省略之后，发现只有一个参数，则圆括号可以省略；其他情况：没有参数和参数超过1的永远不能省略圆括号。</p>
</li>
<li><p>匿名函数如果只有一行，则大括号也可以省略</p>
</li>
<li><p>如果参数只出现一次，则参数省略且后面参数可以用_代替</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br><br>        <span class="hljs-comment">// （1）定义一个函数：参数包含数据和逻辑函数</span><br>        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">operation</span></span>(arr: <span class="hljs-type">Array</span>[<span class="hljs-type">Int</span>], op: <span class="hljs-type">Int</span> =&gt; <span class="hljs-type">Int</span>) = &#123;<br>            <span class="hljs-keyword">for</span> (elem &lt;- arr) <span class="hljs-keyword">yield</span> op(elem)<br>        &#125;<br><br>        <span class="hljs-comment">// （2）定义逻辑函数</span><br>        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">op</span></span>(ele: <span class="hljs-type">Int</span>): <span class="hljs-type">Int</span> = &#123;<br>            ele + <span class="hljs-number">1</span><br>        &#125;<br><br>        <span class="hljs-comment">// （3）标准函数调用</span><br>        <span class="hljs-keyword">val</span> arr = operation(<span class="hljs-type">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>), op)<br>        println(arr.mkString(<span class="hljs-string">&quot;,&quot;</span>))<br><br>        <span class="hljs-comment">// （4）采用匿名函数</span><br>        <span class="hljs-keyword">val</span> arr1 = operation(<span class="hljs-type">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>), (ele: <span class="hljs-type">Int</span>) =&gt; &#123;<br>            ele + <span class="hljs-number">1</span><br>        &#125;)<br>        println(arr1.mkString(<span class="hljs-string">&quot;,&quot;</span>))<br><br>        <span class="hljs-comment">// （4.1）参数的类型可以省略，会根据形参进行自动的推导;</span><br>        <span class="hljs-keyword">val</span> arr2 = operation(<span class="hljs-type">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>), (ele) =&gt; &#123;<br>            ele + <span class="hljs-number">1</span><br>        &#125;)<br>        println(arr2.mkString(<span class="hljs-string">&quot;,&quot;</span>))<br><br>        <span class="hljs-comment">// （4.2）类型省略之后，发现只有一个参数，则圆括号可以省略；其他情况：没有参数和参数超过1的永远不能省略圆括号。</span><br>        <span class="hljs-keyword">val</span> arr3 = operation(<span class="hljs-type">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>), ele =&gt; &#123;<br>            ele + <span class="hljs-number">1</span><br>        &#125;)<br>        println(arr3.mkString(<span class="hljs-string">&quot;,&quot;</span>))<br><br>        <span class="hljs-comment">// (4.3) 匿名函数如果只有一行，则大括号也可以省略</span><br>        <span class="hljs-keyword">val</span> arr4 = operation(<span class="hljs-type">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>), ele =&gt; ele + <span class="hljs-number">1</span>)<br>        println(arr4.mkString(<span class="hljs-string">&quot;,&quot;</span>))<br><br>        <span class="hljs-comment">//（4.4）如果参数只出现一次，则参数省略且后面参数可以用_代替</span><br>        <span class="hljs-keyword">val</span> arr5 = operation(<span class="hljs-type">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>), _ + <span class="hljs-number">1</span>)<br>        println(arr5.mkString(<span class="hljs-string">&quot;,&quot;</span>))<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h2><h3 id="对象和类"><a href="#对象和类" class="headerlink" title="对象和类"></a>对象和类</h3><ul>
<li><p>在自然界中，只要是客观存在的都是对象（万物皆对象）</p>
</li>
<li><p>对大量对象共性的抽象，抽取为类</p>
<ul>
<li>有什么         属性</li>
<li>能做什么     方法</li>
</ul>
</li>
<li><p>在面向对象语言中，类是创建对象的模板</p>
</li>
<li><p>类是客观事务在人脑中的主观反映</p>
</li>
<li><p>Scala的属性：</p>
<ul>
<li>在Scala语言中，类，方法，属性，默认修饰符都是public，但是没有public关键字</li>
<li>对于Scala中的属性，底层会用private修饰，同时提供公开的设置以及获取属性的方法—-面向封装</li>
<li>如果要生成满足JavaBean规范的get和set方法的话，需要在属性上加@BeanProperty注解</li>
</ul>
</li>
<li><p>访问权限</p>
<ul>
<li><p>Java</p>
<figure class="highlight actionscript"><table><tr><td class="code"><pre><code class="hljs actionscript"><span class="hljs-keyword">private</span>		私有的，只能在本类中被访问<br><span class="hljs-keyword">default</span>		默认的，可以在本类以及同包的其它类中被访问<br><span class="hljs-keyword">protected</span>	受保护的，可以在本类、同包的其它类以及非同包的子类中被访问<br><span class="hljs-keyword">public</span> 		公开的，所有类<br></code></pre></td></tr></table></figure>
</li>
<li><p>Scala</p>
<figure class="highlight actionscript"><table><tr><td class="code"><pre><code class="hljs actionscript"><span class="hljs-keyword">private</span>		      私有的，只能在本类中被访问<br><span class="hljs-keyword">public</span>(默认)	     公开的，所有类<br><span class="hljs-keyword">protected</span>		  比Java设置的更严格，只能在本类以及子类中被访问，同包其他类访问不了<br><span class="hljs-keyword">private</span>[包名] 	 可以让指定的包进行访问<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="创建对象的方式"><a href="#创建对象的方式" class="headerlink" title="创建对象的方式"></a>创建对象的方式</h3><ul>
<li><p>构造器</p>
<ul>
<li><p>主构造方法</p>
<ul>
<li>在声明类的同时，主构造方法也被声明</li>
<li>主构造方法只能有一个</li>
<li>如果主构造方法没有参数，那么声明以及调用的时候，小括号可以省略。</li>
</ul>
</li>
<li><p>辅助构造方法</p>
<ul>
<li>方法名必修叫this</li>
<li>辅助构造方法可以重载</li>
<li>辅助构造方法中的第一行代码必须直接或者间接调用主构造方法</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-comment">//（1）如果主构造器无参数，小括号可省略</span><br><span class="hljs-comment">//class Person ()&#123;</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;<br><br>    <span class="hljs-keyword">var</span> name: <span class="hljs-type">String</span> = _<br><br>    <span class="hljs-keyword">var</span> age: <span class="hljs-type">Int</span> = _<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">this</span></span>(age: <span class="hljs-type">Int</span>) &#123;<br>        <span class="hljs-keyword">this</span>()<br>        <span class="hljs-keyword">this</span>.age = age<br>        println(<span class="hljs-string">&quot;辅助构造器&quot;</span>)<br>    &#125;<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">this</span></span>(age: <span class="hljs-type">Int</span>, name: <span class="hljs-type">String</span>) &#123;<br>        <span class="hljs-keyword">this</span>(age)<br>        <span class="hljs-keyword">this</span>.name = name<br>    &#125;<br><br>    println(<span class="hljs-string">&quot;主构造器&quot;</span>)<br>&#125;<br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">Person</span> </span>&#123;<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br><br>        <span class="hljs-keyword">val</span> person2 = <span class="hljs-keyword">new</span> <span class="hljs-type">Person</span>(<span class="hljs-number">18</span>)<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>继承</p>
<ul>
<li>class 子类名 extends 父类名  { 类体 }</li>
<li>Scala是单继承</li>
</ul>
</li>
<li><p>创建对象的方式</p>
<ul>
<li><p>new   底层调用的是构造方法</p>
</li>
<li><p>类名()  底层调用的是伴生对象中apply方法</p>
</li>
<li><p>实现单例设计模式</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">Test</span> </span>&#123;<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br><br>        <span class="hljs-comment">//（1）通过伴生对象的apply方法，实现不使用new关键字创建对象。</span><br>        <span class="hljs-keyword">val</span> p1 = <span class="hljs-type">Person</span>()<br>        println(<span class="hljs-string">&quot;p1.name=&quot;</span> + p1.name)<br><br>        <span class="hljs-keyword">val</span> p2 = <span class="hljs-type">Person</span>(<span class="hljs-string">&quot;bobo&quot;</span>)<br>        println(<span class="hljs-string">&quot;p2.name=&quot;</span> + p2.name)<br>    &#125;<br>&#125;<br><br><span class="hljs-comment">//（2）如果想让主构造器变成私有的，可以在()之前加上private</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> <span class="hljs-title">private</span>(<span class="hljs-params">cName: <span class="hljs-type">String</span></span>) </span>&#123;<br>    <span class="hljs-keyword">var</span> name: <span class="hljs-type">String</span> = cName<br>&#125;<br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">Person</span> </span>&#123;<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">apply</span></span>(): <span class="hljs-type">Person</span> = &#123;<br>        println(<span class="hljs-string">&quot;apply空参被调用&quot;</span>)<br>        <span class="hljs-keyword">new</span> <span class="hljs-type">Person</span>(<span class="hljs-string">&quot;xx&quot;</span>)<br>    &#125;<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">apply</span></span>(name: <span class="hljs-type">String</span>): <span class="hljs-type">Person</span> = &#123;<br>        println(<span class="hljs-string">&quot;apply有参被调用&quot;</span>)<br>        <span class="hljs-keyword">new</span> <span class="hljs-type">Person</span>(name)<br>&#125;<br><span class="hljs-comment">//注意：也可以创建其它类型对象，并不一定是伴生类对象</span><br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="特质和抽象类"><a href="#特质和抽象类" class="headerlink" title="特质和抽象类"></a>特质和抽象类</h3><ul>
<li><p>使用abstract关键字定义抽象类</p>
<ul>
<li>定义抽象类：abstract class Person{} &#x2F;&#x2F;通过abstract关键字标记抽象类</li>
</ul>
</li>
<li><p>抽象类一般和抽象属性以及抽象方法配合使用</p>
</li>
<li><p>抽象属性</p>
<ul>
<li>属性只有声明，但是没有赋值</li>
<li>定义抽象属性：val|var name:String &#x2F;&#x2F;一个属性没有初始化，就是抽象属性</li>
</ul>
</li>
<li><p>抽象方法</p>
<ul>
<li>方法只有声明，没有实现</li>
<li>定义抽象方法：def hello():String &#x2F;&#x2F;只声明而没有实现的方法，就是抽象方法</li>
</ul>
</li>
<li><p>抽象类总结</p>
<ul>
<li>在一个类中，如果存在抽象属性或者抽象方法，那么这个类一定是抽象类</li>
<li>如果一个类是抽象类，那么它不一定包含抽象属性和抽象方法</li>
<li>如果一个类中存在抽象属性或者抽象方法，那么具体的实现应该交给子类完成</li>
<li>如果子类也实现不了抽象内容，那么子类也应该声明为抽象类</li>
<li>如果重写(实现)抽象属性或者方法，那么override关键字可以省略</li>
<li>如果重写(覆盖)非抽象属性或者方法，那么override关键字不能省略，必须得加</li>
<li>如果对非抽象属性进行覆盖，要求属性必须得用val修饰</li>
<li>可以通过super关键字调用父类的方法，但是不能super调用父类的属性</li>
</ul>
</li>
<li><p>在Scala中，属性和方法都是动态绑定</p>
<ul>
<li>静态绑定（编译器绑定）<ul>
<li>在编译阶段，确定属性或者方法所属类型，多态的时候根据这个来看。（编译看左，运行看右）</li>
</ul>
</li>
<li>动态绑定<ul>
<li>在运行阶段，根据实际创建的对象类型来决定属性或者方法所属类型</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="特质（Trait）"><a href="#特质（Trait）" class="headerlink" title="特质（Trait）"></a>特质（Trait）</h4><ul>
<li><p>Scala中采用特质trait来代替接口的概念。</p>
</li>
<li><p>Scala中的trait中即<strong>可以有抽象属性和方法，也可以有具体的属性和方法</strong>，<strong>一个类可以混入（mixin）多个特质</strong>。这种感觉类似于Java中的抽象类。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">trait</span> <span class="hljs-title">PersonTrait</span> </span>&#123;<br><br>    <span class="hljs-comment">// 声明属性</span><br>    <span class="hljs-keyword">var</span> name:<span class="hljs-type">String</span> = _<br><br>    <span class="hljs-comment">// 声明方法</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">eat</span></span>():<span class="hljs-type">Unit</span>=&#123;<br><br>    &#125;<br><br>    <span class="hljs-comment">// 抽象属性</span><br>    <span class="hljs-keyword">var</span> age:<span class="hljs-type">Int</span><br>    <br>    <span class="hljs-comment">// 抽象方法</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">say</span></span>():<span class="hljs-type">Unit</span><br>&#125;<br><span class="hljs-comment">//通过查看字节码，可以看到特质=抽象类+接口</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>基本语法</p>
<ul>
<li><strong>没有父类</strong>：class  类名 <em><strong>extends</strong></em> 特质1  <em><strong>with</strong></em>  特质2  <em><strong>with</strong></em>  特质3 …</li>
<li><strong>有父类</strong>：class  类名  <em><strong>extends</strong></em> 父类  <em><strong>with</strong></em> 特质1  <em><strong>with</strong></em>  特质2  <em><strong>with</strong></em> 特质3…</li>
</ul>
</li>
<li><p>特质（trait）叠加：</p>
<ul>
<li><p>由于一个类可以混入（mixin）多个trait，且trait中可以有具体的属性和方法，若混入的特质中具有相同的方法（方法名，参数列表，返回值均相同），必然会出现继承冲突问题。冲突分为以下两种：</p>
<ul>
<li>第一种，一个类（Sub）混入的两个trait（TraitA，TraitB）中具有相同的具体方法，且两个trait之间没有任何关系，解决这类冲突问题，直接在类（Sub）中重写冲突方法。</li>
<li>第二种，一个类（Sub）混入的两个trait（TraitA，TraitB）中具有相同的具体方法，且两个trait继承自相同的trait（TraitC），及所谓的“钻石问题”，解决这类冲突问题，Scala采用了<em><strong>特质叠加</strong></em>的策略。</li>
</ul>
</li>
<li><p>特质叠加顺序</p>
<ul>
<li>第一步：列出第一个混入特质的继承关系，作为临时叠加的顺序</li>
<li>第二步：列出第二个混入特质的继承关系,并且该顺序放到临时叠加顺序的前面，已经出现的特质不在出现   </li>
<li>第三步：将子类放到临时叠加顺序的第一个</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">trait</span> <span class="hljs-title">Ball</span> </span>&#123;<br>   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">describe</span></span>(): <span class="hljs-type">String</span> = &#123;<br>      <span class="hljs-string">&quot;ball&quot;</span><br>   &#125;<br>&#125;<br><br><span class="hljs-class"><span class="hljs-keyword">trait</span> <span class="hljs-title">Color</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Ball</span> </span>&#123;<br>   <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">describe</span></span>(): <span class="hljs-type">String</span> = &#123;<br>      <span class="hljs-string">&quot;blue-&quot;</span> + <span class="hljs-keyword">super</span>.describe()<br>   &#125;<br>&#125;<br><br><span class="hljs-class"><span class="hljs-keyword">trait</span> <span class="hljs-title">Category</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Ball</span> </span>&#123;<br>   <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">describe</span></span>(): <span class="hljs-type">String</span> = &#123;<br>      <span class="hljs-string">&quot;foot-&quot;</span> + <span class="hljs-keyword">super</span>.describe()<br>   &#125;<br>&#125;<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyBall</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Category</span> <span class="hljs-keyword">with</span> <span class="hljs-title">Color</span> </span>&#123;<br>   <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">describe</span></span>(): <span class="hljs-type">String</span> = &#123;<br>      <span class="hljs-string">&quot;my ball is a &quot;</span> + <span class="hljs-keyword">super</span>.describe()<br>   &#125;<br>&#125;<br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">TestTrait</span> </span>&#123;<br>   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>      println(<span class="hljs-keyword">new</span> <span class="hljs-type">MyBall</span>().describe())<br>   &#125;<br>&#125;<br><span class="hljs-comment">//注意：这个时候super不是调用父类中的方法了，而是调用特质叠加顺序上下一个节点的方法</span><br><span class="hljs-comment">//运行结果：my ball is a blue-foot-ball</span><br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>特质和抽象类的区别</p>
<ul>
<li>优先使用特质。一个类扩展多个特质是很方便的，但却只能扩展一个抽象类。</li>
<li>如果你需要构造函数参数，使用抽象类。因为抽象类可以定义带参数的构造函数，而特质不行（有无参构造）。</li>
</ul>
</li>
<li><p>类型检查和转换</p>
<ul>
<li>obj.isInstanceOf[T]：判断obj是不是T类型。</li>
<li>obj.asInstanceOf[T]：将obj强转成T类型。</li>
<li>classOf获取对象的类名。</li>
</ul>
</li>
</ul>
<h2 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h2><h3 id="集合基本概述"><a href="#集合基本概述" class="headerlink" title="集合基本概述"></a>集合基本概述</h3><ul>
<li><p>存放单值类型</p>
<ul>
<li>Seq   有序，可重复</li>
<li>Set    无序，不能重复</li>
</ul>
</li>
<li><p>存放键值对</p>
<ul>
<li>Map   以k-v键值对的形式存放数据，其中key无序不能重复</li>
</ul>
</li>
<li><p>对于几乎所有的集合类，Scala都同时提供了可变和不可变的版本，分别位于以下两个包</p>
<ul>
<li>不可变集合：scala.collection.immutable<ul>
<li>对集合进行添加或者删除操作的时候，会创建新的集合对象</li>
</ul>
</li>
<li>可变集合：scala.collection.mutable<ul>
<li>对集合进行添加或者删除操作的时候，直接在原来的集合上操作，不会创建新的集合对象</li>
</ul>
</li>
</ul>
</li>
<li><p>数组</p>
<ul>
<li><p>Array</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">TestArray</span></span>&#123;<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br><br>        <span class="hljs-comment">//（1）数组定义</span><br>        <span class="hljs-keyword">val</span> arr01 = <span class="hljs-keyword">new</span> <span class="hljs-type">Array</span>[<span class="hljs-type">Int</span>](<span class="hljs-number">4</span>)<br>        println(arr01.length) <span class="hljs-comment">// 4</span><br>        <br>        <span class="hljs-keyword">var</span> arr02 = <span class="hljs-type">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-string">&quot;bobo&quot;</span>)<br>        println(arr02.length)<br>        <span class="hljs-keyword">for</span> (i &lt;- arr02) &#123;<br>            println(i)<br>        &#125;<br>        <span class="hljs-comment">//（2）数组赋值</span><br><br><br>        <span class="hljs-comment">//（2.1）修改某个元素的值</span><br>        arr01(<span class="hljs-number">3</span>) = <span class="hljs-number">10</span><br>        <span class="hljs-comment">//（2.2）采用方法的形式给数组赋值</span><br>        arr01.update(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment">//（3）遍历数组</span><br>        <span class="hljs-comment">//（3.1）查看数组</span><br>        println(arr01.mkString(<span class="hljs-string">&quot;,&quot;</span>))<br><br>        <span class="hljs-comment">//（3.2）普通遍历</span><br>        <span class="hljs-keyword">for</span> (i &lt;- arr01) &#123;<br>            println(i)<br>        &#125;<br><br>        <span class="hljs-comment">//（3.3）简化遍历</span><br>        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">printx</span></span>(elem:<span class="hljs-type">Int</span>): <span class="hljs-type">Unit</span> = &#123;<br>            println(elem)<br>        &#125;<br>         arr01.foreach(printx)<br>        <span class="hljs-comment">// arr01.foreach((x)=&gt;&#123;println(x)&#125;)</span><br>        <span class="hljs-comment">// arr01.foreach(println(_))</span><br>        arr01.foreach(println)<br><br>        <span class="hljs-comment">//（4）增加元素（由于创建的是不可变数组，增加元素，其实是产生新的数组）</span><br>        println(arr01)<br>        <span class="hljs-keyword">val</span> ints: <span class="hljs-type">Array</span>[<span class="hljs-type">Int</span>] = arr01 :+ <span class="hljs-number">5</span><br>        println(ints)<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>


</li>
<li><p>ArrayBuffer</p>
</li>
<li><p>多维数组</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">DimArray</span> </span>&#123;<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>        <br>        <span class="hljs-comment">//（1）创建了一个二维数组, 有三个元素，每个元素是，含有4个元素一维数组()</span><br>        <span class="hljs-keyword">val</span> arr = <span class="hljs-type">Array</span>.ofDim[<span class="hljs-type">Int</span>](<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br>        arr(<span class="hljs-number">1</span>)(<span class="hljs-number">2</span>) = <span class="hljs-number">88</span><br><br>        <span class="hljs-comment">//（2）遍历二维数组</span><br>        <span class="hljs-keyword">for</span> (i &lt;- arr) &#123; <span class="hljs-comment">//i 就是一维数组</span><br><br>            <span class="hljs-keyword">for</span> (j &lt;- i) &#123;<br>                print(j + <span class="hljs-string">&quot; &quot;</span>)<br>            &#125;<br><br>            println()<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Seq</p>
<ul>
<li><p>List</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">TestList</span> </span>&#123;<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br><br>        <span class="hljs-comment">//（1）List默认为不可变集合</span><br>        <span class="hljs-comment">//（2）创建一个List（数据有顺序，可重复）</span><br>        <span class="hljs-keyword">val</span> list: <span class="hljs-type">List</span>[<span class="hljs-type">Int</span>] = <span class="hljs-type">List</span>(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">3</span>)<br>        <br>        <span class="hljs-comment">//（7）空集合Nil</span><br>        <span class="hljs-keyword">val</span> list5 = <span class="hljs-number">1</span>::<span class="hljs-number">2</span>::<span class="hljs-number">3</span>::<span class="hljs-number">4</span>::<span class="hljs-type">Nil</span><br><br>        <span class="hljs-comment">//（4）List增加数据</span><br>        <span class="hljs-comment">//（4.1）::的运算规则从右向左</span><br>        <span class="hljs-comment">//val list1 = 5::list</span><br>        <span class="hljs-keyword">val</span> list1 = <span class="hljs-number">7</span>::<span class="hljs-number">6</span>::<span class="hljs-number">5</span>::list<br>        <span class="hljs-comment">//（4.2）添加到第一个元素位置</span><br>        <span class="hljs-keyword">val</span> list2 = list.+:(<span class="hljs-number">5</span>)<br><br>        <span class="hljs-comment">//（5）集合间合并：将一个整体拆成一个一个的个体，称为扁平化</span><br>        <span class="hljs-keyword">val</span> list3 = <span class="hljs-type">List</span>(<span class="hljs-number">8</span>,<span class="hljs-number">9</span>)<br>        <span class="hljs-comment">//val list4 = list3::list1</span><br>        <span class="hljs-keyword">val</span> list4 = list3:::list1<br><br>        <span class="hljs-comment">//（6）取指定数据</span><br>        println(list(<span class="hljs-number">0</span>))<br><br>        <span class="hljs-comment">//（3）遍历List</span><br>        <span class="hljs-comment">//list.foreach(println)</span><br>        <span class="hljs-comment">//list1.foreach(println)</span><br>        <span class="hljs-comment">//list3.foreach(println)</span><br>        <span class="hljs-comment">//list4.foreach(println)</span><br>        list5.foreach(println)<br>    &#125;<br></code></pre></td></tr></table></figure>


</li>
<li><p>ListBuffer</p>
</li>
</ul>
</li>
<li><p>Set</p>
</li>
<li><p>Map</p>
<ul>
<li><p>当调用map.get方法的时候，返回的Option类型数据,Option有两个子类型，一个Some，另一个None,可以帮我们避免对空值进行处理的情况，使用getOrElse函数，给空值赋默认值</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">TestMap</span> </span>&#123;<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>        <span class="hljs-comment">// Map</span><br>        <span class="hljs-comment">//（1）创建不可变集合Map</span><br>        <span class="hljs-keyword">val</span> map = <span class="hljs-type">Map</span>( <span class="hljs-string">&quot;a&quot;</span>-&gt;<span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>-&gt;<span class="hljs-number">2</span>, <span class="hljs-string">&quot;c&quot;</span>-&gt;<span class="hljs-number">3</span> )<br><br>        <span class="hljs-comment">//（3）访问数据</span><br>        <span class="hljs-keyword">for</span> (elem &lt;- map.keys) &#123;<br>            <span class="hljs-comment">// 使用get访问map集合的数据，会返回特殊类型Option(选项):有值（Some），无值(None)</span><br>            println(elem + <span class="hljs-string">&quot;=&quot;</span> + map.get(elem).get)<br>        &#125;<br><br>        <span class="hljs-comment">//（4）如果key不存在，返回0</span><br>        println(map.get(<span class="hljs-string">&quot;d&quot;</span>).getOrElse(<span class="hljs-number">0</span>))<br>        println(map.getOrElse(<span class="hljs-string">&quot;d&quot;</span>, <span class="hljs-number">0</span>))<br><br>        <span class="hljs-comment">//（2）循环打印</span><br>        map.foreach((kv)=&gt;&#123;println(kv)&#125;)<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="集合函数"><a href="#集合函数" class="headerlink" title="集合函数"></a>集合函数</h3><ul>
<li><p>基本属性和常用操作</p>
<p>（1）获取集合长度          length</p>
<p>（2）获取集合大小          size</p>
<p>（3）循环遍历                  foreach</p>
<p>（4）迭代器                      iterator</p>
<p>（5）生成字符串              mkString</p>
<p>（6）是否包含                  contains</p>
</li>
<li><p>衍生集合</p>
<figure class="highlight xquery"><table><tr><td class="code"><pre><code class="hljs xquery">（<span class="hljs-number">1</span>）获取集合的头<br><span class="hljs-built_in">	head</span><br>（<span class="hljs-number">2</span>）获取集合的尾（不是头的就是尾）<br><span class="hljs-built_in">	tail</span><br>（<span class="hljs-number">3</span>）集合最后一个数据<br><span class="hljs-built_in">	last</span><br>（<span class="hljs-number">4</span>）集合初始数据（不包含最后一个）<br>	init<br>（<span class="hljs-number">5</span>）反转<br><span class="hljs-built_in">	reverse</span><br>（<span class="hljs-number">6</span>）取前（后）n个元素<br>	take|takeRight<br>（<span class="hljs-number">7</span>）去掉前（后）n个元素<br>	drop|dropRight<br>（<span class="hljs-number">8</span>）并集<br>	<span class="hljs-keyword">union</span><br>（<span class="hljs-number">9</span>）交集<br>	<span class="hljs-keyword">intersect</span><br>（<span class="hljs-number">10</span>）差集<br>	diff<br>（<span class="hljs-number">11</span>）拉链<br>	zip<br>（<span class="hljs-number">12</span>）滑窗<br>	<span class="hljs-keyword">sliding</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>集合计算初级，高级函数</p>
<figure class="highlight coq"><table><tr><td class="code"><pre><code class="hljs coq">		（<span class="hljs-number">1</span>）求和<br>			<span class="hljs-built_in">sum</span><br>		（<span class="hljs-number">2</span>）求乘积<br>			product<br>		（<span class="hljs-number">3</span>）最大值<br>			max<br>		（<span class="hljs-number">4</span>）最小值<br>			min<br>		（<span class="hljs-number">5</span>）排序<br>			sorded|<span class="hljs-type">sortBy</span>|<span class="hljs-type">sortWith</span><br>			<br>集合计算高级函数：<br>		（<span class="hljs-number">1</span>）过滤 filter(函数：指定过滤条件) <br>			 遍历一个集合并从中获取满足指定条件的元素组成一个新的集合<br><br>		（<span class="hljs-number">2</span>）转换/映射  map<br>				 	<br>		（<span class="hljs-number">3</span>）扁平化 flatten   :::<br>			将集合中元素由整体转换为个体的过程<br><br>		（<span class="hljs-number">4</span>）扁平映射  flatMap  <br>			先映射再进行扁平化处理<br><br>		（<span class="hljs-number">5</span>）分组	gruopBy<br>			按照一定的分组规则，将集合中的元素放到不同的组中<br><br>		（<span class="hljs-number">6</span>）简化|<span class="hljs-type">规约</span><br><span class="hljs-type">			&gt;对集合内部元素之间进行聚合 </span><br><span class="hljs-type">			&gt;reduce</span>   聚合的数据类型一致<br>			&gt;reduceLeft|<span class="hljs-type">reduceRight</span> 	聚合的数据类型可以不一致<br><br>		（<span class="hljs-number">7</span>）折叠<br>			&gt;对外部元素和集合内部元素之间进行聚合<br>			&gt;<span class="hljs-built_in">fold</span> 	聚合的数据类型一致<br>			&gt;foldLeft|<span class="hljs-type">foldRight</span>		聚合的数据类型可以不一致<br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="下划线的用法"><a href="#下划线的用法" class="headerlink" title="下划线的用法"></a>下划线的用法</h2><ul>
<li>标识符命名</li>
<li>导包<ul>
<li>导入某一个类下的”静态成员” ：import scala.util.control.Breaks._</li>
<li>导入某一包下的所有类：import java.util._</li>
<li>屏蔽类：import java.sql.{Date&#x3D;&gt;<em>,Array&#x3D;&gt;</em>,_}</li>
</ul>
</li>
<li>匿名函数<ul>
<li>如果在匿名函数中，参数只出现了一次，那么参数可以省略，在函数体使用参数的时候，用下划线代替</li>
</ul>
</li>
<li>在类中声明属性，如果要给属性赋默认值</li>
<li>在模式匹配中<ul>
<li>case_  表示上面所有case都没有匹配成功的情况，相当于default</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>StarRocks</title>
    <url>/2024/01/03/StarRocks/</url>
    <content><![CDATA[<h1 id="StarRocks"><a href="#StarRocks" class="headerlink" title="StarRocks"></a>StarRocks</h1><h2 id="基础介绍"><a href="#基础介绍" class="headerlink" title="基础介绍"></a>基础介绍</h2><ul>
<li>社区还是很活跃的，贴一下官方文档：<a href="https://docs.starrocks.io/zh/docs/introduction/StarRocks_intro/">StarRocks官方文档</a></li>
<li>官方描述：StarRocks 是一款高性能分析型数据库，使用向量化、MPP 架构、CBO、智能物化视图、可实时更新的列式存储引擎等技术实现多维、实时、高并发的数据分析。StarRocks 既支持从各类实时和离线的数据源高效导入数据，也支持直接分析数据湖上各种格式的数据。StarRocks 兼容 MySQL 协议，可使用 MySQL 客户端和常用 BI 工具对接。同时 StarRocks 具备水平扩展，高可用、高可靠、易运维等特性。广泛应用于实时数仓、OLAP 报表、数据湖分析等场景。</li>
<li>StarRocks 是新一代极速全场景 MPP (Massively Parallel Processing) 数据库。StarRocks 的愿景是能够让用户的数据分析变得更加简单和敏捷。</li>
<li>StarRocks 架构简洁，采用了全面向量化引擎，并配备全新设计的 CBO (Cost Based Optimizer) 优化器，查询速度（尤其是多表关联查询）远超同类产品。</li>
</ul>
<h3 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h3><ul>
<li>StarRocks 可以满足企业级用户的多种分析需求，包括 OLAP (Online Analytical Processing) 多维分析、定制报表、实时数据分析和 Ad-hoc 数据分析等。</li>
<li>OLAP多维分析：利用 StarRocks 的 MPP 框架和向量化执行引擎，用户可以灵活的选择雪花模型，星型模型，宽表模型或者预聚合模型。适用于灵活配置的多维分析报表，业务场景包括：<ul>
<li>用户行为分析</li>
<li>用户画像、标签分析、圈人</li>
<li>自助式报表平台</li>
<li>系统监控分析</li>
</ul>
</li>
<li>实时数据仓库：StarRocks 设计和实现了 Primary-Key 模型，能够实时更新数据并极速查询，可以秒级同步 TP (Transaction Processing) 数据库的变化，构建实时数仓，业务场景包括：<ul>
<li>电商大促数据分析</li>
<li>物流行业的运单分析</li>
<li>金融行业绩效分析、指标计算</li>
</ul>
</li>
<li>高并发查询：StarRocks 通过良好的数据分布特性，灵活的索引以及物化视图等特性，可以解决面向用户侧的分析场景，业务场景包括<ul>
<li>广告主报表分析</li>
<li>SaaS 行业面向用户分析报表</li>
</ul>
</li>
<li>统一分析：<ul>
<li>通过使用一套系统解决多维分析、高并发查询、预计算、实时分析查询等场景，降低系统复杂度和多技术栈开发与维护成本。</li>
<li><strong>使用 StarRocks 统一管理数据湖和数据仓库，将高并发和实时性要求很高的业务放在 StarRocks 中分析，也可以使用 External Catalog 和外部表进行数据湖上的分析。</strong></li>
</ul>
</li>
</ul>
<h3 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h3><ul>
<li>架构简洁，核心只有FE(Frontend),BE(Backend)或CN(Compute Node)两类进程，方便部署维护，可水平扩展，有副本机制，确保系统无单点。SR支持Mysql协议接口，支持SQL标准语法。</li>
<li><a href="https://imgse.com/i/pFJGuQg"><img src="https://s11.ax1x.com/2024/02/18/pFJGuQg.png" alt="pFJGuQg.png"></a></li>
<li><strong>FE 节点负责元数据管理、客户端连接管理、查询计划和查询调度。每个 FE 在其内存中存储和维护完整的元数据副本，确保每个 FE 都能提供无差别的服务。</strong></li>
<li>CN 节点在存算分离或存算一体集群中负责执行查询。</li>
<li>BE 节点在存算一体集群中负责数据存储和执行查询。<ul>
<li><strong>执行sql时，一条sql 语句首先会按照语义规划成逻辑单元，然后再按照数据的分布情况拆分成具体的物理执行单元。物理执行单元会在对应的BE节点上执行，实现本地计算，避免数据的传输与拷贝，加速查询</strong></li>
</ul>
</li>
<li>从存算一体(shared-nothing)进化到存算分离(shared-data)<ul>
<li>3.0 版本之前使用存算一体架构，BE 同时负责数据存储和计算，数据访问和分析都在本地进行，提供极速的查询分析体验。</li>
<li>3.0 版本开始引入存算分离架构，数据存储功能从原来的 BE 中抽离，BE 节点升级为无状态的 CN 节点。数据可持久存储在远端对象存储或 HDFS 上，CN 本地磁盘只用于缓存热数据来加速查询。存算分离架构下支持动态增删计算节点，实现秒级的扩缩容能力。</li>
<li>存算分离支持的存储系统：兼容 AWS S3 协议的对象存储系统；Azure Blob Storage；传统数据中心部署的 HDFS</li>
</ul>
</li>
</ul>
<h4 id="数据管理"><a href="#数据管理" class="headerlink" title="数据管理"></a>数据管理</h4><ul>
<li><p>SR采用列式存储，采用分区分桶机制进行数据管理。<strong>一张表可以被划分成多个分区，一个分区内的数据可以根据一列或者多列进行分桶，将数据切分成多个Tablet。</strong>tablet是SR中最小的数据管理单元，每个tablet都会以多副本的形式存储在不同的BE节点中。用户可以指定Tablet 的个数和大小，SR会管理好每个Tablet 副本的分布信息。（<strong>Tablet实际就是数据分桶</strong>）</p>
</li>
<li><p>下面是StarRocks 的数据划分以及 Tablet 多副本机制。表按照日期划分为 4 个分区，第一个分区进一步切分成 4 个 Tablet。每个 Tablet 使用 3 副本进行备份，分布在 3 个不同的 BE 节点上。</p>
<ul>
<li><a href="https://imgse.com/i/pk4ps8x"><img src="https://s21.ax1x.com/2024/07/12/pk4ps8x.png" alt="pk4ps8x.png"></a></li>
</ul>
</li>
<li></li>
<li><p>解释一下catlog的作用，以Flink Catalog为例：</p>
<ul>
<li><p>Catalog 提供了一个统一的 API 来管理元数据，并使其可以从表 API 和 SQL 查询语句中来访问。</p>
</li>
<li><p>Catalog 使用户能够引用他们数据系统中的现有元数据，并自动将它们映射到 Flink 的相应元数据。</p>
</li>
<li><p>例如，Flink 可以将 JDBC 表自动映射到 Flink 表，用户不必在 Flink 中手动重写 DDL。Catalog 大大简化了用户现有系统开始使用 Flink 所需的步骤，并增强了用户体验。</p>
</li>
</ul>
</li>
<li><p>schema的作用：schema是对一个数据库的结构描述。在一个关系型数据库里面，schema定义了表、每个表的字段，还有表和字段之间的关系。</p>
</li>
<li><p>缓存：为了提升存算分离架构的查询性能，SR构建了分级的数据缓存体系，将最热的数据缓存在内存中，距离计算最近；次热数据则缓存在本地磁盘，冷数据位于对象存储，数据根据访问频率在三级存储中自由流动。</p>
<ul>
<li><p>具体细节：</p>
</li>
<li><p>StarRocks 存算分离的统一缓存允许用户在建表时决定是否开启缓存。如果开启，数据写入时会同步写入本地磁盘以及后端对象存储，查询时，CN 节点会优先从本地磁盘读取数据，如果未命中，再从后端对象存储读取原始数据并同时缓存在本地磁盘。</p>
<p>同时，针对未被缓存的冷数据，StarRocks 也进行了针对性优化，可根据应用访问模式，利用数据预读技术、并行扫描技术等手段，减少对于后端对象存储的访问频次，提升查询性能。</p>
</li>
</ul>
</li>
</ul>
<h3 id="产品特性"><a href="#产品特性" class="headerlink" title="产品特性"></a>产品特性</h3><ul>
<li>MPP分布式执行框架<ul>
<li>StarRocks 采用 MPP (Massively Parallel Processing) 分布式执行框架。在MPP执行框架中，<strong>一条查询请求会被拆分成多个物理计算单元，在多机并行执行</strong>。每个执行节点拥有独享的资源（CPU、内存）。</li>
<li>不同逻辑执行单元可以由不同数目的物理执行单元来具体执行，以提高资源使用率，提升查询速度。</li>
<li><strong>在 MPP 框架中，数据会被 Shuffle 到多个节点，并且由多个节点来完成最后的汇总计算。</strong></li>
</ul>
</li>
<li>全面向量化执行引擎<ul>
<li>StarRocks 通过实现全面向量化引擎，充分发挥了 CPU 的处理能力。全面向量化引擎按照列式的方式组织和处理数据。<strong>StarRocks 的数据存储、内存中数据的组织方式，以及 SQL 算子的计算方式，都是列式实现的</strong>。按列的数据组织也会更加充分的利用 CPU 的 Cache，按列计算会有更少的虚函数调用以及更少的分支判断从而获得更加充分的 CPU 指令流水。</li>
</ul>
</li>
<li>存储计算分离<ul>
<li>存算分离，存储与计算解耦，独立扩缩容</li>
</ul>
</li>
<li>CBO优化器<ul>
<li>厉害的查询优化器，选择出相对最优的查询计划</li>
</ul>
</li>
<li>可实时更新的列式存储引擎<ul>
<li><strong>实现列式存储，数据以列的方式存储，相同类型的数据连续存放</strong></li>
<li>提高压缩比，降低I&#x2F;O式的总量，提升查询性能</li>
<li>StarRocks 能够支持秒级的导入延迟，提供准实时的服务能力。StarRocks 的存储引擎在数据导入时能够保证每一次操作的 ACID。一个批次的导入数据生效是原子性的，要么全部导入成功，要么全部失败</li>
</ul>
</li>
<li>智能物化视图<ul>
<li><strong>SR物化视图可以根据原始表更新数据，只要原始表发生变化，物化视图会同步更新，不需要额外的维护操作就可以保证物化视图能够维持与原表一致</strong></li>
<li>物化视图的选择也是自动进行的。StarRocks 在进行查询规划时，如果有合适的物化视图能够加速查询，StarRocks 自动进行查询改写(query rewrite)，将查询自动定位到最适合的物化视图上进行查询加速。</li>
</ul>
</li>
<li>数据湖分析<ul>
<li>StarRocks 不仅能高效的分析本地存储的数据，也<strong>可以作为计算引擎直接分析数据湖中的数据。</strong></li>
<li>利用SR的提供的 External Catalog，轻松查询存储在Apache Hive、Apache Iceberg、Apache Hudi、Delta Lake 等数据湖上的数据</li>
<li><strong>在数据湖分析场景中，StarRocks 主要负责数据的计算分析</strong>，而数据湖则主要负责数据的存储、组织和维护。使用数据湖的优势在于可以使用开放的存储格式和灵活多变的 schema 定义方式，可以让 BI&#x2F;AI&#x2F;Adhoc&#x2F;报表等业务有统一的 single source of truth。</li>
</ul>
</li>
</ul>
<h2 id="表设计"><a href="#表设计" class="headerlink" title="表设计"></a>表设计</h2><ul>
<li><p>SR使用Internal Catlog来管理内部数据，使用 External Catalog 来连接数据湖中的数据。存储在SR中的数据都包含在Internal Catlog下，Internal Catalog 可以包含一个或多个数据库。数据库用于存储、管理和操作 StarRocks 中的数据，可用于管理多种对象，包括表、物化视图、视图等。StarRocks 采用权限系统来管理数据访问权限，定义了用户对哪些对象可以执行哪些操作，提高数据安全性。</p>
</li>
<li><p>Catalog 分为 Internal catalog 和 External catalog。<strong>Internal catalog 是内部数据目录，用于管理导入至 StarRocks 中的数据以及内部的物化视图等。</strong>每个集群都有且只有一个名为 default_catalog 的 Internal catalog，包含一个或多个数据库。StarRocks 作为数据仓库存储数据，能够显著提高查询性能，尤其应对大规模数据的复杂查询分析。</p>
<p>External catalog 是外部数据目录，用于连接数据湖中的数据。您可以将 StarRocks 作为查询引擎，直接查询湖上数据，无需导入数据至 StarRocks。</p>
</li>
<li><p>StarRocks 中的表分为两类：内部表和外部表。</p>
</li>
<li><p>内部表归属于 Internal catalog 的数据库，数据保存在 StarRocks 中。内部表由行和列构成，每一行数据是一条记录。</p>
<ul>
<li>在 StarRocks 中，根据约束的类型将内部表分四种，分别是主键表、明细表、聚合表和更新表，适用于存储和查询多种业务场景中的数据，比如原始日志、实时数据、以及汇总数据。</li>
<li><strong>内部表采用分区+分桶的两级数据分布策略，实现数据均匀分布。并且分桶以多副本形式均匀分布至 BE 节点，保证数据高可用。</strong></li>
</ul>
</li>
<li><p>外部表是 External catalog 中的表，实际数据存在外部数据源中，StarRocks 只保存表对应的元数据，您可以通过外部表查询外部数据。</p>
</li>
</ul>
<h4 id="物化视图"><a href="#物化视图" class="headerlink" title="物化视图"></a>物化视图</h4><ul>
<li><strong>物化视图是特殊的物理表，能够存储基于基表的预计算结果。</strong>当您对基表执行复杂查询时，StarRocks可以自动复用物化视图中的预计算结果，实现查询透明加速，湖仓加速和数据建模等业务需求。</li>
<li>视图（也叫逻辑视图）是虚拟表，不存储数据，其中所展示的数据来自于基表生成的查询结果。每次在查询中引用某个视图时，都会运行定义该视图的查询。</li>
<li>SR有权限系统，权限决定了哪些用户可以对哪些特定对象执行哪些特定的操作。SR采用两种权限模型：<strong>基于用户标识的访问控制和基于角色的访问控制</strong>。您可以将权限赋予给角色然后通过角色传递权限给用户，或直接赋予权限给用户标识。</li>
<li>存算分离架构，数据存储功能从原来的BE中抽离，数据可持久存储在更为可靠廉价的远端对象存储（如S3（AWS S3 全名是 Simple Storage Service，简便的存储服务。亚马逊的云存储业务））或HDFS上，本地磁盘只用于缓存热数据来加速查询</li>
</ul>
<h4 id="表概览"><a href="#表概览" class="headerlink" title="表概览"></a>表概览</h4><ul>
<li>表是数据存储单元。理解 StarRocks 中的表结构，以及如何设计合理的表结构，有利于优化数据组织，提高查询效率。相比于传统的数据库，StarRocks 会以列的方式存储 JSON、ARRAY 等复杂的半结构化数据，保证高效查询。</li>
</ul>
<h4 id="表类型"><a href="#表类型" class="headerlink" title="表类型"></a>表类型</h4><ul>
<li>SR提供四种类型的表，包括明细表，主键表，聚合表和更新表，使用于存储多种业务数据。<ul>
<li>明细表简单易用，表中数据不具有任何约束，相同的数据行可以重复存在。<strong>适用于存储不需要约束和预聚合的原始数据。</strong></li>
<li>主键表能力强大，具有唯一性非空约束。<strong>该表能够支撑实时更新，部分列更新等场景的同时，保证查询性能，使用于实时查询。</strong></li>
<li><strong>聚合表适用于存储预聚合后的数据，可以降低聚合查询时所需扫描和计算的数据量，极大的提高聚合查询的效率。</strong></li>
<li>更新表适用于实时更新的业务场景，目前已逐渐被主键表取代。</li>
</ul>
</li>
</ul>
<h4 id="数据分布"><a href="#数据分布" class="headerlink" title="数据分布"></a>数据分布</h4><ul>
<li>SR采用分区+分桶的两级数据分布策略，将数据均匀分布各个BE节点。查询时能够有效裁剪数据扫描量，最大限度的利用集群的并发性能。</li>
</ul>
<h5 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h5><ul>
<li><strong>第一层级为分区，表中数据可以根据分区列（通常是时间和日期）分成一个个更小的数据管理单元。</strong>查询时，通过分区裁剪，可以减少扫描的数据量，显著优化查询性能。</li>
<li>SR提供简单易用的<strong>表达式分区</strong>。还提供更加灵活的分区方式，<strong>Range分区和List分区</strong>。</li>
</ul>
<h5 id="分桶"><a href="#分桶" class="headerlink" title="分桶"></a>分桶</h5><ul>
<li><strong>第二层级为分桶，同一个分区中的数据通过分桶，划分成更小的数据管理单元。</strong>并且分桶以多副本的形式均匀分布在BE节点上，保证数据的高可用。</li>
<li>SR有两种分桶方式：<ul>
<li>哈希分桶：根据数据的分桶键值，将数据划分至分桶。选择查询时经常使用的条件组成分桶键，能够有效的提高查询效率。</li>
<li>随机分桶：随机划分数据至分桶，这种方式更加简单易用。</li>
</ul>
</li>
</ul>
<h5 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h5><ul>
<li>支持多种数据类型，和hive差不多</li>
</ul>
<h5 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h5><ul>
<li>索引是一种特殊的数据结构，相当于数据的目录。</li>
<li>SR提供内置索引，包括前缀索引，Ordinal索引和ZoneMap索引。也支持手动创建索引，提高查询效率，包括Bitmap和Bloom Filter索引。</li>
</ul>
<h5 id="约束"><a href="#约束" class="headerlink" title="约束"></a>约束</h5><ul>
<li>约束用于确保数据的完整性、一致性和准确性。主键表的 Primary Key 列具有唯一非空约束，聚合表的 Aggregate Key 列和更新表的 Unique Key 列具有唯一约束。</li>
</ul>
<h3 id="表类型-1"><a href="#表类型-1" class="headerlink" title="表类型"></a>表类型</h3><h4 id="基础信息"><a href="#基础信息" class="headerlink" title="基础信息"></a>基础信息</h4><ul>
<li>四种表类型：分别是明细表 (Duplicate key table)、聚合表 (Aggregate table)、更新表 (Unique Key table) 和主键表 ( Primary Key table)。</li>
</ul>
<h4 id="排序键"><a href="#排序键" class="headerlink" title="排序键"></a>排序键</h4><ul>
<li>数据导入至使用某个类型的表，会按照建表时指定的一列或多列排序后存储，这部分用于排序的列就称为排序键。排序键通常为查询时过滤条件频繁使用的一个或者多个列，用以加速查询。 明细表中，数据按照排序键 <code>DUPLICATE KEY</code> 排序，并且排序键不需要满足唯一性约束。 聚合表中，数据按照排序键 <code>AGGREGATE KEY</code> 聚合后排序，并且排序键需要满足唯一性约束。 更新表中，数据按照排序键 <code>UNIQUE KEY</code> REPLACE 后排序，并且排序键需要满足唯一性约束。 主键表支持分别定义主键和排序键，主键 <code>PRIMARY KEY</code> 需要满足唯一性和非空约束，主键相同的数据进行 REPLACE。排序键是用于排序，由 <code>ORDER BY</code> 指定 。</li>
<li>在建表语句中，排序键必须定义在其他列之前。</li>
<li>在创建表时，您可以将一个或多个列定义为排序键。排序键在建表语句中的出现次序，为数据存储时多重排序的次序。</li>
<li>不支持排序键的数据类型为 BITMAP、HLL。</li>
<li>前缀索引的长度限制为 36 字节。如果排序键中全部列的值的长度加起来超过 36 字节，则前缀索引仅会保存限制范围内排序键的若干前缀列。</li>
<li>如果导入的数据存在重复的主键，则数据导入至不同类型的表时，存储在 StarRocks 时，则会按照如下方式进行处理：<ul>
<li>明细表：表中会存在主键重复的数据行，并且与导入的数据是完全对应的。您可以召回所导入的全部历史数据。</li>
<li>聚合表：表中不存在主键重复的数据行，主键满足唯一性约束。导入的数据中主键重复的数据行聚合为一行，即具有相同主键的指标列，会通过聚合函数进行聚合。您只能召回导入的全部历史数据的聚合结果，但是无法召回历史明细数据。</li>
<li>主键表和更新表：表中不存在主键重复的数据行，主键满足唯一性约束。最新导入的数据行，替换掉其他主键重复的数据行。这两种类型的表可以视为聚合表的特殊情况，相当于在聚合表中，为表的指标列指定聚合函数为 REPLACE，REPLACE 函数返回主键相同的一组数据中的最新数据。</li>
</ul>
</li>
</ul>
<h4 id="明细表"><a href="#明细表" class="headerlink" title="明细表"></a>明细表</h4><ul>
<li><p>明细表是默认创建的表类型。如果在建表时未指定任何key，默认创建的是明细表</p>
</li>
<li><p>创建表时，支持定义排序键。如果查询的过滤条件包含排序键，则 StarRocks 能够快速地过滤数据，提高查询效率。<strong>明细表适用于日志数据分析等场景，支持追加新数据，不支持修改历史数据。</strong></p>
</li>
<li><p>建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">CREATE TABLE IF NOT EXISTS detail (<br>    event_time DATETIME NOT NULL COMMENT &quot;datetime of event&quot;,<br>    event_type INT NOT NULL COMMENT &quot;type of event&quot;,<br>    user_id INT COMMENT &quot;id of user&quot;,<br>    device_code INT COMMENT &quot;device code&quot;,<br>    channel INT COMMENT &quot;&quot;<br>)<br>DUPLICATE KEY(event_time, event_type)<br>DISTRIBUTED BY HASH(user_id)<br>PROPERTIES (<br>&quot;replication_num&quot; = &quot;3&quot;<br>);<br></code></pre></td></tr></table></figure>

<ul>
<li><p>排序键的相关说明：</p>
<ul>
<li><p>在建表语句中，排序键必须定义在其他列之前。</p>
</li>
<li><p>排序键可以通过 <code>DUPLICATE KEY</code> 显式定义。本示例中排序键为 <code>event_time</code> 和 <code>event_type</code>。</p>
<blockquote>
<p>如果未指定，则默认选择表的前三列作为排序键。</p>
</blockquote>
</li>
<li><p>明细表中的排序键可以为部分或全部维度列。</p>
</li>
</ul>
</li>
<li><p>建表时，支持为指标列创建 BITMAP、Bloom Filter 等索引。</p>
</li>
</ul>
</li>
</ul>
<h4 id="聚合表"><a href="#聚合表" class="headerlink" title="聚合表"></a>聚合表</h4><ul>
<li><p><strong>建表时，支持定义排序键和指标列，并为指标列指定聚合函数。当多条数据具有相同的排序键时，指标列会进行聚合。</strong>在分析统计和汇总数据时，聚合表能够减少查询时所需要处理的数据，提升查询效率。</p>
</li>
<li><p>适用于分析统计和汇总数据</p>
</li>
<li><p>原理：<strong>从数据导入至数据查询阶段，聚合表内部同一排序键的数据会多次聚合</strong>，步骤：</p>
<ol>
<li>数据导入阶段：数据按批次导入至聚合表时，每一个批次的数据形成一个版本。在一个版本中，同一排序键的数据会进行一次聚合</li>
<li>后台文件合并阶段（Compaction）：数据分批次多次导入至聚合表中，会生成多个版本的文件，多个版本的文件定期合并成一个大版本文件时，同一排序键的数据会进行一次聚合。</li>
<li>查询阶段：所有版本中同一排序键的数据进行聚合，然后返回查询结果</li>
</ol>
</li>
<li><p>聚合表中数据进行多次聚合，能够减少查询时所需要处理的数据量，进而提升查询的效率</p>
</li>
<li><p>创建表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">/*<br>例如需要分析某一段时间内，来自不同城市的用户，访问不同网页的总次数。则可以将网页地址 site_id、日期 date 和城市代码 city_code 作为排序键，将访问次数 pv 作为指标列，并为指标列 pv 指定聚合函数为 SUM。<br>*/<br>CREATE TABLE IF NOT EXISTS example_db.aggregate_tbl (<br>    site_id LARGEINT NOT NULL COMMENT &quot;id of site&quot;,<br>    date DATE NOT NULL COMMENT &quot;time of event&quot;,<br>    city_code VARCHAR(20) COMMENT &quot;city_code of user&quot;,<br>    pv BIGINT SUM DEFAULT &quot;0&quot; COMMENT &quot;total page views&quot;<br>)<br>AGGREGATE KEY(site_id, date, city_code)<br>DISTRIBUTED BY HASH(site_id)<br>PROPERTIES (<br>&quot;replication_num&quot; = &quot;3&quot;<br>);<br></code></pre></td></tr></table></figure>
</li>
<li><p>建表时必须使用 <code>DISTRIBUTED BY HASH</code> 子句指定分桶键。分桶键的更多说明，请参见<a href="https://docs.starrocks.io/zh/docs/table_design/Data_distribution/#%E5%88%86%E6%A1%B6">分桶</a>。</p>
</li>
<li><p>自 2.5.7 版本起，StarRocks 支持在建表和新增分区时自动设置分桶数量 (BUCKETS)，您无需手动设置分桶数量。更多信息，请参见 <a href="https://docs.starrocks.io/zh/docs/table_design/Data_distribution/#%E8%AE%BE%E7%BD%AE%E5%88%86%E6%A1%B6%E6%95%B0%E9%87%8F">设置分桶数量</a>。</p>
</li>
<li><p>排序键的相关说明：</p>
<ul>
<li><p>在建表语句中，<strong>排序键必须定义在其他列之前</strong>。</p>
</li>
<li><p>排序键可以通过 <code>AGGREGATE KEY</code> 显式定义。</p>
<blockquote>
<ul>
<li>如果 <code>AGGREGATE KEY</code> 未包含全部维度列（除指标列之外的列），则建表会失败。</li>
<li>如果不通过 <code>AGGREGATE KEY</code> 显示定义排序键，则默认除指标列之外的列均为排序键。</li>
</ul>
</blockquote>
</li>
<li><p>排序键必须满足唯一性约束，必须包含全部维度列，并且列的值不会更新。</p>
</li>
</ul>
</li>
<li><p>指标列：通过在列名后指定聚合函数，定义该列为指标列。一般为需要汇总统计的数据。</p>
</li>
<li><p>聚合函数：指标列使用的聚合函数。聚合表支持的聚合函数，请参见 <a href="https://docs.starrocks.io/zh/docs/sql-reference/sql-statements/data-definition/CREATE_TABLE/">CREATE TABLE</a>。</p>
</li>
<li><p>查询时，排序键在多版聚合之前就能进行过滤，而指标列的过滤在多版本聚合之后。因此建议将频繁使用的过滤字段作为排序键，在聚合前就能过滤数据，从而提升查询性能。</p>
</li>
<li><p>建表时，不支持为指标列创建 BITMAP、Bloom Filter 等索引。</p>
</li>
</ul>
<h4 id="更新表"><a href="#更新表" class="headerlink" title="更新表"></a>更新表</h4><ul>
<li><p>建表时，支持定义主键和指标列，查询时返回主键相同的一组数据中的最新数据。相对于明细表，更新表简化了数据导入流程，能够更好地支撑实时和频繁更新的场景。</p>
</li>
<li><p>建表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">CREATE TABLE IF NOT EXISTS orders (<br>    create_time DATE NOT NULL COMMENT &quot;create time of an order&quot;,<br>    order_id BIGINT NOT NULL COMMENT &quot;id of an order&quot;,<br>    order_state INT COMMENT &quot;state of an order&quot;,<br>    total_price BIGINT COMMENT &quot;price of an order&quot;<br>)<br>UNIQUE KEY(create_time, order_id)<br>DISTRIBUTED BY HASH(order_id)<br>PROPERTIES (<br>&quot;replication_num&quot; = &quot;3&quot;<br>); <br></code></pre></td></tr></table></figure></li>
</ul>
<h4 id="主键表"><a href="#主键表" class="headerlink" title="主键表"></a>主键表</h4><ul>
<li><p>主键表支持分别定义主键和排序键。数据导入至主键表时先按照排序键排序后存储。查询时返回主键相同的一组数据中的最新数据。相对于更新表，主键表在查询时不需要执行聚合操作，并且支持谓词和索引下推，能够在支持<strong>实时和频繁更新</strong>等场景的同时，提供高效查询。</p>
</li>
<li><p>适用场景：</p>
<ul>
<li><strong>实时对接事务型数据至StarRocks。</strong><a href="https://docs.starrocks.io/zh/docs/loading/Flink_cdc_load/">通过 Flink-CDC 等工具直接对接 TP 的 Binlog</a>，实时同步增删改的数据至主键表，可以简化数据同步流程，并且相对于 Merge-On-Read 策略的更新表，查询性能能够提升 3~10 倍。</li>
<li><strong>利用部分列更新轻松实现多流JOIN</strong>。在用户画像等分析场景中，一般会采用大宽表方式来提升多维分析的性能，同时简化数据分析师的使用模型。而这种场景中的上游数据，往往可能来自于多个不同业务（比如来自购物消费业务、快递业务、银行业务等）或系统（比如计算用户不同标签属性的机器学习系统），主键表的部分列更新功能就很好地满足这种需求，不同业务直接各自按需更新与业务相关的列即可，并且继续享受主键表的实时同步增删改数据及高效的查询性能。</li>
</ul>
</li>
<li><p>注意事项：</p>
<ul>
<li><p><strong>单条主键编码后的最大长度为128字节</strong>，开启持久化索引，可以大大降低主键对内存的占用。因为少部分主键索引存在内存中，大部分主键索引存在磁盘中。</p>
</li>
<li><p>应用场景：</p>
<ul>
<li><p><strong>数据有冷热特征</strong>，即最近几天的热数据才经常被修改，老的冷数据很少被修改。</p>
</li>
<li><p><strong>大宽表</strong>（数百到数千列）。主键只占整个数据的很小一部分，其内存开销比较低。比如用户状态和画像表，虽然列非常多，但总的用户数不大（千万至亿级别），主键索引内存占用相对可控。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>案例</p>
<ul>
<li><pre><code class="sqlite">create table users (
    user_id bigint NOT NULL,
    name string NOT NULL,
    email string NULL,
    address string NULL,
    age tinyint NULL,
    sex tinyint NULL,
    last_active datetime,
    property0 tinyint NOT NULL,
    property1 tinyint NOT NULL,
    property2 tinyint NOT NULL,
    property3 tinyint NOT NULL
) PRIMARY KEY (user_id)
DISTRIBUTED BY HASH(user_id)
ORDER BY(`address`,`last_active`)
PROPERTIES (
    &quot;replication_num&quot; = &quot;3&quot;,
    &quot;enable_persistent_index&quot; = &quot;true&quot;
);
-- 建表时必须使用ISTRIBUTED BY HASH 子句指定分桶键，否则建表失败
-- 自 2.5.7 版本起，StarRocks 支持在建表和新增分区时自动设置分桶数量 (BUCKETS)，您无需手动设置分桶数量
<figure class="highlight markdown"><table><tr><td class="code"><pre><code class="hljs markdown"><br><span class="hljs-bullet">  -</span> 主键表相关说明<br><br><span class="hljs-bullet">    -</span> 在建表语句中，主键必须定义在其他列之前。<br><span class="hljs-bullet">    -</span> 主键通过 <span class="hljs-code">`PRIMARY KEY`</span> 定义。<br><span class="hljs-bullet">    -</span> 主键必须满足唯一性约束，且列的值不会修改。本示例中主键为 <span class="hljs-code">`dt`</span>、<span class="hljs-code">`order_id`</span>。<br><span class="hljs-bullet">    -</span> 主键支持以下数据类型：BOOLEAN、TINYINT、SMALLINT、INT、BIGINT、LARGEINT、DATE、DATETIME、VARCHAR/STRING。并且不允许为 NULL。<br><span class="hljs-bullet">    -</span> 分区列和分桶列必须在主键中。<br><br><span class="hljs-bullet">  -</span> <span class="hljs-code">`enable_persistent_index`</span>：是否持久化主键索引，同时使用磁盘和内存存储主键索引，避免主键索引占用过大内存空间。<br><br><span class="hljs-bullet">    -</span> 可以在建表时，在<span class="hljs-code">`PROPERTIES`</span>中配置该参数，取值范围为 <span class="hljs-code">`true`</span> 或者 <span class="hljs-code">`false`</span>（默认值）。<br><span class="hljs-bullet">    -</span> 如果磁盘为固态硬盘 SSD，则建议设置为 <span class="hljs-code">`true`</span>。如果磁盘为机械硬盘 HDD，并且导入频率不高，则也可以设置为 <span class="hljs-code">`true`</span>。<br><br><span class="hljs-bullet">  -</span> 如果不开启持久化，可能会导致占用内存较多，建议合理设置主键，防治内存溢出，建议先进行内存占用的计算：(12 + 9(每行固定开销) ) * 1000W * 3 * 1.5（哈希表平均额外开销) = 945 M<br><br><span class="hljs-section">### 数据分布</span><br><br><span class="hljs-bullet">*</span> 合理设置分区和分桶，实现数据均匀分布和查询性能提升。<br><br><span class="hljs-section">#### 常见的数据分布方式</span><br><br><span class="hljs-bullet">*</span> 现代分布式数据库中，常见的数据分布方式有：Round-Robin,Range,LIst和Hash<br><span class="hljs-bullet">  -</span> [<span class="hljs-string">![数据分布方式</span>](<span class="hljs-link">https://s21.ax1x.com/2024/03/18/pFR8OhR.png</span>)](<span class="hljs-link">https://imgse.com/i/pFR8OhR</span>)<br><span class="hljs-bullet">  -</span> Round-Robin：以轮询的方式把数据逐个放置在相邻节点上<br><span class="hljs-bullet">  -</span> Range：按区间进行数据分布，如图所示，区间[1-3],[4-6]分别对应不同的范围(Range)<br><span class="hljs-bullet">  -</span> List：直接基于离散的各个取值做数据分布。比如性别，省份等数据就满足这种离散的特性<br><span class="hljs-bullet">  -</span> Hash：通过哈希函数把数据映射到不同节点上<br><span class="hljs-bullet">*</span> 还可以根据业务场景需求组合使用这些数据分方式。常见的组合方式有 Hash+Hash、Range+Hash、Hash+List。<br><br><span class="hljs-section">#### SR的数据分布方式</span><br><br><span class="hljs-bullet">*</span> SR支持单独和组合使用数据分布方式<br><span class="hljs-bullet">*</span> SR通过分区+分桶的方式来实现数据分布<br><span class="hljs-bullet">  -</span> 第一层为分区：在一张表中，可以进行分区，支持的分区方式有表达式分区，Range分区和List分区，或者不分区<br><span class="hljs-bullet">  -</span> 第二层为分桶：在一个分区中，必须进行分桶，支持的分桶方式有哈希分桶和随机分桶<br><br><span class="hljs-section">#### 创建分区</span><br><br><span class="hljs-bullet">*</span> 表达式分区<br><span class="hljs-bullet">  -</span> 您仅需要在建表时使用分区表达式（时间函数表达式或列表达式），即可实现导入数据时自动创建分区，不需要预先创建出分区或者配置动态分区属性。<br><span class="hljs-bullet">*</span> Range分区<br><span class="hljs-bullet">  -</span> Range 分区适用于简单且具有连续性的数据<br><span class="hljs-bullet">*</span> 动态分区<br><span class="hljs-bullet">  -</span> 区别于表达式分区中自动创建分区功能，动态创建分区只是根据您配置的动态分区属性，定期提前创建一些分区。<br><br><span class="hljs-section">##### 批量创建分区</span><br><br><span class="hljs-bullet">*</span> 建表时和建表后，支持批量创建分区，通过 START、END 指定批量分区的开始和结束，EVERY 子句指定分区增量值。其中，批量分区包含 START 的值，但是不包含 END 的值。分区的命名规则同动态分区一样。<br><br><span class="hljs-section">#### 管理分区</span><br><br><span class="hljs-bullet">*</span> 增加分区<br><br><span class="hljs-bullet">  -</span> 对于 Range 分区和 List 分区，您可以手动增加新的分区，用于存储新的数据，而表达式分区可以实现导入新数据时自动创建分区，您无需手动新增分区。 新增分区的默认分桶数量和原分区相同。您也可以根据新分区的数据规模调整分桶数量。<br><br><span class="hljs-bullet">  -</span> <span class="hljs-code">````sql</span><br><span class="hljs-code">    ALTER TABLE site_access</span><br><span class="hljs-code">    ADD PARTITION p4 VALUES LESS THAN (&quot;2020-04-30&quot;)</span><br><span class="hljs-code">    DISTRIBUTED BY HASH(site_id);</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li></li>
</ul>
</li>
<li><p>删除分区</p>
<ul>
<li>分区中的数据不会立即删除，会在Trash中保留一段时间(默认为一天)</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> site_access<br><span class="hljs-keyword">DROP</span> <span class="hljs-keyword">PARTITION</span> p1;<br></code></pre></td></tr></table></figure>
</li>
<li><p>恢复分区</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql">RECOVER <span class="hljs-keyword">PARTITION</span> p1 <span class="hljs-keyword">FROM</span> site_access;<br></code></pre></td></tr></table></figure>
</li>
<li><p>查看分区</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SHOW</span> PARTITIONS <span class="hljs-keyword">FROM</span> site_access;<br></code></pre></td></tr></table></figure></li>
</ul>
<h4 id="设置分桶"><a href="#设置分桶" class="headerlink" title="设置分桶"></a>设置分桶</h4><h5 id="随机分桶"><a href="#随机分桶" class="headerlink" title="随机分桶"></a>随机分桶</h5><ul>
<li>对每个分区的数据，StarRocks 将数据随机地分布在所有分桶中，适用于数据量不大，对查询性能要求不高的场景。如果您不设置分桶方式，则默认由 StarRocks 使用随机分桶，并且自动设置分桶数量。</li>
</ul>
<h5 id="哈希分桶"><a href="#哈希分桶" class="headerlink" title="哈希分桶"></a>哈希分桶</h5><ul>
<li>对每个分区的数据，StarRocks 会根据<strong>分桶键</strong>和<strong>分桶数量</strong>进行哈希分桶。在哈希分桶中，使用特定的列值作为输入，通过哈希函数计算出一个哈希值，然后将数据根据该哈希值分配到相应的桶中。</li>
<li>优点<ul>
<li>提高查询性能。<strong>相同分桶键值的行会被分配到一个分桶中，在查询时能减少扫描数据量。</strong></li>
<li>均匀分布数据。<strong>通过选取较高基数（唯一值的数量较多）的列作为分桶键，能更均匀的分布数据到每一个分桶中。</strong></li>
</ul>
</li>
<li>选择分桶键<ul>
<li>假设存在列同时满足高基数和经常作为查询条件，则建议您选择其为分桶键，进行哈希分桶。若不存在这样的列，根据查询进行判断<ul>
<li><strong>如果查询复杂，建议选择高基数的列为分桶键，保证数据在各个分桶中尽量均衡，提高资源利用率。</strong></li>
<li><strong>如果查询比较简单，则建议选择经常作为查询条件的列为分桶键，提高查询效率</strong></li>
</ul>
</li>
<li>如果数据倾斜很严重，可以选择多个列作为数据的分桶键，建议不超过三个列</li>
</ul>
</li>
<li>设置分桶数<ul>
<li>自 2.5.7 版本起，StarRocks 支持根据机器资源和数据量自动设置分区中分桶数量。</li>
<li>如果表单个分区原始数据规模预计<strong>超过 100 GB</strong>，建议您手动设置分区中分桶数量。如果需要<strong>开启并行扫描 Tablet</strong>，则您需要确保系统变量 <code>enable_tablet_internal_parallel</code> 全局生效 <code>SET GLOBAL enable_tablet_internal_parallel = true;</code>。<strong>首先预估每个分区的数据量，然后按照每10GB原始数据一个Tablet计算，从而确定分桶数量</strong></li>
</ul>
</li>
<li>最佳实践<ul>
<li><strong>数据倾斜：</strong>如果业务场景中单独采用倾斜度大的列做分桶，很大程度会导致访问数据倾斜，那么建议采用多列组合的方式进行数据分桶。</li>
<li><strong>高并发：</strong>分区和分桶应该尽量覆盖查询语句所带的条件，这样可以有效减少扫描数据，提高并发。</li>
<li><strong>高吞吐：</strong>尽量把数据打散，让集群以更高的并发扫描数据，完成相应计算。</li>
<li><strong>元数据管理：</strong>Tablet 过多会增加 FE&#x2F;BE 的元数据管理和调度的资源消耗。</li>
</ul>
</li>
</ul>
<h4 id="表达式分区（推荐）"><a href="#表达式分区（推荐）" class="headerlink" title="表达式分区（推荐）"></a>表达式分区（推荐）</h4><h5 id="时间函数表达式分区"><a href="#时间函数表达式分区" class="headerlink" title="时间函数表达式分区"></a>时间函数表达式分区</h5><ul>
<li><p>时间函数表达式分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">PARTITION BY expression<br><br>expression ::=<br>    &#123; date_trunc ( &lt;time_unit&gt; , &lt;partition_column&gt; ) |<br>      time_slice ( &lt;partition_column&gt; , INTERVAL &lt;N&gt; &lt;time_unit&gt; [ , boundary ] ) &#125;<br></code></pre></td></tr></table></figure>
</li>
<li></li>
<li><table>
<thead>
<tr>
<th>参数</th>
<th>是否必填</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>expression</code></td>
<td>是</td>
<td>目前仅支持 <a href="https://docs.starrocks.io/zh/docs/3.1/sql-reference/sql-functions/date-time-functions/date_trunc/">date_trunc</a> 和 <a href="https://docs.starrocks.io/zh/docs/3.1/sql-reference/sql-functions/date-time-functions/time_slice/">time_slice</a> 函数。并且如果您使用 <code>time_slice</code> 函数，则可以不传入参数 <code>boundary</code>，因为在该场景中该参数默认且仅支持为 <code>floor</code>，不支持为 <code>ceil</code>。</td>
</tr>
<tr>
<td><code>time_unit</code></td>
<td>是</td>
<td>分区粒度，目前仅支持为 <code>hour</code>、<code>day</code>、<code>month</code> 或 <code>year</code>，暂时不支持为 <code>week</code>。如果分区粒度为 <code>hour</code>，则仅支持分区列为 DATETIME 类型，不支持为 DATE 类型。</td>
</tr>
<tr>
<td><code>partition_column</code></td>
<td>是</td>
<td>分区列。 仅支持为日期类型（DATE 或 DATETIME），不支持为其它类型。如果使用 <code>date_trunc</code> 函数，则分区列支持为 DATE 或 DATETIME 类型。如果使用 <code>time_slice</code> 函数，则分区列仅支持为 DATETIME 类型。分区列的值支持为 <code>NULL</code>。如果分区列是 DATE 类型，则范围支持为 [0000-01-01 ~ 9999-12-31]。如果分区列是 DATETIME 类型，则范围支持为 [0000-01-01 01:01:01 ~ 9999-12-31 23:59:59]。目前仅支持指定一个分区列，不支持指定多个分区列。</td>
</tr>
</tbody></table>
</li>
<li><pre><code class="sqlite">CREATE TABLE site_access1 (
    event_day DATETIME NOT NULL,
    site_id INT DEFAULT &#39;10&#39;,
    city_code VARCHAR(100),
    user_name VARCHAR(32) DEFAULT &#39;&#39;,
    pv BIGINT DEFAULT &#39;0&#39;
)
DUPLICATE KEY(event_day, site_id, city_code, user_name)
PARTITION BY date_trunc(&#39;day&#39;, event_day)
DISTRIBUTED BY HASH(event_day, site_id);

-- 分区生命周期管理，即仅保留最近一段时间的分区，删除历史分区，可以使用partition_live_number设置只保留最近多少数量的分区
<figure class="highlight autohotkey"><table><tr><td class="code"><pre><code class="hljs autohotkey">  <br>* 在导入的过程中 StarRocks 根据导入数据已经自动创建了一些分区，但是由于某些原因导入作业最终失败，则在当前版本中，已经自动创建的分区并不会由于导入失败而自动删除。<br><br>* StarRocks 自动创建分区数量上限默认为 <span class="hljs-number">4096</span>，由 FE 配置参数 `max_automatic_partition_number` 决定。该参数可以防止您由于误操作而创建大量分区。<br><br>* 分区命名规则：如果存在多个分区列，则不同分区列的值以下划线（_）连接。例如：存在有两个分区列 `dt` 和 `city`，均为字符串类型，导入一条数据 `2022-<span class="hljs-number">04</span>-<span class="hljs-number">01</span>`, `beijing`，则自动创建的分区名称为 `p20220401_beijing`。<br><br>##### 列表达式分区<br><br>* 语法<br><br>  ````sqlite<br>  PARTITION BY expression<br>  <br><span class="hljs-title">  expression :</span>:=<br>      ( &lt;partition_columns&gt; )<br>      <br><span class="hljs-title">  partition_columns :</span>:=<br>      &lt;column&gt;, [ &lt;column&gt; [,...] ]<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li></li>
<li><table>
<thead>
<tr>
<th>参数</th>
<th>是否必填</th>
<th>参数</th>
</tr>
</thead>
<tbody><tr>
<td><code>partition_columns</code></td>
<td>是</td>
<td>分区列。 支持为字符串（不支持 BINARY）、日期、整数和布尔值。不支持分区列的值为 <code>NULL</code>。导入后自动创建的一个分区中只能包含各分区列的一个值，如果需要包含各分区列的多值，请使用 <a href="https://docs.starrocks.io/zh/docs/3.1/table_design/list_partitioning/">List 分区</a>。</td>
</tr>
<tr>
<td><code>partition_live_number</code></td>
<td>否</td>
<td>保留多少数量的分区。比较这些分区包含的值，定期删除值小的分区，保留值大的。后台会定时调度任务来管理分区数量，调度间隔可以通过 FE 动态参数 <code>dynamic_partition_check_interval_seconds</code> 配置，默认为 600 秒，即 10 分钟。 <strong>说明</strong> 如果分区列里是字符串类型的值，则比较分区名称的字典序，定期保留排在前面的分区，删除排在后面的分区。</td>
</tr>
</tbody></table>
</li>
<li><pre><code class="sqlite">CREATE TABLE t_recharge_detail1 (
    id bigint,
    user_id bigint,
    recharge_money decimal(32,2), 
    city varchar(20) not null,
    dt varchar(20) not null
)
DUPLICATE KEY(id)
PARTITION BY (dt,city)
DISTRIBUTED BY HASH(`id`);
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><br><span class="hljs-operator">*</span> ````sqlite<br>  <span class="hljs-comment">-- 查看分区</span><br>  <span class="hljs-keyword">SHOW</span> PARTITIONS <span class="hljs-keyword">FROM</span> t_recharge_detail1;<br>  <br>  <span class="hljs-comment">-- 时间表达式分区导入数据</span><br>  <span class="hljs-keyword">INSERT</span> OVERWRITE site_access1 <span class="hljs-keyword">PARTITION</span>(event_day<span class="hljs-operator">=</span><span class="hljs-string">&#x27;2022-06-08 20:12:04&#x27;</span>)<br>      <span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> site_access2 <span class="hljs-keyword">PARTITION</span>(p20220608);<br>      <br>  <span class="hljs-comment">-- 列表达式分区导入数据</span><br>  <span class="hljs-keyword">INSERT</span> OVERWRITE t_recharge_detail1 <span class="hljs-keyword">PARTITION</span>(dt<span class="hljs-operator">=</span><span class="hljs-string">&#x27;2022-04-02&#x27;</span>,city<span class="hljs-operator">=</span><span class="hljs-string">&#x27;texas&#x27;</span>)<br>      <span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> t_recharge_detail2 <span class="hljs-keyword">PARTITION</span>(p20220402_texas);<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<h4 id="List分区"><a href="#List分区" class="headerlink" title="List分区"></a>List分区</h4><ul>
<li><p>适用于一个分区中需要包含分区列的多个值的场景</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">CREATE TABLE t_recharge_detail2 (<br>    id bigint,<br>    user_id bigint,<br>    recharge_money decimal(32,2), <br>    city varchar(20) not null,<br>    dt varchar(20) not null<br>)<br>DUPLICATE KEY(id)<br>PARTITION BY LIST (city) (<br>   PARTITION pCalifornia VALUES IN (&quot;Los Angeles&quot;,&quot;San Francisco&quot;,&quot;San Diego&quot;), -- 这些城市同属一个州<br>   PARTITION pTexas VALUES IN (&quot;Houston&quot;,&quot;Dallas&quot;,&quot;Austin&quot;)<br>)<br>DISTRIBUTED BY HASH(`id`);<br></code></pre></td></tr></table></figure></li>
</ul>
<h4 id="动态分区"><a href="#动态分区" class="headerlink" title="动态分区"></a>动态分区</h4><ul>
<li><p>动态分区开启后，按需为新数据动态地创建分区。同时SR会自动删除过期分区，从而保证数据地时效性</p>
</li>
<li><pre><code class="sqlite">CREATE TABLE site_access(
event_day DATE,
site_id INT DEFAULT &#39;10&#39;,
city_code VARCHAR(100),
user_name VARCHAR(32) DEFAULT &#39;&#39;,
pv BIGINT DEFAULT &#39;0&#39;
)
DUPLICATE KEY(event_day, site_id, city_code, user_name)
PARTITION BY RANGE(event_day)(
PARTITION p20200321 VALUES LESS THAN (&quot;2020-03-22&quot;),
PARTITION p20200322 VALUES LESS THAN (&quot;2020-03-23&quot;),
PARTITION p20200323 VALUES LESS THAN (&quot;2020-03-24&quot;),
PARTITION p20200324 VALUES LESS THAN (&quot;2020-03-25&quot;)
)
DISTRIBUTED BY HASH(event_day, site_id)
PROPERTIES(
    &quot;dynamic_partition.enable&quot; = &quot;true&quot;,
    &quot;dynamic_partition.time_unit&quot; = &quot;DAY&quot;,
    &quot;dynamic_partition.start&quot; = &quot;-3&quot;,
    &quot;dynamic_partition.end&quot; = &quot;3&quot;,
    &quot;dynamic_partition.prefix&quot; = &quot;p&quot;,
    &quot;dynamic_partition.history_partition_num&quot; = &quot;0&quot;
);
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><br>#### 临时分区<br><br>* 可以在一张已经定义分区规则的分区表上，创建临时分区，并为这些临时分区设定单独的数据分布策略<br><br>* ````sqlite<br>  <span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> &lt;<span class="hljs-built_in">table_name</span>&gt; <br>  <span class="hljs-keyword">ADD</span> <span class="hljs-keyword">TEMPORARY</span> <span class="hljs-keyword">PARTITION</span> &lt;temporary_partition_name&gt; <span class="hljs-keyword">VALUES</span> [(&quot;value1&quot;), &#123;<span class="hljs-keyword">MAXVALUE</span>|(&quot;value2&quot;)&#125;)]<br>  [(partition_desc)]<br>  [DISTRIBUTED <span class="hljs-keyword">BY</span> HASH(&lt;bucket_key&gt;)];<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<h3 id="索引-1"><a href="#索引-1" class="headerlink" title="索引"></a>索引</h3><ul>
<li>SR自动创建的索引，称为内置索引，包括前缀索引，Ordinal索引，ZoneMap索引<ul>
<li>Ordinal索引：分块存储，每个 Data Page 大小一般为 64*1024 个字节（data_page_size &#x3D; 64 * 1024）。每一个列 Date Page 会对应生成一条 Ordinal 索引项，记录 Data Page 的起始行号等信息。这样 Ordinal 索引提供了通过行号来查找列 Data Page 数据页的物理地址。其他索引查找数据时，最终都要通过 Ordinal 索引查找列 Data Page 的位置。</li>
<li>ZoneMap索引：ZoneMap 索引存储了每块数据统计信息，统计信息包括 Min 最大值、Max 最小值、HasNull 空值、HasNotNull 不全为空的信息。在查询时，StarRocks 可以根据这些统计信息，<strong>快速判断这些数据块是否可以过滤掉，从而减少扫描数据量</strong>，提升查询速度。<ul>
<li>在实现上，“每块”数据可以是一个 Segment，也可以是一个列的一个 Data Page，相应的 ZoneMap 索引有两种：一种是存每个 Segment 的统计信息，另一种是存每个 Data Page 的统计信息</li>
</ul>
</li>
</ul>
</li>
<li>SR用户手动创建索引，包括Bitmap索引和Bloom filter索引</li>
</ul>
<h4 id="前缀索引和排序键"><a href="#前缀索引和排序键" class="headerlink" title="前缀索引和排序键"></a>前缀索引和排序键</h4><ul>
<li>建表时会指定一个或多个列构成排序键(Sort Key)。表中的数据行会根据排序键进行排序后再落入磁盘存储。<strong>在数据写入过程中，会自动生成前缀索引。数据按照指定的排序键后，每写入1024行数据构成一个逻辑数据块（Data Bolck），在前缀索引表中存储一个索引项，内容为改逻辑数据块中第一行数据的排序列组成的前缀</strong>。通过这样两层的排序结构，利用二分查找快速跳过不符合条件的数据。</li>
<li>注意事项：<ul>
<li>前缀索引最大长度为36字节，超过部分会被截断，前缀字段的数量不超过三个</li>
<li><strong>前缀字段中 char，varchar，string 类型的列只能出现一次，并且出现在末尾</strong></li>
</ul>
</li>
<li>合理选择排序键<ul>
<li><strong>选择经常作为查询过滤条件的列为排序列</strong></li>
<li>如果多个排序列作为过滤条件的频率差不多，则可以衡量各排序列的<strong>基数特点</strong>（即数据的离散度）<ul>
<li>选择基数高的列可以过滤更多的数据，选择基数低的列则存储压缩率会高很多。</li>
</ul>
</li>
<li>排序列的数据类型支持数值（除 DOUBLE、FLOAT）、字符串和日期类型。</li>
</ul>
</li>
</ul>
<h4 id="Bitmap索引"><a href="#Bitmap索引" class="headerlink" title="Bitmap索引"></a>Bitmap索引</h4><ul>
<li><p>bitmap(位图索引)，bitmap为一个bit数组，一个bit取值有两种：0或1。每个bit对应数据表中的一行，并根据改行的取值情况来决定bit的取值是0还是1。一般适用于高基数列或者高基数列的组合</p>
</li>
<li><p>优势：</p>
<ul>
<li>bitmap所占存储空间小</li>
<li>支持为多列创建bitmap索引，提高多列的查询效率</li>
</ul>
</li>
<li><p>使用说明：</p>
<ul>
<li>适用于等值条件(<code>=</code>)查询或<code>IN</code>的范围查询的列</li>
<li>主键表和明细表中所有都可以创建bitmap索引；聚合表和更新表中，只有维度列（即 Key 列）支持创建 bitmap 索引。</li>
<li>以下类型的列支持创建bitmap索引：<ul>
<li>日期类型：DATE、DATETIME。</li>
<li>数值类型：TINYINT、SMALLINT、INT、BIGINT、LARGEINT、DECIMAL 和 BOOLEAN。</li>
<li>字符串类型：CHAR、STRING 和 VARCHAR。</li>
<li>其他类型：HLL。</li>
</ul>
</li>
</ul>
</li>
<li><p>案列</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">-- 建表时创建索引<br>CREATE TABLE d0.table_hash<br>(<br>    k1 TINYINT,<br>    k2 DECIMAL(10, 2) DEFAULT &quot;10.5&quot;,<br>    v1 CHAR(10) REPLACE,<br>    v2 INT SUM,<br>    INDEX index_name (column_name) [USING BITMAP] [COMMENT &#x27;&#x27;]<br>)<br>ENGINE = olap<br>AGGREGATE KEY(k1, k2)<br>DISTRIBUTED BY HASH(k1)<br>PROPERTIES (&quot;storage_type&quot; = &quot;column&quot;);<br><br>-- 建表后创建<br>CREATE INDEX index_name ON table_name (column_name) [USING BITMAP] [COMMENT &#x27;&#x27;];<br><br>-- 创建 Bitmap 索引为异步过程，查看索引创建进度<br>SHOW ALTER TABLE COLUMN [FROM db_name];<br><br>-- 查看索引<br>SHOW &#123; INDEX[ES] | KEY[S] &#125; FROM [db_name.]table_name [FROM db_name];<br><br>-- 删除索引<br>DROP INDEX index_name ON [db_name.]table_name;<br></code></pre></td></tr></table></figure></li>
</ul>
<h4 id="Bloom-filter索引"><a href="#Bloom-filter索引" class="headerlink" title="Bloom filter索引"></a>Bloom filter索引</h4><ul>
<li><p>Bloom filter 索引可以快速判断表的数据文件中是否可能包含要查询的数据，如果不包含就跳过，从而减少扫描的数据量。Bloom filter 索引空间效率高，适用于基数较高的列，如 ID 列。前缀索引对字段长度有限制。超出限制时就可以考虑Bloom filter索引</p>
</li>
<li><p>索引原理：</p>
<ul>
<li>Bloom filter索引判断一个数据文件中不存在目标数据，那SR会跳过该文件，从而提高查询效率</li>
<li>如果Bloom filter索引判断一个数据文件中可能存在数据目标，那SR会读取该文件确认目标数据是否存在。Bloom filter索引存在一定的误判率，即假阳性概率（False Positive Probability）</li>
</ul>
</li>
<li><p>使用说明基本和bitmap索引一致</p>
</li>
<li><p>案例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">-- 创建索引<br>CREATE TABLE table1<br>(<br>    k1 BIGINT,<br>    k2 LARGEINT,<br>    v1 VARCHAR(2048) REPLACE,<br>    v2 SMALLINT DEFAULT &quot;10&quot;<br>)<br>ENGINE = olap<br>PRIMARY KEY(k1, k2)<br>DISTRIBUTED BY HASH (k1, k2)<br>PROPERTIES(&quot;bloom_filter_columns&quot; = &quot;k1,k2&quot;);<br><br>-- 查看索引<br>SHOW CREATE TABLE table1;<br><br>-- 修改索引<br>ALTER TABLE table1 SET (&quot;bloom_filter_columns&quot; = &quot;k1,k2,v1&quot;);<br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h3><ul>
<li>SR支持对表和索引数据进行压缩(compression)。数据压缩不仅有助于节省存储空间，还可以提高 I&#x2F;O 密集型任务的性能。请注意，压缩和解压缩数据需要额外的 CPU 资源。</li>
<li>SR支持的压缩算法的压缩率排名：zlib &gt; Zstandard &gt; LZ4 &gt; Snappy</li>
<li>如果您对存储空间占用没有特殊需求，建议您使用 LZ4 或 Zstandard 算法。SR默认使用LZ4</li>
</ul>
<h2 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h2><p>更多的案例详情直接看<a href="https://docs.starrocks.io/zh/docs/loading/">官方文档</a>,这里列举几个我常用的</p>
<h3 id="导入概览"><a href="#导入概览" class="headerlink" title="导入概览"></a>导入概览</h3><ul>
<li>有多种导入方案，<a href="https://docs.starrocks.io/zh/docs/loading/loading_introduction/Loading_intro/">详情</a></li>
<li>导入数据需要Insert权限，通过GRANT给用户赋权</li>
<li>访问协议：<ul>
<li>StarRocks 支持通过以下两种访问协议来提交导入作业：MySQL 和 HTTP。当前只有 Stream Load 支持 HTTP 协议，其余导入方式均支持 MySQL 协议。</li>
</ul>
</li>
<li>标签机制<ul>
<li>StarRocks 通过导入作业实现数据导入。每个导入作业都有一个标签 (Label)，由用户指定或系统自动生成，用于标识该导入作业。每个标签在一个数据库内都是唯一的，仅可用于一个成功的导入作业。一个导入作业成功后，其标签不可再用于提交其他导入作业。只有失败的导入作业的标签，才可再用于提交其他导入作业。</li>
</ul>
</li>
<li>数据类型<ul>
<li>个别的类型可能存在问题，详情可看SR支持的<a href="https://docs.starrocks.io/zh/docs/sql-reference/data-types/data-type-list/">数据类型</a></li>
</ul>
</li>
<li>导入模式<ul>
<li>同步导入<ul>
<li>支持同步倒入的方式有Stream Load和INSERT</li>
</ul>
</li>
<li>异步导入<ul>
<li>异步导入是指您创建导入作业以后，StarRocks 直接返回作业创建结果。</li>
</ul>
</li>
<li>导入数据流程：<a href="https://docs.starrocks.io/zh/docs/loading/loading_introduction/loading_concepts/">参考连接</a></li>
</ul>
</li>
</ul>
<h3 id="从本地文件系统导入"><a href="#从本地文件系统导入" class="headerlink" title="从本地文件系统导入"></a>从本地文件系统导入</h3><h3 id="从HDFS导入"><a href="#从HDFS导入" class="headerlink" title="从HDFS导入"></a>从HDFS导入</h3><ul>
<li>SR支持通过以下方式从HDFS导入数据<ul>
<li><p>使用INSERT+<code>FILES()</code>进行同步导入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">CREATE TABLE tmp.xinjiang AS<br>SELECT * FROM FILES<br>(<br>    &quot;path&quot; = &quot;hdfs://192.168.0.156:8020/warehouse/tablespace/external/hive/temp.db/xinjiang/*&quot;,<br>    &quot;format&quot; = &quot;orc&quot;,<br>    &quot;hadoop.security.authentication&quot; = &quot;simple&quot;,<br>    &quot;username&quot; = &quot;&lt;hdfs_username&gt;&quot;,<br>    &quot;password&quot; = &quot;&lt;hdfs_password&gt;&quot;<br>);<br><br>SELECT * FROM information_schema.loads ORDER BY JOB_ID DESC;<br><br>SELECT * FROM information_schema.loads where label=&#x27;insert_e0876288-4353-11ef-9ab8-000c291d2710&#x27;;<br></code></pre></td></tr></table></figure>


</li>
<li><p>使用Broker Load进行异步导入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">案例：<br>/*Hive2SR同步示例*/<br>/*1、每次同步需换label*/<br>LOAD LABEL dwd.dwd_policy_etp_project_library_enterprise_ref_df2<br>(/*2、如下填写Hive对应的hdfs地址，需使用外部表，建表时需要注意，管理表存在权限问题*/<br>DATA INFILE(&quot;hdfs://192.168.0.156:8020/warehouse/tablespace/external/hive/dwd.db/dwd_policy_etp_project_library_enterprise_ref_wf/*&quot;)/*确认目前active的hdfs路径*/<br>INTO TABLE dwd_policy_etp_project_library_enterprise_ref_df<br>COLUMNS TERMINATED BY &quot;|^|&quot;<br>FORMAT AS &quot;orc&quot;<br>(eid, project_id, province_code,city_code,area_code,project_name,score,match_items_ids)/*填写SR表字段顺序映射到hive对应hive的字段名称*/<br>/*set可以设置sr字段名和hive字段的映射*/<br>set(<br>    eid=eid,<br>    project_id=project_id,<br>    province_code=province_code,<br>    city_code=city_code,<br>    area_code=area_code,<br>    project_name=project_name,<br>    score=score,<br>    match_item_ids=match_items_ids<br>)<br>)<br>WITH BROKER &#x27;hdfs_broker&#x27;;<br><br>/*查看同步状态*/<br>use dwd;<br>show load where label = &#x27;dwd_policy_etp_project_library_enterprise_ref_df2&#x27;;<br></code></pre></td></tr></table></figure>
</li>
<li><p>使用Pipe进行持续的异步导入</p>
</li>
</ul>
</li>
</ul>
<h4 id="INSERT-FILES的优势"><a href="#INSERT-FILES的优势" class="headerlink" title="INSERT+FILES的优势"></a>INSERT+FILES的优势</h4><ul>
<li><p><code>FILES()</code> 会根据给定的数据路径等参数读取数据，并自动根据数据文件的格式、列信息等推断出表结构，最终以数据行的形式返回文件中的数据。</p>
</li>
<li><p>使用方法：</p>
<ul>
<li>使用SELECT语句直接从HDFS查询数据</li>
<li>通过CREATE TABLE AS SELECT 语句实现自动建表和导入语句</li>
<li>手动建表，然后通过INSERT导入数据</li>
</ul>
</li>
<li><p>案例</p>
<ul>
<li><pre><code class="sqlite">-- 使用SELECT语句直接从HDFS查询数据
SELECT * FROM FILES
(
    &quot;path&quot; = &quot;hdfs://&lt;hdfs_ip&gt;:&lt;hdfs_port&gt;/user/amber/user_behavior_ten_million_rows.parquet&quot;,
    &quot;format&quot; = &quot;parquet&quot;,
    &quot;hadoop.security.authentication&quot; = &quot;simple&quot;,
    &quot;username&quot; = &quot;&lt;hdfs_username&gt;&quot;,
    &quot;password&quot; = &quot;&lt;hdfs_password&gt;&quot;
)
LIMIT 3;
-- CTAS自动建表并导入数据
CREATE TABLE user_behavior_inferred AS
SELECT * FROM FILES
(
    &quot;path&quot; = &quot;hdfs://&lt;hdfs_ip&gt;:&lt;hdfs_port&gt;/user/amber/user_behavior_ten_million_rows.parquet&quot;,
    &quot;format&quot; = &quot;parquet&quot;,
    &quot;hadoop.security.authentication&quot; = &quot;simple&quot;,
    &quot;username&quot; = &quot;&lt;hdfs_username&gt;&quot;,
    &quot;password&quot; = &quot;&lt;hdfs_password&gt;&quot;
);
-- 手动建表并通过INSERT导入数据
CREATE TABLE user_behavior_declared
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp varbinary
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID);
-- 导入数据
INSERT INTO user_behavior_declared
SELECT * FROM FILES
(
    &quot;path&quot; = &quot;hdfs://&lt;hdfs_ip&gt;:&lt;hdfs_port&gt;/user/amber/user_behavior_ten_million_rows.parquet&quot;,
    &quot;format&quot; = &quot;parquet&quot;,
    &quot;hadoop.security.authentication&quot; = &quot;simple&quot;,
    &quot;username&quot; = &quot;&lt;hdfs_username&gt;&quot;,
    &quot;password&quot; = &quot;&lt;hdfs_password&gt;&quot;
);
-- 查看导入进度
SELECT * FROM information_schema.loads ORDER BY JOB_ID DESC;
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk"><br><span class="hljs-comment">### 通过导入实现数据变更</span><br><br>* StarRocks 的[主键表](https:<span class="hljs-regexp">//</span>docs.starrocks.io<span class="hljs-regexp">/zh/</span>docs<span class="hljs-regexp">/table_design/</span>table_types<span class="hljs-regexp">/primary_key_table/</span>)支持通过 [Stream Load](https:<span class="hljs-regexp">//</span>docs.starrocks.io<span class="hljs-regexp">/zh/</span>docs<span class="hljs-regexp">/sql-reference/</span>sql-statements<span class="hljs-regexp">/data-manipulation/</span>STREAM_LOAD<span class="hljs-regexp">/)、[Broker Load](https:/</span><span class="hljs-regexp">/docs.starrocks.io/</span>zh<span class="hljs-regexp">/docs/</span>sql-reference<span class="hljs-regexp">/sql-statements/</span>data-manipulation<span class="hljs-regexp">/BROKER_LOAD/</span>) 或 [Routine Load](https:<span class="hljs-regexp">//</span>docs.starrocks.io<span class="hljs-regexp">/zh/</span>docs<span class="hljs-regexp">/sql-reference/</span>sql-statements<span class="hljs-regexp">/data-manipulation/</span>CREATE_ROUTINE_LOAD<span class="hljs-regexp">/) 导入作业，对 StarRocks 表进行数据变更，包括插入、更新和删除数据。不支持通过 [Spark Load](https:/</span><span class="hljs-regexp">/docs.starrocks.io/</span>zh<span class="hljs-regexp">/docs/</span>sql-reference<span class="hljs-regexp">/sql-statements/</span>data-manipulation<span class="hljs-regexp">/SPARK_LOAD/</span>) 导入作业或 [INSERT](https:<span class="hljs-regexp">//</span>docs.starrocks.io<span class="hljs-regexp">/zh/</span>docs<span class="hljs-regexp">/sql-reference/</span>sql-statements<span class="hljs-regexp">/data-manipulation/</span>INSERT/) 语句对 StarRocks 表进行数据变更。<br><br><span class="hljs-comment">## 数据湖</span><br><br>* [![SR的数据湖架构](https:<span class="hljs-regexp">//</span>s21.ax1x.com<span class="hljs-regexp">/2024/</span><span class="hljs-number">04</span><span class="hljs-regexp">/01/</span>pF78qOJ.png)](https:<span class="hljs-regexp">//img</span>se.com<span class="hljs-regexp">/i/</span>pF78qOJ)<br><br>* StarRocks 不仅能高效的分析本地存储的数据，也可以作为计算引擎直接分析数据湖中的数据。用户可以通过 StarRocks 提供的 External Catalog，轻松查询存储在 Apache Hive、Apache Iceberg、Apache Hudi、Delta Lake 等数据湖上的数据，无需进行数据迁移。支持的存储系统包括 HDFS、S3、OSS，支持的文件格式包括 Parquet、ORC、CSV。<br>* 如上图所示，在数据湖分析场景中，StarRocks 主要负责数据的计算分析，而数据湖则主要负责数据的存储、组织和维护。使用数据湖的优势在于可以使用开放的存储格式和灵活多变的 schema 定义方式，可以让 BI<span class="hljs-regexp">/AI/</span>Adhoc/报表等业务有统一的 single source of truth。而 StarRocks 作为数据湖的计算引擎，可以充分发挥向量化引擎和 CBO 的优势，大大提升了数据湖分析的性能。<br>* SR湖仓一体方案重点在于：<br>  - 规范的Catalog及元数据服务集成<br>    - StarRocks 提供两种类型的 Catalog：Internal Catalog 和 External Catalog。Internal Catalog 用于管理 StarRocks 数据库中存储的数据的元数据。External Catalog 用于连接存储在 Hive、Iceberg、Hudi、Delta Lake 等各种外部数据源中的数据。<br>  - 弹性可扩展的计算节点<br>    - 在存算分离架构下，存储和计算的分离降低了扩展的复杂度。StarRocks 计算节点仅存储本地缓存，方便您根据负载情况灵活地添加或移除计算节点。<br>  - 灵活的缓存机制<br>    - 您可以根据实际情况选择打开或者关闭本地缓存。在计算节点因为负载变化过快而频繁启动和关闭、或者是查询大多集中在近期数据等场景下，数据缓存意义不大，可以无需开启缓存。<br><br><span class="hljs-comment">### Catalog</span><br><br>* SR自<span class="hljs-number">2.3</span>版本开始支持Catalog(数据目录)功能，实现在一套系统内同时维护内外部数据，方便用户轻松访问并查询存储在各类外部源的数据。<br>* **内部数据**指是保存在SR中的数据。<br>* **外部数据**是指保存在外部数据源中的数据。（如hive,iceberg,jdbc）<br>* [![Catalog](https:<span class="hljs-regexp">//</span>s21.ax1x.com<span class="hljs-regexp">/2024/</span><span class="hljs-number">04</span><span class="hljs-regexp">/01/</span>pF7YJzT.png)](https:<span class="hljs-regexp">//img</span>se.com<span class="hljs-regexp">/i/</span>pF7YJzT)<br><br>- **Internal catalog**: 内部数据目录，用于管理 StarRocks 所有内部数据。例如，执行 CREATE DATABASE 和 CREATE TABLE 语句创建的数据库和数据表都由 internal catalog 管理。 每个 StarRocks 集群都有且只有一个 internal catalog 名为 [default_catalog](https:<span class="hljs-regexp">//</span>docs.starrocks.io<span class="hljs-regexp">/zh/</span>docs<span class="hljs-regexp">/data_source/</span>catalog<span class="hljs-regexp">/default_catalog/</span>)。<br><br>- **External catalog**: 外部数据目录，用于连接外部 metastore。在 StarRocks 中，您可以通过 external catalog 直接查询外部数据，无需进行数据导入或迁移。当前支持创建以下类型的 external catalog：<br><br>  - hive<br><br>    - ````sqlite<br>      -- 案例<br>      CREATE EXTERNAL CATALOG hive_catalog_hms<br>      PROPERTIES<br>      (<br>          <span class="hljs-string">&quot;type&quot;</span> = <span class="hljs-string">&quot;hive&quot;</span>,<br>          <span class="hljs-string">&quot;hive.metastore.type&quot;</span> = <span class="hljs-string">&quot;hive&quot;</span>,<br>          <span class="hljs-string">&quot;hive.metastore.uris&quot;</span> = <span class="hljs-string">&quot;thrift://xx.xx.xx.xx:9083&quot;</span><br>      );<br>      -- HA模式<br>      CREATE EXTERNAL CATALOG `hive_metastore_catalog`<br>      PROPERTIES (<span class="hljs-string">&quot;hive.metastore.uris&quot;</span>  =  <span class="hljs-string">&quot;thrift://x.x.x.xxx:9083,thrift://x.x.x.xxx:9083&quot;</span>,<br>      <span class="hljs-string">&quot;metastore_cache_refresh_interval_sec&quot;</span>  =  <span class="hljs-string">&quot;30&quot;</span>,<br>      <span class="hljs-string">&quot;type&quot;</span>  =  <span class="hljs-string">&quot;hive&quot;</span>,<br>      <span class="hljs-string">&quot;enable_profile&quot;</span>  =  <span class="hljs-string">&quot;true&quot;</span>,<br>      <span class="hljs-string">&quot;enable_metastore_cache&quot;</span>  =  <span class="hljs-string">&quot;true&quot;</span><br>      )<br>      -- 查看所有的catalog<br>      SHOW CATALOGS;<br>      <br></code></pre></td></tr></table></figure>

- 
</code></pre>
</li>
<li><p>iceberg</p>
</li>
<li><p>hudi</p>
</li>
<li><p>delta lake</p>
</li>
<li><p>jdbc</p>
</li>
<li><p><a href="https://docs.starrocks.io/zh/docs/data_source/catalog/elasticsearch_catalog/">Elasticsearch catalog</a>：用于查询 Elasticsearch 中的数据。该特性自 3.1 版本起支持。</p>
</li>
<li><p><a href="https://docs.starrocks.io/zh/docs/data_source/catalog/paimon_catalog/">Paimon catalog</a>：用于查询 Paimon 中的数据。该特性自 3.1 版本起支持。</p>
</li>
<li><p><a href="https://docs.starrocks.io/zh/docs/data_source/catalog/unified_catalog/">Unified catalog</a>：把 Hive、Iceberg、Hudi 和 Delta Lake 作为一个融合的数据源，从中查询数据。该特性自 3.2 版本起支持。</p>
</li>
<li><p>在使用external catalog查询数据时，SR会用到外部数据源的两个组件：</p>
<ul>
<li><strong>元数据服务</strong>：用于将元数据暴露出来供SR的FE进行查询规划。</li>
<li><strong>存储系统</strong>：用于存储数据。数据文件以不同的格式存储在分布式文件系统或对象存储系统中。当FE将生成的查询计划分发给各个BE后，各个BE会并行扫描存储系统中的目标数据，并执行计算返回查询结果。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h3><ul>
<li>StarRocks 支持以外部表 (External Table) 的形式，接入其他数据源。外部表指的是保存在其他数据源中的数据表，而 StartRocks 只保存表对应的元数据，并直接向外部表所在数据源发起查询。目前 StarRocks 已支持的第三方数据源包括 MySQL、StarRocks、Elasticsearch、Apache Hive™、Apache Iceberg 和 Apache Hudi。<strong>对于 StarRocks 数据源，现阶段只支持 Insert 写入，不支持读取，对于其他数据源，现阶段只支持读取，还不支持写入</strong>。</li>
<li>推荐使用JDBC catalog，Hive catalog等</li>
</ul>
<h3 id="文件外部表"><a href="#文件外部表" class="headerlink" title="文件外部表"></a>文件外部表</h3><ul>
<li><p>文件外部表 (File External Table) 是一种特殊的外部表。您可以通过文件外部表直接查询外部存储系统上的 Parquet 和 ORC 格式的数据文件，无需导入数据。同时，文件外部表也不依赖任何 Metastore。StarRocks 当前支持的外部存储系统包括 HDFS、Amazon S3 及其他兼容 S3 协议的对象存储、阿里云对象存储 OSS 和腾讯云对象存储 COS。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql">USE db_example;<br><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">EXTERNAL</span> <span class="hljs-keyword">TABLE</span> t0<br>(<br>    name string, <br>    id <span class="hljs-type">int</span><br>) <br>ENGINE<span class="hljs-operator">=</span>file<br>PROPERTIES <br>(<br>    &quot;path&quot;<span class="hljs-operator">=</span>&quot;hdfs://x.x.x.x:8020/user/hive/warehouse/person_parq/&quot;, <br>    &quot;format&quot;<span class="hljs-operator">=</span>&quot;parquet&quot;<br>);<br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="Data-Cache"><a href="#Data-Cache" class="headerlink" title="Data Cache"></a>Data Cache</h3><ul>
<li><p>在数据湖分析场景中，SR作为OLAP查询引擎需要扫描HDFS或者其他的外部存储系统，查询实际读取的文件数量越多，I&#x2F;O开销也越大。为提高查询外部存储系统数据的性能，Data Cache功能可以将外部存储系统的原始数据按照一定策略切分成多个block后，缓存至Starrocks本地的BE节点，避免重复的远端数据拉取开销，实现热点数据查询分析性能的进一步提升。</p>
</li>
<li><p>StarRocks 以 BE 节点的内存和磁盘作为缓存的存储介质，支持全内存缓存或者内存+磁盘的两级缓存。</p>
</li>
<li><p>缓存淘汰机制（LRU）策略来缓存和淘汰数据</p>
<ul>
<li>优先从内存读取数据，内存中没有则再从磁盘上读取。从磁盘上读取的数据，会尝试加载到内存中</li>
<li>从内存中淘汰的数据，会尝试写入磁盘；从磁盘上淘汰的数据，会被废弃</li>
</ul>
</li>
<li><p>常见配置</p>
<ul>
<li><pre><code># 开启 Data Cache。
datacache_enable = true  

# 设置磁盘路径，假设 BE 机器有两块磁盘。
datacache_disk_path = /home/disk1/sr/dla_cache_data/;/home/disk2/sr/dla_cache_data/ 

# 设置内存缓存数据量的上限为 2 GB。
datacache_mem_size = 2147483648

# 设置单个磁盘缓存数据量的上限为 1.2 TB。
datacache_disk_size = 1288490188800
<figure class="highlight markdown"><table><tr><td class="code"><pre><code class="hljs markdown"><br><span class="hljs-bullet">*</span> Data Cache命中情况可以查看query profile，通过以下三次参数评判cache的命中情况：<br><br><span class="hljs-bullet">  -</span> <span class="hljs-code">`DataCacheReadBytes`</span>：从内存和磁盘中读取的数据量。<br><span class="hljs-bullet">  -</span> <span class="hljs-code">`DataCacheWriteBytes`</span>：从外部存储系统加载到内存和磁盘的数据量。<br><span class="hljs-bullet">  -</span> <span class="hljs-code">`BytesRead`</span>：总共读取的数据量，包括从内存、磁盘以及外部存储读取的数据量。<br><br><span class="hljs-section">#### Data Cache预热</span><br><br><span class="hljs-bullet">*</span> Data Cache预热是主动填充cache的过程，提前将想要查询的数据放到cache中，是基于Data Cache的扩展。<br><br><span class="hljs-bullet">*</span> 适用场景<br><br><span class="hljs-bullet">  -</span> 缓存的磁盘容量大于待预热的数据量<br><span class="hljs-bullet">  -</span> 缓存盘的数据访问空间比较稳定。<br><br>  <span class="hljs-code">````sqlite</span><br><span class="hljs-code">  CACHE SELECT &lt;column_name&gt; [, ...]</span><br><span class="hljs-code">  FROM [&lt;catalog_name&gt;.][&lt;db_name&gt;.]&lt;table_name&gt; [WHERE &lt;boolean_expression&gt;]</span><br><span class="hljs-code">  [PROPERTIES(&quot;verbose&quot;=&quot;true&quot;)]</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p>CACHE SELECT 可以和 <a href="https://docs.starrocks.io/zh/docs/sql-reference/sql-statements/data-manipulation/SUBMIT_TASK/">SUBMIT TASK</a> 结合(v3.3后的版本)，实现周期性的预热。</p>
</li>
</ul>
<h2 id="查询加速"><a href="#查询加速" class="headerlink" title="查询加速"></a>查询加速</h2><h3 id="CBO统计信息"><a href="#CBO统计信息" class="headerlink" title="CBO统计信息"></a>CBO统计信息</h3><ul>
<li>StarRocks CBO 优化器（Cost-based Optimizer）(基于成本的优化器)</li>
<li>CBO 优化器是查询优化的关键。<strong>一条 SQL 查询到达 StarRocks 后，会解析为一条逻辑执行计划，CBO 优化器对逻辑计划进行改写和转换，生成多个物理执行计划。通过估算计划中每个算子的执行代价（CPU、内存、网络、I&#x2F;O 等资源消耗），选择代价最低的一条查询路径作为最终的物理查询计划。</strong></li>
<li><strong>基于多种统计信息进行代价估算，能够在数万级别的执行计划中，选择代价最低的执行计划，提升复杂查询的效率和性能。</strong></li>
<li>SR会采集多种统计信息，为查询优化提供代价估算的参考。<a href="https://docs.starrocks.io/zh/docs/using_starrocks/Cost_based_optimizer/">详细信息</a></li>
<li></li>
</ul>
<h3 id="同步物化视图"><a href="#同步物化视图" class="headerlink" title="同步物化视图"></a>同步物化视图</h3><ul>
<li><p><strong>同步物化视图下，所有对于基表的数据变更都会自动同步更新到物化视图中。</strong>您无需手动调用刷新命令，即可实现自动同步刷新物化视图。同步物化视图的管理成本和更新成本都比较低，适合实时场景下单表聚合查询的透明加速。</p>
<ul>
<li>目前， StarRocks 存算分离集群暂不支持同步物化视图。</li>
</ul>
</li>
<li><p>下表从支持的特性角度比较了 StarRocks 2.5、2.4 中的异步物化视图以及同步物化视图（Rollup）：</p>
<table>
<thead>
<tr>
<th></th>
<th><strong>单表聚合</strong></th>
<th><strong>多表关联</strong></th>
<th><strong>查询改写</strong></th>
<th><strong>刷新策略</strong></th>
<th><strong>基表</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>异步物化视图</strong></td>
<td>是</td>
<td>是</td>
<td>是</td>
<td>异步刷新手动刷新</td>
<td>支持多表构建。基表可以来自：Default CatalogExternal Catalog（v2.5）已有异步物化视图（v2.5）已有视图（v3.1）</td>
</tr>
<tr>
<td><strong>同步物化视图（Rollup）</strong></td>
<td>仅部分聚合函数</td>
<td>否</td>
<td>是</td>
<td>导入同步刷新</td>
<td>仅支持基于 Default Catalog 的单表构建</td>
</tr>
</tbody></table>
</li>
<li><p><strong>查询改写是指在对已构建了物化视图的基表进行查询时，系统自动判断是否可以复用物化视图中的预计算结果处理查询。</strong>如果可以复用，系统会直接从相关的物化视图读取预计算结果，以避免重复计算消耗系统资源和时间。</p>
</li>
<li><pre><code class="sqlite">-- 创建同步物化视图
CREATE MATERIALIZED VIEW store_amt AS
SELECT store_id, SUM(sale_amt)
FROM sales_records
GROUP BY store_id;
-- 查看同步物化视图构建状态
-- 创建同步物化视图是一个异步的操作。CREATE MATERIALIZED VIEW 命令执行成功即代表创建同步物化视图的任务提交成功。您可以通过 SHOW ALTER MATERIALIZED VIEW 命令查看当前数据库中同步物化视图的构建状态。
SHOW ALTER MATERIALIZED VIEW
-- 查看同步物化视图的表结构
DESC sales_records ALL
-- 删除正在创建的同步物化视图
-- 首先需要通过 查看同步物化视图构建状态 获取该同步物化视图的任务 ID JobID
CANCEL ALTER TABLE ROLLUP FROM sales_records (12090);
-- 删除已创建的同步物化视图
DROP MATERIALIZED VIEW store_amt;
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><br>* 最佳实践<br><br>  ````sqlite<br>  <span class="hljs-comment">-- 以下示例基于一张广告业务相关的明细表 advertiser_view_record，其中记录了点击日期 click_time、广告客户 advertiser、点击渠道 channel 以及点击用户 ID user_id。</span><br>  <span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> advertiser_view_record(<br>      click_time <span class="hljs-type">DATE</span>,<br>      advertiser <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">10</span>),<br>      channel <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">10</span>),<br>      user_id <span class="hljs-type">INT</span><br>  ) distributed <span class="hljs-keyword">BY</span> hash(click_time);<br>  <span class="hljs-comment">-- 该场景需要频繁使用如下语句查询点击广告的 UV。</span><br>  <span class="hljs-keyword">SELECT</span> advertiser, channel, count(<span class="hljs-keyword">distinct</span> user_id)<br>  <span class="hljs-keyword">FROM</span> advertiser_view_record<br>  <span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> advertiser, channel;<br>  <span class="hljs-comment">-- 如需实现精确去重查询加速，您可以基于该明细表创建一张同步物化视图，并使用 bitmap_union() 函数预先聚合数据。</span><br>  <span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">MATERIALIZED</span> <span class="hljs-keyword">VIEW</span> advertiser_uv <span class="hljs-keyword">AS</span><br>  <span class="hljs-keyword">SELECT</span> advertiser, channel, bitmap_union(to_bitmap(user_id))<br>  <span class="hljs-keyword">FROM</span> advertiser_view_record<br>  <span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> advertiser, channel;<br>  <span class="hljs-comment">-- 同步物化视图创建完成后，后续查询语句中的子查询 count(distinct user_id) 会被自动改写为 bitmap_union_count (to_bitmap(user_id)) 以便查询命中物化视图。</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<h3 id="异步物化视图"><a href="#异步物化视图" class="headerlink" title="异步物化视图"></a>异步物化视图</h3><ul>
<li>相较于同步物化视图，<strong>异步物化视图支持多表关联以及更加丰富的聚合算子。异步物化视图可以通过手动调用或定时任务的方式刷新，并且支持刷新部分分区，可以大幅降低刷新成本</strong>。除此之外，异步物化视图支持多种查询改写场景，实现自动、透明查询加速。</li>
</ul>
<h4 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h4><ul>
<li>加速重复聚合查询<ul>
<li>假设您的数仓环境中存在大量包含相同聚合函数子查询的查询，占用了大量计算资源，您可以根据该子查询建立异步物化视图，计算并保存该子查询的所有结果。建立成功后，系统将自动改写查询语句，直接查询异步物化视图中的中间结果，从而降低负载，加速查询。</li>
</ul>
</li>
<li>周期性多表关联查询<ul>
<li>假设您需要定期将数据仓库中多张表关联，生成一张新的宽表，您可以为这些表建立异步物化视图，并设定定期刷新规则，从而避免手动调度关联任务。异步物化视图建立成功后，查询将直接基于异步物化视图返回结果，从而避免关联操作带来的延迟。</li>
</ul>
</li>
<li>数仓分层<ul>
<li>假设您的基表中包含大量原始数据，查询需要进行复杂的 ETL 操作，您可以通过对数据建立多层异步物化视图实现数仓分层。如此可以将复杂查询分解为多层简单查询，既可以减少重复计算，又能够帮助维护人员快速定位问题。除此之外，数仓分层还可以将原始数据与统计数据解耦，从而保护敏感性原始数据。</li>
</ul>
</li>
<li>湖仓加速<ul>
<li>查询数据湖可能由于网络延迟和对象存储的吞吐限制而变慢。您可以通过在数据湖之上构建异步物化视图来提升查询性能。此外，StarRocks 可以智能改写查询以使用现有的物化视图，省去了手动修改查询的麻烦。</li>
</ul>
</li>
</ul>
<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><ul>
<li><p>创建异步物化视图的一些说明</p>
<ul>
<li>创建异步物化视图时必须至少指定分桶和刷新策略其中之一。</li>
<li>您可以为异步物化视图设置与其基表不同的分区和分桶策略，但异步物化视图的分区列和分桶列必须在查询语句中。</li>
<li>异步物化视图支持分区上卷。例如，基表基于天做分区方式，您可以设置异步物化视图按月做分区。</li>
<li>异步物化视图暂不支持使用 List 分区策略，亦不支持基于使用 List 分区的基表创建。</li>
<li>创建物化视图的查询语句不支持非确定性函数，其中包括 rand()、random()、uuid() 和 sleep()。</li>
<li>异步物化视图支持多种数据类型。有关详细信息，请参阅 <a href="https://docs.starrocks.io/zh/docs/sql-reference/sql-statements/data-definition/CREATE_MATERIALIZED_VIEW/#%E6%94%AF%E6%8C%81%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B">CREATE MATERIALIZED VIEW - 支持数据类型</a>。</li>
<li>默认情况下，执行 CREATE MATERIALIZED VIEW 语句后，StarRocks 将立即开始刷新任务，这将会占用一定系统资源。如需推迟刷新时间，请添加 REFRESH DEFERRED 参数。</li>
</ul>
</li>
<li><p>异步物化视图刷新，支持异步刷新(ASYNC)和手动刷新(MANUAL)</p>
<ul>
<li>支持设置刷新最大分区数。当一张异步物化视图拥有较多分区时，单次刷新将耗费较多资源。您可以通过设置该刷新机制来指定单次刷新的最大分区数量，从而将刷新任务进行拆分，保证数据量多的物化视图能够分批、稳定的完成刷新。</li>
<li>支持为异步物化视图的分区指定 Time to Live（TTL），从而减少异步物化视图占用的存储空间。</li>
<li>支持指定刷新范围，只刷新最新的几个分区，减少刷新开销。</li>
<li>支持设置数据变更不会触发对应物化视图自动刷新的基表。</li>
<li>支持为刷新任务设置资源组。</li>
</ul>
</li>
<li><p>嵌套物化视图：<strong>即基于异步物化视图构建新的异步物化视图。每个异步物化视图的刷新方式仅影响当前物化视图。</strong>建议嵌套不要超过三层</p>
</li>
<li><p>案例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">-- 基于查询语句创建异步物化视图<br>CREATE MATERIALIZED VIEW order_mv<br>DISTRIBUTED BY HASH(`order_id`)<br>REFRESH ASYNC START(&#x27;2022-09-01 10:00:00&#x27;) EVERY (interval 1 day)<br>AS SELECT<br>    order_list.order_id,<br>    sum(goods.price) as total<br>FROM order_list INNER JOIN goods ON goods.item_id1 = order_list.item_id2<br>GROUP BY order_id;<br><br>-- 手动刷新物化视图<br>-- 异步调用刷新任务。<br>REFRESH MATERIALIZED VIEW order_mv;<br>-- 同步调用刷新任务。<br>REFRESH MATERIALIZED VIEW order_mv WITH SYNC MODE;<br>-- 强制刷新物化视图<br>refresh MATERIALIZED VIEW tmp.zt_test force;<br><br>-- 修改异步物化视图<br>-- 启用被禁用的异步物化视图（将物化视图的状态设置为 Active）<br>ALTER MATERIALIZED VIEW order_mv ACTIVE;<br>-- 修改异步物化视图名称为 order_total<br>ALTER MATERIALIZED VIEW order_mv RENAME order_total;<br>-- 修改异步物化视图的最大刷新间隔为 2 天<br>ALTER MATERIALIZED VIEW order_mv REFRESH ASYNC EVERY(INTERVAL 2 DAY);<br><br>-- 查看异步物化视图<br>SHOW MATERIALIZED VIEWS;<br>SHOW MATERIALIZED VIEWS WHERE NAME = &quot;order_mv&quot;;<br>SHOW MATERIALIZED VIEWS WHERE NAME LIKE &quot;order%&quot;;<br><br>-- 查看异步物化视图创建语句<br>SHOW CREATE MATERIALIZED VIEW order_mv;<br><br>-- 删除异步物化视图<br>DROP MATERIALIZED VIEW order_mv;<br></code></pre></td></tr></table></figure></li>
</ul>
<h4 id="使用物化视图进行数据建模"><a href="#使用物化视图进行数据建模" class="headerlink" title="使用物化视图进行数据建模"></a>使用物化视图进行数据建模</h4><ul>
<li><p>异步物化视图的能力</p>
<ul>
<li><strong>自动刷新</strong>：在数据导入至基表后，物化视图可以自动刷新。您无需在外部维护调度任务。</li>
<li><strong>分区刷新</strong>：通过有时序属性的报表，可以通过分区刷新实现近实时计算。</li>
<li><strong>与视图协同使用</strong>：通过协同使用物化视图和逻辑视图，您可以实现多层建模，从而实现中间层的重复使用和数据模型的简化。</li>
<li><strong>Schema Change</strong>：您可以通过简单的 SQL 语句更改计算结果，无需修改复杂的数据流水线。</li>
</ul>
</li>
<li><p>分层建模</p>
<ul>
<li><p>利用物化视图实现分层建模</p>
<ul>
<li><pre><code class="sqlite">-- 修改基表。
ALTER TABLE &lt;table_name&gt; ADD COLUMN &lt;column_desc&gt;;

-- 原子替换基表。
ALTER TABLE &lt;table1&gt; SWAP WITH &lt;table2&gt;;

-- 修改视图定义。
ALTER VIEW &lt;view_name&gt; AS &lt;query&gt;;

-- 原子替换物化视图（替换两个物化视图的名字，并不修改其中数据）。
ALTER MATERIALIZED VIEW &lt;mv1&gt; SWAP WITH &lt;mv2&gt;;

-- 重新启用物化视图。
ALTER MATERIALIZED VIEW &lt;mv_name&gt; ACTIVE;
<figure class="highlight markdown"><table><tr><td class="code"><pre><code class="hljs markdown"><br><span class="hljs-bullet">*</span> 分区建模<br><br><span class="hljs-bullet">  -</span> 利用SR的分区管理，合理的搭配数据建模，实现高效的数据更新。<br><span class="hljs-bullet">  -</span> 分区管理可以支持多种业务场景<br><span class="hljs-bullet">    -</span> <span class="hljs-strong">**事实表更新**</span>：您可以将事实表分区到细粒度级别，例如按日或按小时。在事实表更新后，物化视图中相应的分区将自动刷新。<br><span class="hljs-bullet">    -</span> <span class="hljs-strong">**维度表更新**</span>：通常，维度表中的数据更新将导致所有关联结果的刷新，刷新代价较大。您可以选择忽略某些维度表中的数据更新，以避免刷新整个物化视图，或者您可以指定一个时间范围，从而只有在该时间范围内的分区才能被刷新。<br><span class="hljs-bullet">    -</span> <span class="hljs-strong">**外部表的自动刷新**</span>：在类似于 Apache Hive 或 Apache Iceberg 这样的外部数据源中，数据往往以分区的粒度进行变更。StarRocks 的物化视图可以订阅外表分区级别的数据更新，只刷新物化视图的相应分区。<br><span class="hljs-bullet">    -</span> <span class="hljs-strong">**TTL**</span>：在为物化视图设置分区策略时，您可以设置要保留的最近分区的数量，从而仅保留最新的数据。其对应的业务场景对数据时效性有较高要求，例如，分析师仅需要查询某个时间窗口内的最新数据，而无需保留所有历史数据。<br><br><span class="hljs-section">#### 物化视图查询改写</span><br><br><span class="hljs-bullet">*</span> 利用物化视图来查询改写并加速查询。（创建物化视图后，类似的查询会利用物化视图预计算的结果，会大幅提升查询速度，降低计算成本）<br><span class="hljs-bullet">*</span> 功能特点<br><span class="hljs-bullet">  -</span> <span class="hljs-strong">**强数据一致性**</span>：如果基表是 StarRocks 内表，StarRocks 可以保证通过物化视图查询改写获得的结果与直接查询基表的结果一致。<br><span class="hljs-bullet">  -</span> <span class="hljs-strong">**Staleness rewrite**</span>：StarRocks 支持 Staleness rewrite，即允许容忍一定程度的数据过期，以应对数据变更频繁的情况。<br><span class="hljs-bullet">  -</span> <span class="hljs-strong">**多表 Join**</span>：StarRocks 的异步物化视图支持各种类型的 Join，包括一些复杂的 Join 场景，如 View Delta Join 和 Join 派生改写，可用于加速涉及大宽表的查询场景。<br><span class="hljs-bullet">  -</span> <span class="hljs-strong">**聚合改写**</span>：StarRocks 可以改写带有聚合操作的查询，以提高报表性能。<br><span class="hljs-bullet">  -</span> <span class="hljs-strong">**嵌套物化视图**</span>：StarRocks 支持基于嵌套物化视图改写复杂查询，扩展了可改写的查询范围。<br><span class="hljs-bullet">  -</span> <span class="hljs-strong">**Union 改写**</span>：您可以将 Union 改写特性与物化视图分区的生存时间（TTL）相结合，实现冷热数据的分离，允许您从物化视图查询热数据，从基表查询历史数据。<br><span class="hljs-bullet">  -</span> <span class="hljs-strong">**基于视图构建物化视图**</span>：您可以在基于视图建模的情景下加速查询。<br><span class="hljs-bullet">  -</span> <span class="hljs-strong">**基于 External Catalog 构建物化视图**</span>：您可以通过该特性加速数据湖中的查询。<br><span class="hljs-bullet">  -</span> <span class="hljs-strong">**复杂表达式改写**</span>：支持在表达式中调用函数和算术运算，满足复杂分析和计算需求。<br><br><span class="hljs-section">##### join改写</span><br><br><span class="hljs-bullet">*</span> StarRocks 支持改写具有各种类型 Join 的查询<br><span class="hljs-bullet">*</span> StarRocks 异步物化视图的多表聚合查询改写支持所有聚合函数，包括 bitmap<span class="hljs-emphasis">_union、hll_</span>union 和 percentile<span class="hljs-emphasis">_union 等</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">##### 聚合改写</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">* 聚合上卷改写</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">  - StarRocks 支持通过聚合上卷改写查询，即 StarRocks 可以使用通过 GROUP BY a,b 子句创建的异步物化视图改写带有 GROUP BY a 子句的聚合查询</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">* 聚合下推</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">  - 从 v3.3.0 版本开始，StarRocks 支持物化视图查询改写的聚合下推功能</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">* count distinct改写</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">  - StarRocks 支持将 COUNT DISTINCT 计算改写为 BITMAP 类型的计算，从而使用物化视图实现高性能、精确的去重。</span><br><span class="hljs-emphasis"></span><br><span class="hljs-emphasis">  - ````sqlite</span><br><span class="hljs-emphasis">    CREATE MATERIALIZED VIEW distinct_</span>mv<br><span class="hljs-code">    DISTRIBUTED BY hash(lo_orderkey)</span><br><span class="hljs-code">    AS</span><br><span class="hljs-code">    SELECT lo_orderkey, bitmap_union(to_bitmap(lo_custkey)) AS distinct_customer</span><br><span class="hljs-code">    FROM lineorder</span><br><span class="hljs-code">    GROUP BY lo_orderkey;</span><br><span class="hljs-code">    </span><br><span class="hljs-code">    -- 上面的物化视图可以改写下面的查询</span><br><span class="hljs-code">    SELECT lo_orderkey, count(distinct lo_custkey) </span><br><span class="hljs-code">    FROM lineorder </span><br><span class="hljs-code">    GROUP BY lo_orderkey;</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li></li>
</ul>
</li>
</ul>
<h5 id="嵌套物化视图改写"><a href="#嵌套物化视图改写" class="headerlink" title="嵌套物化视图改写"></a>嵌套物化视图改写</h5><ul>
<li>StarRocks 支持使用嵌套物化视图改写查询。</li>
</ul>
<h5 id="Union改写"><a href="#Union改写" class="headerlink" title="Union改写"></a>Union改写</h5><ul>
<li><p>谓词Union改写：当物化视图的谓词范围是查询的谓词范围的子集时，可以使用 UNION 操作改写查询。</p>
<ul>
<li><pre><code class="sqlite">CREATE MATERIALIZED VIEW agg_mv4
DISTRIBUTED BY hash(lo_orderkey)
AS
SELECT 
  lo_orderkey, 
  sum(lo_revenue) AS total_revenue, 
  max(lo_discount) AS max_discount 
FROM lineorder
WHERE lo_orderkey &lt; 300000000
GROUP BY lo_orderkey;

-- 该物化视图可以改写以下查询
select 
  lo_orderkey, 
  sum(lo_revenue) AS total_revenue, 
  max(lo_discount) AS max_discount 
FROM lineorder
GROUP BY lo_orderkey;
<figure class="highlight gherkin"><table><tr><td class="code"><pre><code class="hljs gherkin"><br><span class="hljs-symbol">*</span> 分区Union改写：假设基于分区表创建了一个分区物化视图。当查询扫描的分区范围是物化视图最新分区范围的超集时，查询可被 UNION 改写。<br><br><span class="hljs-comment">##### 基于视图的物化视图查询改写</span><br><br>- 自 v3.1.0 起，StarRocks 支持基于视图创建物化视图。如果基于视图的查询为 SPJG 类型，StarRocks 将会内联展开查询，然后进行改写。默认情况下，对视图的查询会自动展开为对视图的基表的查询，然后进行透明匹配和改写。<br><br><span class="hljs-comment">##### 基于External Catalog构建物化视图</span><br><br><span class="hljs-symbol">*</span> StarRocks 支持基于 Hive Catalog、Hudi Catalog、Iceberg Catalog 和 Paimon Catalog 的外部数据源上构建异步物化视图，并支持透明地改写查询。基于 External Catalog 的物化视图支持大多数查询改写功能<br><br><span class="hljs-comment">##### 验证查询是否改写</span><br><br><span class="hljs-symbol">*</span> 可以使用 EXPLAIN 语句查看对应 Query Plan。如果其中 OlapScanNode 项目下的 TABLE 为对应异步物化视图名称，则表示该查询已基于异步物化视图改写。<br><br><span class="hljs-comment">#### 使用物化视图加速数据湖查询</span><br><br><span class="hljs-symbol">*</span> StarRocks 提供了开箱即用的数据湖查询功能，非常适用于对湖中的数据进行探查式查询分析。在大多数情况下，[Data Cache](https://docs.starrocks.io/zh/docs/data_source/data_cache/) 可以提供 Block 级文件缓存，避免由远程存储抖动和大量I/O操作引起的性能下降。<br><br>  然而，当涉及到使用湖中数据构建复杂和高效的报表，或进一步加速这些查询时，您可能仍然会遇到性能挑战。通过使用异步物化视图，您可以为数据湖中的报表和应用实现更高的并发，以及更好的性能。<br><br><span class="hljs-comment">##### 概述</span><br><br><span class="hljs-symbol">*</span> 数据湖报表的透明加速<br><span class="hljs-symbol">*</span> 实时数据与离线数据关联的增量计算<br><span class="hljs-symbol">*</span> 指标层的快速搭建<br><br>|<span class="hljs-string">                    </span>|<span class="hljs-string"> Data Cache                                                   </span>|<span class="hljs-string"> 物化视图                                                   </span>|<span class="hljs-string"> 本地表                                   </span>|<br>|<span class="hljs-string"> ------------------ </span>|<span class="hljs-string"> ------------------------------------------------------------ </span>|<span class="hljs-string"> ---------------------------------------------------------- </span>|<span class="hljs-string"> ---------------------------------------- </span>|<br>|<span class="hljs-string"> **数据导入和更新** </span>|<span class="hljs-string"> 查询会自动触发数据缓存                                       </span>|<span class="hljs-string"> 自动触发刷新任务                                           </span>|<span class="hljs-string"> 支持各种导入方法，但需要手动维护导入任务 </span>|<br>|<span class="hljs-string"> **数据缓存粒度**   </span>|<span class="hljs-string"> 支持 Block 级数据缓存遵循 LRU 缓存淘汰机制不缓存计算结果     </span>|<span class="hljs-string"> 存储预计算的查询结果                                       </span>|<span class="hljs-string"> 基于表定义存储数据                       </span>|<br>|<span class="hljs-string"> **查询性能**       </span>|<span class="hljs-string"> Data Cache ≤ 物化视图 = 本地表                               </span>|<span class="hljs-string">                                                            </span>|<span class="hljs-string">                                          </span>|<br>|<span class="hljs-string"> **查询语句**       </span>|<span class="hljs-string"> 无需修改针对湖数据的查询语句一旦查询命中缓存，就会进行现场计算。 </span>|<span class="hljs-string"> 无需修改针对湖数据的查询语句利用查询改写重用预先计算的结果 </span>|<span class="hljs-string"> 需要修改查询语句以查询本地表             </span>|<br><br><span class="hljs-symbol">*</span> 与直接查询数据湖数据或将数据导入到本地表中相比，物化视图有几个独特的优势：<br>  - <span class="hljs-symbol">*</span><span class="hljs-symbol">*</span>本地存储加速<span class="hljs-symbol">*</span><span class="hljs-symbol">*</span>：物化视图可以利用 StarRocks 的本地存储加速优势，如索引、分区分桶和 Colocate Group，从而相较直接从数据湖查询数据具有更好的查询性能。<br>  - <span class="hljs-symbol">*</span><span class="hljs-symbol">*</span>无需维护加载任务<span class="hljs-symbol">*</span><span class="hljs-symbol">*</span>：物化视图通过自动刷新任务透明地更新数据，无需维护导入任务。此外，基于 Hive、Iceberg 和 Paimon Catalog 的物化视图可以检测数据更改并在分区级别执行增量刷新。<br>  - <span class="hljs-symbol">*</span><span class="hljs-symbol">*</span>智能查询改写<span class="hljs-symbol">*</span><span class="hljs-symbol">*</span>：查询可以被透明改写至物化视图，无需修改应用使用的查询语句即可加速查询。<br><br><span class="hljs-comment">#### 创建分区物化视图</span><br><br><span class="hljs-symbol">*</span> SR的异步物化视图支持多种分区策略和函数，方便实现<br><br>  - <span class="hljs-symbol">*</span><span class="hljs-symbol">*</span>增量构建<span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><br><br>    在创建分区物化视图时，您可以设置分区刷新任务分批执行，以避免所有分区并行刷新导致过多资源消耗。<br><br>  - <span class="hljs-symbol">*</span><span class="hljs-symbol">*</span>增量刷新<span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><br><br>    您可以将刷新任务设置为基表分区有数据更新时，仅更新物化视图的相应分区。分区级别的刷新可以显著减少刷新整个物化视图所导致的资源浪费。<br><br>  - <span class="hljs-symbol">*</span><span class="hljs-symbol">*</span>局部物化<span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><br><br>    您可以为物化视图分区设置 TTL，从而实现数据的部分物化。<br><br>  - <span class="hljs-symbol">*</span><span class="hljs-symbol">*</span>透明查询改写<span class="hljs-symbol">*</span><span class="hljs-symbol">*</span><br><br>    查询可以仅基于最新的物化视图分区进行透明改写。过期的分区不会参与查询计划，相应查询将在基表上直接执行，从而确保数据的一致性。<br>    <br>  - ````sqlite<br>    CREATE MATERIALIZED VIEW <span class="hljs-variable">&lt;name&gt;</span><br>    REFRESH ASYNC<br>    PARTITION BY <br>        [<br>            <span class="hljs-variable">&lt;base_table_column&gt;</span> |<span class="hljs-string"> </span><br><span class="hljs-string">            date_trunc(&lt;granularity&gt;, &lt;base_table_column&gt;) </span>|<br>            time_slice(<span class="hljs-variable">&lt;base_table_column&gt;</span>, <span class="hljs-variable">&lt;granularity&gt;</span>) |<span class="hljs-string"> </span><br><span class="hljs-string">            date_slice(&lt;base_table_column&gt;, &lt;granularity&gt;)</span><br><span class="hljs-string">        ]</span><br><span class="hljs-string">    AS &lt;query&gt;</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li></li>
</ul>
</li>
<li><p>使用限制</p>
<ul>
<li>分区物化视图只能在分区基表（通常是事实表）上创建。您需要通过映射基表和物化视图之间的分区关系建立两者之间的协同关系。</li>
</ul>
</li>
</ul>
<h4 id="语法使用"><a href="#语法使用" class="headerlink" title="语法使用"></a>语法使用</h4><ul>
<li><p>常见使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">-- 创建语句(CREATE)<br>CREATE MATERIALIZED VIEW [IF NOT EXISTS] [database.]&lt;mv_name&gt;<br>[COMMENT &quot;&quot;]<br>-- distribution_desc<br>[DISTRIBUTED BY HASH(&lt;bucket_key&gt;[,&lt;bucket_key2&gt; ...]) [BUCKETS &lt;bucket_number&gt;]]<br>-- refresh_desc<br>[REFRESH <br>-- refresh_moment<br>    [IMMEDIATE | DEFERRED]<br>-- refresh_scheme<br>    [ASYNC | ASYNC [START (&lt;start_time&gt;)] EVERY (INTERVAL &lt;refresh_interval&gt;) | MANUAL]<br>]<br>-- partition_expression<br>[PARTITION BY <br>    &#123;&lt;date_column&gt; | date_trunc(fmt, &lt;date_column&gt;)&#125;<br>]<br>-- order_by_expression<br>[ORDER BY (&lt;sort_key&gt;)]<br>[PROPERTIES (&quot;key&quot;=&quot;value&quot;, ...)]<br>AS <br>&lt;query_statement&gt;<br><br>-- 修改语句(ALTER)<br>-- 修改物化视图名称<br>ALTER MATERIALIZED VIEW lo_mv1 RENAME lo_mv1_new_name;<br>-- 修改物化视图刷新间隔<br>ALTER MATERIALIZED VIEW lo_mv2 REFRESH ASYNC EVERY(INTERVAL 1 DAY);<br>-- 修改物化视图属性<br>-- 修改 mv1 的 query_timeout 为 40000 秒。<br>ALTER MATERIALIZED VIEW mv1 SET (&quot;session.query_timeout&quot; = &quot;40000&quot;);<br>-- 修改 mv1 的 mv_rewrite_staleness_second 为 600 秒。<br>ALTER MATERIALIZED VIEW mv1 SET (&quot;mv_rewrite_staleness_second&quot; = &quot;600&quot;);<br>-- 修改物化视图状态为 Active<br>ALTER MATERIALIZED VIEW order_mv ACTIVE;<br>-- 原子替换物化视图 order_mv 和 order_mv1<br>ALTER MATERIALIZED VIEW order_mv SWAP WITH order_mv1;<br><br>-- 删除语句(DROP)<br>DROP MATERIALIZED VIEW [IF EXISTS] [database.]mv_name<br><br>-- 展示所有或指定异步物化视图信息(SHOW)<br>SHOW MATERIALIZED VIEWS<br>[FROM db_name]<br>[<br>WHERE NAME &#123; = &quot;mv_name&quot; | LIKE &quot;mv_name_matcher&quot;&#125;<br>]<br></code></pre></td></tr></table></figure></li>
</ul>
<h4 id="异步物化视图故障排除"><a href="#异步物化视图故障排除" class="headerlink" title="异步物化视图故障排除"></a>异步物化视图故障排除</h4><h5 id="检查异步物化视图"><a href="#检查异步物化视图" class="headerlink" title="检查异步物化视图"></a>检查异步物化视图</h5><ul>
<li><pre><code class="sqlite">-- 检查异步物化视图工作状态
SHOW MATERIALIZED VIEWS like &#39;dwd_msg_indu_ref_hf&#39;
-- 查看异步物化视图的刷新历史
-- task_name 通过异步物化视图的工作状态结果查看
SELECT * FROM information_schema.task_runs WHERE task_name =&#39;mv-242138&#39;
-- 监控异步物化视图的资源消耗情况
show PROC 
-- 验证查询是否被异步物化视图改写
-- 通过explain查看执行计划，检查查询是否可以被异步物化视图重写
EXPLAIN LOGICAL select * from dwd_msg_indu_ref_hf;
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><br>* <br><br>##### 诊断并解决故障<br><br>* ````sqlite<br>  <span class="hljs-comment">-- 创建物化视图失败</span><br>  <span class="hljs-comment">-- 1.检查是否误用了创建同步物化视图的 SQL 语句。</span><br>  <span class="hljs-comment">-- 2.检查是否指定了正确的 Partition By 列。</span><br>  <span class="hljs-comment">-- 3.检查您是否具有创建物化视图所需的权限。</span><br>  <span class="hljs-comment">-- 物化视图刷新失败</span><br>  <span class="hljs-comment">-- 1.检查是否采用了不合适的刷新策略。</span><br>  <span class="hljs-comment">-- 2.检查刷新任务是否超出了内存限制。</span><br>  <span class="hljs-comment">-- 3.检查刷新任务是否超时。</span><br>  <span class="hljs-comment">-- 物化视图不可用</span><br>  <span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">MATERIALIZED</span> <span class="hljs-keyword">VIEW</span> mv1 ACTIVE;<br>  <span class="hljs-comment">-- 物化视图刷新任务占用过多资源</span><br>  <span class="hljs-comment">-- 1.检查创建的物化视图是否过大</span><br>  <span class="hljs-comment">-- 2.检查刷新间隔是否过于频繁</span><br>  <span class="hljs-comment">-- 3.检查物化视图是否已分区</span><br>  <span class="hljs-comment">-- 物化视图无法改写查询</span><br>  <span class="hljs-comment">-- 1.检查物化视图和查询是否匹配</span><br>  <span class="hljs-comment">-- 2.检查物化视图的状态是否为 Active</span><br>  <span class="hljs-comment">-- 3.检查物化视图是否满足数据一致性要求</span><br>  <span class="hljs-comment">-- 4.检查物化视图的查询语句是否缺少输出列</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<h3 id="Colocate-join"><a href="#Colocate-join" class="headerlink" title="Colocate join"></a>Colocate join</h3><ul>
<li>Colocate Join 功能是分布式系统实现 Join 数据分布的策略之一，能够减少数据多节点分布时 Join 操作引起的数据移动和网络传输，从而提高查询性能。</li>
</ul>
<h3 id="Lateral-join-实现列转行"><a href="#Lateral-join-实现列转行" class="headerlink" title="Lateral join 实现列转行"></a>Lateral join 实现列转行</h3><ul>
<li><p>行列转换功能,Lateral Join 功能能够将每行数据和内部的子查询或者 Table Function 关联。通过 Lateral Join 与 Unnest 功能配合，您可以实现一行转多行的功能。 <a href="https://docs.starrocks.io/zh/docs/3.0/sql-reference/sql-functions/array-functions/unnest/">unnest函数详情</a>。</p>
</li>
<li><p>unnest函数注意事项</p>
<ul>
<li>UNNEST 必须与 lateral join 一起使用，但是 lateral join 关键字可以在查询中省略。</li>
<li><strong>支持输入多个数组，数组的长度和类型可以不同。</strong></li>
<li>多个 array 的元素类型和长度（元素个数）可以不同。<strong>对于长度不同的情况，以最长数组的长度为基准，长度小于这个长度的数组使用 NULL 进行元素补充</strong></li>
<li><strong>如果输入的数组为 NULL 或 空，则计算时跳过。</strong></li>
<li>如果数组中的某个元素为 NULL，该元素对应的位置返回 NULL。</li>
</ul>
</li>
<li><pre><code class="sqlite">-- 完整 SQL 语句。
SELECT student, score, t.unnest
FROM tests
CROSS JOIN LATERAL UNNEST(scores) AS t;

-- 简化 SQL 语句。您可以使用 Unnest 关键字省略 Lateral Join 关键字。
SELECT student, score, t.unnest
FROM tests, UNNEST(scores) AS t;
-- 示例一：UNNEST 接收一个参数
-- 创建表 student_score，其中 scores 为 ARRAY 类型的列。
CREATE TABLE student_score
(
    `id` bigint(20) NULL COMMENT &quot;&quot;,
    `scores` ARRAY&lt;int&gt; NULL COMMENT &quot;&quot;
)
DUPLICATE KEY (id)
DISTRIBUTED BY HASH(`id`);

-- 向表插入数据。
INSERT INTO student_score VALUES
(1, [80,85,87]),
(2, [77, null, 89]),
(3, null),
(4, []),
(5, [90,92]);

--查询表数据。
SELECT * FROM student_score ORDER BY id;
+------+--------------+
| id   | scores       |
+------+--------------+
|    1 | [80,85,87]   |
|    2 | [77,null,89] |
|    3 | NULL         |
|    4 | []           |
|    5 | [90,92]      |
+------+--------------+

-- 将 scores 列中的数组元素展开成多行。
SELECT id, scores, unnest FROM student_score, unnest(scores) AS unnest;
+------+--------------+--------+
| id   | scores       | unnest |
+------+--------------+--------+
|    1 | [80,85,87]   |     80 |
|    1 | [80,85,87]   |     85 |
|    1 | [80,85,87]   |     87 |
|    2 | [77,null,89] |     77 |
|    2 | [77,null,89] |   NULL |
|    2 | [77,null,89] |     89 |
|    5 | [90,92]      |     90 |
|    5 | [90,92]      |     92 |
+------+--------------+--------+
-- 示例二：UNNEST 接收多个参数
-- 创建表。
CREATE TABLE example_table (
id varchar(65533) NULL COMMENT &quot;&quot;,
type varchar(65533) NULL COMMENT &quot;&quot;,
scores ARRAY&lt;int&gt; NULL COMMENT &quot;&quot;
) ENGINE=OLAP
DUPLICATE KEY(id)
COMMENT &quot;OLAP&quot;
DISTRIBUTED BY HASH(id)
PROPERTIES (
&quot;replication_num&quot; = &quot;3&quot;);

-- 向表插入数据。
INSERT INTO example_table VALUES
(&quot;1&quot;, &quot;typeA;typeB&quot;, [80,85,88]),
(&quot;2&quot;, &quot;typeA;typeB;typeC&quot;, [87,90,95]);

-- 查询表中数据。
SELECT * FROM example_table;
+------+-------------------+------------+
| id   | type              | scores     |
+------+-------------------+------------+
| 1    | typeA;typeB       | [80,85,88] |
| 2    | typeA;typeB;typeC | [87,90,95] |
+------+-------------------+------------+

-- 使用 UNNEST 将 type 和 scores 这两列中的元素展开为多行。
SELECT id, unnest.type, unnest.scores
FROM example_table, unnest(split(type, &quot;;&quot;), scores) AS unnest(type,scores);
+------+-------+--------+
| id   | type  | scores |
+------+-------+--------+
| 1    | typeA |     80 |
| 1    | typeB |     85 |
| 1    | NULL  |     88 |
| 2    | typeA |     87 |
| 2    | typeB |     90 |
| 2    | typeC |     95 |
+------+-------+--------+
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><br>* ````sqlite<br>  <span class="hljs-comment">-- 如果输入的数组为 NULL 或 空，则计算时跳过。</span><br>  <span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> lateral_test (<br>      `v1` <span class="hljs-type">bigint</span>(<span class="hljs-number">20</span>) <span class="hljs-keyword">NULL</span> <span class="hljs-keyword">COMMENT</span> &quot;&quot;,<br>      `v2` <span class="hljs-keyword">ARRAY</span>&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-keyword">NULL</span> <span class="hljs-keyword">COMMENT</span> &quot;&quot;<br>  ) <br>  DUPLICATE KEY(v1)<br>  DISTRIBUTED <span class="hljs-keyword">BY</span> HASH(`v1`)<br>  PROPERTIES (<br>      &quot;replication_num&quot; = &quot;3&quot;,<br>      &quot;storage_format&quot; = &quot;DEFAULT&quot;<br>  );<br>  <br>  <span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> lateral_test <span class="hljs-keyword">VALUES</span> (<span class="hljs-number">1</span>, [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]), (<span class="hljs-number">2</span>, [<span class="hljs-number">1</span>, <span class="hljs-keyword">null</span>, <span class="hljs-number">3</span>]), (<span class="hljs-number">3</span>, <span class="hljs-keyword">null</span>);<br>  <br>  mysql&gt; <span class="hljs-keyword">select</span> v1, v2, unnest <span class="hljs-keyword">from</span> lateral_test, unnest(v2) <span class="hljs-keyword">as</span> unnest;<br>  <br>  +<span class="hljs-comment">------+------------+--------+</span><br>  | v1   | v2         | unnest |<br>  +<span class="hljs-comment">------+------------+--------+</span><br>  |    <span class="hljs-number">1</span> | [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]      |      <span class="hljs-number">1</span> |<br>  |    <span class="hljs-number">1</span> | [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]      |      <span class="hljs-number">2</span> |<br>  |    <span class="hljs-number">2</span> | [<span class="hljs-number">1</span>,<span class="hljs-keyword">null</span>,<span class="hljs-number">3</span>] |      <span class="hljs-number">1</span> |<br>  |    <span class="hljs-number">2</span> | [<span class="hljs-number">1</span>,<span class="hljs-keyword">null</span>,<span class="hljs-number">3</span>] |   <span class="hljs-keyword">NULL</span> |<br>  |    <span class="hljs-number">2</span> | [<span class="hljs-number">1</span>,<span class="hljs-keyword">null</span>,<span class="hljs-number">3</span>] |      <span class="hljs-number">3</span> |<br>  +<span class="hljs-comment">------+------------+--------+</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li></li>
</ul>
<h3 id="Query-Cache"><a href="#Query-Cache" class="headerlink" title="Query Cache"></a>Query Cache</h3><ul>
<li><p>开启 Query Cache 后，<strong>每次处理聚合查询时，StarRocks 都会将本地聚合的中间结果缓存于内存中</strong>。后续收到相同或类似的聚合查询时，StarRocks 就能够直接从 Query Cache 获取匹配的聚合结果，而无需从磁盘读取数据并进行计算，大大节省查询的时间和资源成本</p>
</li>
<li><p>应用场景</p>
<ul>
<li>查询多为宽表模型下的单表聚合查询或星型模型下简单多表 JOIN 的聚合查询。</li>
<li>聚合查询以非 GROUP BY 聚合和低基数 GROUP BY 聚合为主。</li>
<li>查询的数据以按时间分区追加的形式导入，并且在不同时间分区上的访问表现出冷热性。</li>
</ul>
</li>
<li></li>
</ul>
<h3 id="数据去重"><a href="#数据去重" class="headerlink" title="数据去重"></a>数据去重</h3><h4 id="使用Bitmap实现精确去重"><a href="#使用Bitmap实现精确去重" class="headerlink" title="使用Bitmap实现精确去重"></a>使用Bitmap实现精确去重</h4><ul>
<li><p>bitmap就是利用所谓 BitMap 就是用一个 bit 位来标记某个元素对应的 value，而 key 即是这个元素。由于采用bit为单位来存储数据，因此在可以大大的节省存储空间。</p>
<ul>
<li>Bitmap 去重能够准确计算一个数据集中不重复元素的数量，相比传统的 Count Distinct，可以节省存储空间、加速计算。<strong>例如，给定一个数组 A，其取值范围为 [0, n)，可采用 (n+7)&#x2F;8 的字节长度的 bitmap 对该数组去重。即将所有 bit 初始化为 0，然后以数组 A 中元素的取值作为 bit 的下标，并将 bit 置为 1，那么 bitmap 中 1 的个数即为数组 A 中不同元素 (Count Distinct) 的数量。</strong></li>
</ul>
</li>
<li><p>Bitmap index 和 Bitmap 去重二者虽然都使用 Bitmap 技术，但引入原因和解决的问题完全不同。前者用于低基数的枚举型列的等值条件过滤，后者则用于计算一组数据行的指标列的不重复元素的个数。</p>
</li>
<li><p>从 StarRocks 2.3 版本开始，所有数据模型表的指标列均支持设置为 BITMAP 类型，但是所有数据模型表的不支持<a href="https://docs.starrocks.io/zh/docs/3.0/table_design/Sort_key/">排序键</a>为 BITMAP 类型。</p>
</li>
<li><p>建表时，指定指标列类型为 BITMAP，使用 <a href="https://docs.starrocks.io/zh/docs/3.0/sql-reference/sql-functions/bitmap-functions/bitmap_union/">BITMAP_UNION</a> 函数进行聚合。</p>
</li>
<li><p>StarRocks 的 bitmap 去重是基于 Roaring Bitmap 实现的，roaring bitmap 只能对 TINYINT，SMALLINT，INT 和 BIGINT 类型的数据去重。如想要使用 Roaring Bitmap 对其他类型的数据去重，则需要构建全局字典。</p>
</li>
<li><p>bitmap去重案例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">-- <br>CREATE TABLE `page_uv` (<br>  `page_id` INT NOT NULL COMMENT &#x27;页面id&#x27;,<br>  `visit_date` datetime NOT NULL COMMENT &#x27;访问时间&#x27;,<br>  `visit_users` BITMAP BITMAP_UNION NOT NULL COMMENT &#x27;访问用户id&#x27;<br>) ENGINE=OLAP<br>AGGREGATE KEY(`page_id`, `visit_date`)<br>DISTRIBUTED BY HASH(`page_id`)<br>PROPERTIES (<br>  &quot;replication_num&quot; = &quot;3&quot;,<br>  &quot;storage_format&quot; = &quot;DEFAULT&quot;<br>);<br><br>INSERT INTO page_uv VALUES<br>(1, &#x27;2020-06-23 01:30:30&#x27;, to_bitmap(13)),<br>(1, &#x27;2020-06-23 01:30:30&#x27;, to_bitmap(23)),<br>(1, &#x27;2020-06-23 01:30:30&#x27;, to_bitmap(33)),<br>(1, &#x27;2020-06-23 02:30:30&#x27;, to_bitmap(13)),<br>(2, &#x27;2020-06-23 01:30:30&#x27;, to_bitmap(23));<br><br>在 page_id = 1， visit_date = &#x27;2020-06-23 01:30:30&#x27; 数据行，visit_users 字段包含 3 个 bitmap 元素（13，23，33）；<br>在 page_id = 1， visit_date = &#x27;2020-06-23 02:30:30&#x27; 的数据行，visit_users 字段包含 1 个 bitmap 元素（13）；<br>在 page_id = 2， visit_date = &#x27;2020-06-23 01:30:30&#x27; 的数据行，visit_users 字段包含 1 个 bitmap 元素（23）。<br><br>select page_id, count(distinct visit_users) from page_uv group by page_id;<br><br>+-----------+------------------------------+<br>|  page_id  | count(DISTINCT `visit_users`) |<br>+-----------+------------------------------+<br>|         1 |                            3 |<br>+-----------+------------------------------+<br>|         2 |                            1 |<br>+-----------+------------------------------+<br>2 row in set (0.00 sec)<br>-- bitmap_union(value)使用<br>-- value支持的类型为bitmap<br>select page_id, bitmap_count(bitmap_union(user_id))<br>from table<br>group by page_id;<br><br>-- 等价于下面的sql<br>select page_id, count(distinct user_id)<br>from table<br>group by page_id;<br></code></pre></td></tr></table></figure>
</li>
<li><p>目前bitmap类型不支持除整数类型之外的类型，如果想对string等类型作为bitmap的输入，则需要构建全局字典</p>
</li>
</ul>
<h4 id="使用HyperLogLog实现近似去重"><a href="#使用HyperLogLog实现近似去重" class="headerlink" title="使用HyperLogLog实现近似去重"></a>使用HyperLogLog实现近似去重</h4><ul>
<li>HLL 是一种近似去重算法，在部分对去重精度要求不高的场景下，您可以选择使用 HLL 算法减轻数据去重分析的计算压力。根据数据集大小以及所采用的哈希函数的类型，HLL 算法的误差可控制在 1% 至 10% 左右。</li>
</ul>
<h4 id="使用-AUTO-INCREMENT-列构建全局字典以加速精确去重计算和-Join"><a href="#使用-AUTO-INCREMENT-列构建全局字典以加速精确去重计算和-Join" class="headerlink" title="使用 AUTO INCREMENT 列构建全局字典以加速精确去重计算和 Join"></a>使用 AUTO INCREMENT 列构建全局字典以加速精确去重计算和 Join</h4><ul>
<li>阶段一： 创建全局字典并构建 STRING 值和 INTEGER 值之间的映射关系。字典中 key 列为 STRING 类型，value 列为 INTEGER 类型且为自增列。每次导入数据时候，系统都会自动为每个 STRING 值生成一个表内全局唯一的 ID，如此就建立了 STRING 值和 INTEGER 值之间的映射关系。</li>
<li>阶段二：将订单数据和全局字典的映射关系导入至目标表。</li>
<li>阶段三：后续查询分析时基于目标表的 INTEGER 列来计算精确去重或 Join，可以显著提高性能。</li>
<li>阶段四：为了进一步优化性能，您还可以在 INTEGER 列上使用 bitmap 函数来进一步加速计算精确去重。</li>
</ul>
<h3 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h3><ul>
<li>关于StarRocks的字符串类型的一些问题：<br>1.SR字符串的单位是字节。varchar(255)，表示只能存255字节的数据。<br>2.但是对于中文字符来说，一个中文字符占了三个字节。所以同样varchar(255)的情况下，mysql会比SR多存很多个中文字符。<br>3.创建物化视图时，当SR varcha(255)存不下的时候，该条数据会被舍弃。<br>4.还有一个就是SR的string类型，最大字节数是65533，即varchar(65533)。如果长度超长，用中台同步数据不会丢失，但是该字段为空。<br>5.自 StarRocks 2.1 版本开始，varchar(M) M的取值范围为 [1, 1048576]。M是字节数<br>所以后续可能要特别注意中文字符串丢数据或者缺失数据的情况。</li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>alibaba_big_data_road</title>
    <url>/2024/02/27/alibaba-big-data-road/</url>
    <content><![CDATA[<h1 id="《大数据之路》"><a href="#《大数据之路》" class="headerlink" title="《大数据之路》"></a>《大数据之路》</h1><ul>
<li>阿里巴巴出品的经典书籍，这里是一些笔记。推荐阅读原版书籍</li>
</ul>
<h2 id="数据模型篇"><a href="#数据模型篇" class="headerlink" title="数据模型篇"></a>数据模型篇</h2><h3 id="第8章：大数据领域建模综述"><a href="#第8章：大数据领域建模综述" class="headerlink" title="第8章：大数据领域建模综述"></a>第8章：大数据领域建模综述</h3><ul>
<li>数据仓库是一个<strong>面向主题的、集成的、 非易失的且随时间变化的数据集合</strong>，用来支持管理人员的<strong>决策</strong>。</li>
</ul>
<ul>
<li>好的数据建模会从以下几个方面带来提升：<ul>
<li>性能：良好的数据模型可以快速查询数据，减少I&#x2F;O</li>
<li>成本：减少非必要的数据冗余，计算结果的复用，降低大数据系统中的存储和计算成本</li>
<li>效率：提升使用数据的体验，提高使用数据的效率</li>
<li>质量：良好的数据模型可以改善数据口径的不一致，减少数据计算错误的可能性</li>
</ul>
</li>
</ul>
<h4 id="OLAP和OLTP系统"><a href="#OLAP和OLTP系统" class="headerlink" title="OLAP和OLTP系统"></a>OLAP和OLTP系统</h4><ul>
<li>埃德加·弗兰克·科德是“<a href="https://baike.baidu.com/item/%E5%85%B3%E7%B3%BB%E6%A8%A1%E5%9E%8B/3189329?fromModule=lemma_inlink">关系模型</a>”的发明者，关系型数据数据的鼻祖，之后类似于oracle,mysql相关的产品喷涌而出。OLTP系统（OnLine Transaction Processsing 联机事务处理）主要操作是数据随机读写，一般采用3NF的实体关系模型存储数据，从而在事务处理中解决数据的冗余和一致性问题。</li>
<li>OLAP（OnLine Analytical Processing联机分析处理）主要面向批量读写，关注在大数据下的复杂查询问题</li>
</ul>
<h4 id="维度模型"><a href="#维度模型" class="headerlink" title="维度模型"></a>维度模型</h4><ul>
<li><p>Kimball维度建模主要探究需求分析，高层模型，详细模型，和模型审查的过程。一般构建维度模型要经历三个阶段：</p>
<ul>
<li>第一阶段是高层设计时期，定义业务过程维度模型的范围，提供每种星形模型的技术和功能描述。</li>
<li>第二阶段是详细模型设计时期，对每个星形模型添加属性和度量信息</li>
<li>第三阶段是进行模型的审查，再设计和验证等工作</li>
<li>第四阶段是产生详细设计文档，提交ETL设计和开发</li>
</ul>
</li>
<li><p>维度模型是数据仓库领域的 Ralph Kimball 大师所倡导的，是目前数仓领域最流行的</p>
</li>
<li><p>维度建模从<strong>分析决策的需求</strong>出发构建模型，为分析需求服务，因此它重点关注用户如何能更快速地完成需求分析。同时具有较好的大规模复杂查询的响应性能。其典型的代表是星形模型，以及在一些特殊场景下</p>
</li>
<li><p>基本步骤</p>
<ul>
<li>选择需要进行分析决策的业务过程。业务过程可以是单个业务事件，比如交易的支付、退款等；也可以是某个事件的状态，比如当前的账户余额等；还可以是一系列相关业务事件组成的业务流程，具体需要看我们分析的是某些事件发生情况，还是当前状态，或是事件流转效率。</li>
<li>选择粒度。在事件分析中，我们要预判所有分析需要细分的程度，从而决定选择的粒度。粒度是维度的一个组合。</li>
<li>识别维表。选择好粒度之后，就需要基于此粒度设计维表，包括维度属性，用于分析时进行分组和筛选。</li>
<li>选择事实。确定分析需要衡量的指标</li>
</ul>
</li>
</ul>
<h3 id="第九章：数据整合及管理体系"><a href="#第九章：数据整合及管理体系" class="headerlink" title="第九章：数据整合及管理体系"></a>第九章：数据整合及管理体系</h3><ul>
<li>OneData 即是阿里巴巴内部进行数据整合及管理的方法体系和工具。阿里巴巴的大数据工程师在这一体系下，构建统一、规范、可共用的全域数据体系，避免数据的冗余和重复建设，规避数据烟囱和不一致性，充分发挥阿里巴巴在大数据海量、多样性方面的独特优势。借助统一化数据整合及管理的方法体系，我们构建了阿里巴巴的数据公共层，并可以帮助相似的大数据项目快速落地实现。</li>
</ul>
<h4 id="9-1概览"><a href="#9-1概览" class="headerlink" title="9.1概览"></a>9.1概览</h4><ul>
<li>体系架构<a href="https://imgse.com/i/pkIxq74"><img src="https://s21.ax1x.com/2024/07/17/pkIxq74.png" alt="pkIxq74.png"></a><ul>
<li>业务板块：根据公司的业务属性，分出几个相对独立的业务板块，业务板块之间重叠性较小</li>
<li>规范定义：设计一套数据规范命名体系，规范定义会在模型设计中体现出来</li>
<li>模型设计：以维度建模理论为基础，基于维度建模总线架构，构建一致性的维度和事实</li>
</ul>
</li>
</ul>
<h4 id="9-2-规范定义"><a href="#9-2-规范定义" class="headerlink" title="9.2 规范定义"></a>9.2 规范定义</h4><ul>
<li>规范定义指以维度建模作为理论基础 构建总线矩阵，划分和定义数据域、业务过程、维度、度量 原子指标、修饰类型、修饰词、时间周期、派生指标。<ul>
<li><a href="https://imgse.com/i/pkIziHe"><img src="https://s21.ax1x.com/2024/07/17/pkIziHe.png" alt="pkIziHe.png"></a></li>
<li>名词解释<a href="https://imgse.com/i/pkIztg0"><img src="https://s21.ax1x.com/2024/07/17/pkIztg0.png" alt="pkIztg0.png"></a></li>
</ul>
</li>
<li>指标体系<ul>
<li><a href="https://imgse.com/i/pkopPFH"><img src="https://s21.ax1x.com/2024/07/17/pkopPFH.png" alt="pkopPFH.png"></a></li>
<li>原子指标，修饰类型及修饰词，直接归属在业务过程下，其中修饰词继承修饰类型的数据域</li>
<li>派生指标可以选择多个修饰词，修饰词之间的关系为“或”或者“且”，由具体的派生指标语决定。</li>
<li>派生指标唯一归属一个原子指标 ，继承原子指标的数据域， 与修饰词的数据域无关。</li>
<li>指标命名要清晰。可以参考书本第九章143页的内容<ul>
<li><a href="https://imgse.com/i/pkopV6P"><img src="https://s21.ax1x.com/2024/07/17/pkopV6P.png" alt="pkopV6P.png"></a></li>
</ul>
</li>
<li>派生指标分为三类：事务型指标，存量型指标和复合型指标。<ul>
<li>事务型指标 是指对业务活动进行衡量的指标。例如新发商品数、重发商品数、新增注册会员数、订单支付金额，这类指标需维护原子指标及修饰词，在此基础上创建派生指标。</li>
<li>存量型指标：是指对实体对象（如商品、会员）某些状态的统计。例如商品总数、注册会员总数，这类指标需维护原子指标及修饰词，在此基础上创建派生指标，对应的时间周期 般为“历史截至当前某个时间”。</li>
<li>复合型指标：是在事务型指标和存量型指标的基础上复合而成的。例如浏览 UV－下单买家数转化率 有些需要 建新原子指标，有些则可以在事务型或存量型原子指标的基础上增加修饰词得到派生指标。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="9-3模型设计"><a href="#9-3模型设计" class="headerlink" title="9.3模型设计"></a>9.3模型设计</h4><ul>
<li>模型层次关系图<a href="https://imgse.com/i/pkoplfs"><img src="https://s21.ax1x.com/2024/07/17/pkoplfs.png" alt="pkoplfs.png"></a><ul>
<li>操作数据层（ ODS ）：把操作系统数据几乎无处理地存放在数据仓库系统中。</li>
<li>公共维度模型层（ CDM ）：存放明细事实数据、维表数据及公共指标汇总数据。 其中明细事实数据、维表数据一般根据 ODS 层数据加工生成 ：公共指标汇总数据一般根据维表数据和明细事实数据加工生成。<ul>
<li>CDM层又细分为DWD层和DWS层，分别是明细数据层和汇总数据层，<strong>采用维度模型方法作为理论基础，更多的采用一些维度退化手法，将维度退化至事实表中，减少事实表和维表的关联，提高明细数据表的易用性。（DWD）同时在汇总数据层，加强指标的维度退化，采用更多的宽表化手段构建公共指标数据层，提升公共指标的复用性，减少重复加工。(DWS)</strong><ul>
<li>组合相关和相似数据：采用明细宽表，复用关联计算，减少数据扫描。</li>
<li>公共指标统一加工：基于 OneData 体系构建命名规范、口径一致和算法统一的统计指标，为上层数据产品、应用和服务提供公共指标；建立逻辑汇总宽表。</li>
</ul>
</li>
</ul>
</li>
<li>应用数据层（ADS）：存放数据产品个性化的统计指标数据。根据CDM层和ODS层加工生成<ul>
<li>个性化指标加工：不公用性，复杂性</li>
<li>基于应用的数据组装，大宽表集市，趋势指标串</li>
</ul>
</li>
<li>数据模型架构<a href="https://imgse.com/i/pkopbB8"><img src="https://s21.ax1x.com/2024/07/17/pkopbB8.png" alt="pkopbB8.png"></a></li>
</ul>
</li>
<li>CDM层的一些简单的规范：1.高内聚，低耦合。2.数据可回滚（多次运行，结果一致）。3.命名清晰，可理解，一致性（表和字段命名清晰，相同含义字段在不同表命名要相同）</li>
</ul>
<h4 id="9-4-模型实施"><a href="#9-4-模型实施" class="headerlink" title="9.4 模型实施"></a>9.4 模型实施</h4><ul>
<li><p>Kimball维度建模实施过程：第一个阶段是高层设计时期定义业务过程维度模型的范围，提供每种星型模式的技术和功能描述；第二个阶段是详细模型设计时期，对每个星形模型添加属性和度量信息；第三个阶段是进行模型的审查、再设计和验证等工作；第四个阶段是产生详细设计文档，提交 ETL 设计和开发。</p>
<ul>
<li>高层模型<ul>
<li>高层模型设计阶段的直接产出目标是创建高层维度模型图，它是对业务过程中的维表和事实表的图形描述。确定维表创建初始属性列表，为每个事实表创建提议度量。</li>
</ul>
</li>
<li>详细模型<ul>
<li>详细的维度建模过程是为高层模型填补缺失的信息，解决设计问题，并不断测试模型能否满足业务需求，确保模型的完备性。确定每个维表的属性和每个事实表的度量，并确定信息来源的位置、定义，确定属性和度量如何填人模型的初步业务规则。</li>
</ul>
</li>
<li>模型审查，再设计和验证<ul>
<li>本阶段主要召集相关人员进行模型的审查和验证，根据审查结果对详细维度进行再设计。</li>
</ul>
</li>
<li>提交ETL设计和开发<ul>
<li>最后，完成模型详细设计文档，提交ETL 开发人员，进入ETL设计和开发阶段，由 ETL 人员完成物理模型的设计和开发。</li>
</ul>
</li>
</ul>
</li>
<li><p>阿里OneData体系下的数仓实施过程</p>
<ul>
<li><p><a href="https://imgse.com/i/pko9MDK"><img src="https://s21.ax1x.com/2024/07/17/pko9MDK.png" alt="数仓实施流程"></a></p>
</li>
<li><p>数据调研</p>
<ul>
<li><p>业务调研，业务系统要充分了解，有条件可以多使用，多感受，看数据是怎么流动的。得出业务板块，业务系统的一些信息，可以抽象整理成具体的表</p>
</li>
<li><p><a href="https://imgse.com/i/pko93Ue"><img src="https://s21.ax1x.com/2024/07/17/pko93Ue.png" alt="业务调研流程"></a></p>
</li>
<li><p><a href="https://imgse.com/i/pko9WrV"><img src="https://s21.ax1x.com/2024/07/17/pko9WrV.png" alt="业务动作"></a></p>
</li>
<li><p>需求调研：与分析师，业务运营人员沟通数据需求</p>
</li>
</ul>
</li>
<li><p>数据域划分：根据业务过程进行归纳，抽象出数据域</p>
<ul>
<li><a href="https://imgse.com/i/pkoCniQ"><img src="https://s21.ax1x.com/2024/07/17/pkoCniQ.png" alt="数据域划分"></a></li>
</ul>
</li>
<li><p>构建总线矩阵：在充分的业务调研和需求调研后，就可以构建总线矩阵了。主要是两件事：明确每个数据域下有哪些业务过程；业务过程与哪些维度相关，并定义每个数据域下的业务过程和维度。</p>
<ul>
<li><a href="https://imgse.com/i/pkoCKRs"><img src="https://s21.ax1x.com/2024/07/17/pkoCKRs.png" alt="总线矩阵"></a></li>
</ul>
</li>
<li><p>规范定义：规范定义主要定义指标体系，包括原子指标，修饰词，时间周期和派生指标</p>
</li>
<li><p>模型设计：主要就是CDM层的设计，维度和属性的规范定义。是核心内容</p>
</li>
</ul>
</li>
<li><p>OneData实施过程是一个高度迭代和动态的过程,一般采用螺旋式实施方法。在总体架构设计完成之后，开始根据数据域进行迭代式模型设计和评审。在架构设计、规范定义和模型设计等模型实施过程中，都会引入评审机制，以确保模型实施过程的正确性。</p>
</li>
</ul>
<h3 id="第十章：维度设计"><a href="#第十章：维度设计" class="headerlink" title="第十章：维度设计"></a>第十章：维度设计</h3><ul>
<li>维度是维度建模的基础和灵魂。在维度建模中，将度量称为<strong>事实</strong>，将环境描述为<strong>维度</strong>。维度是用于分析事实所需要的多种环境。</li>
<li>维度所包含的列表示维度的列，称为维度属性。维度属性是查询约束条件，分组和报表标签生成的基本来源。</li>
</ul>
<h4 id="10-1：维度的基本设计方法"><a href="#10-1：维度的基本设计方法" class="headerlink" title="10.1：维度的基本设计方法"></a>10.1：维度的基本设计方法</h4><ul>
<li>第一步：<strong>选择维度或新建维度。</strong>作为维度建模的核心，在企业级数据仓库中必须保证维度的唯一性。作为维度建模的核心，在企业级数据仓库中必须保证维度的唯一性。以淘宝商品维度为例，有且只允许有一个维度定义。</li>
<li>第二步：<strong>确定主维表。</strong>此处的主维表一般是 ODS 表，直接与业务系统同步。以淘宝商品维度为例， <em>auction</em> auctions 是与前台商品中心系统同步的商品表，此表即是主维表。</li>
<li>第三步：<strong>确定相关维表。</strong>数据仓库是业务源系统的数据整合，不同业务系统或者同一业务系统中的表之间存在关联性。根据对业务的梳理，确定哪些表和主维表存在关联关系，并选择其中的某些表用于生成维度属性。以淘宝商品维度为例，根据对业务逻辑的梳理，可以得到商品与类目、 SPU 卖家、店铺等维度存在关联关系。</li>
<li>第四步：<strong>确定维度属性。</strong>包括两步<ul>
<li>从主维表中选择维度属性或生成新的维度属性</li>
<li>从相关维表中选择维度属性或生成新的维度属性。<ul>
<li>以淘宝商品维度为例，从<strong>主维表</strong>（ s_auction_auctions ）<strong>和类目、 SPU 、卖家、店铺等相关维表</strong>中选择维度属性或生成新的维度属性。</li>
</ul>
</li>
</ul>
</li>
<li>确定维度的几个要点：<ul>
<li>尽可能生成丰富的维度属性</li>
<li>尽可能多地给出包括一些富有意义的文字性描述</li>
<li>区分数值型属性和事实</li>
<li>尽量沉淀出通用的维度属性</li>
</ul>
</li>
</ul>
<h5 id="10-1-3维度的层次结构"><a href="#10-1-3维度的层次结构" class="headerlink" title="10.1.3维度的层次结构"></a>10.1.3维度的层次结构</h5><ul>
<li>维度中的一些描述属性以层次方式或一对多的方式相互关联，可以理解为包含连续主从关系的属性层次。比如企业维度，有行业，产业等，但是行业，产业也是有细分的级别的，一级行业下的某产业。</li>
</ul>
<h5 id="10-1-4-规范化与反规范化"><a href="#10-1-4-规范化与反规范化" class="headerlink" title="10.1.4 规范化与反规范化"></a>10.1.4 规范化与反规范化</h5><ul>
<li>规范化就是传统OLTP系统中的类似的范式模型（和雪花模型类似）。如果进行反规范化处理，将一些相关维度冗余到主维度表来，可以更加方便查询，对于OLAP系统来说，这是很重要的<ul>
<li>规范化处理<a href="https://imgse.com/i/pkTwFYV"><img src="https://s21.ax1x.com/2024/07/20/pkTwFYV.png" alt="pkTwFYV.png"></a></li>
<li>反规范化处理<a href="https://imgse.com/i/pkTwkWT"><img src="https://s21.ax1x.com/2024/07/20/pkTwkWT.png" alt="pkTwkWT.png"></a></li>
</ul>
</li>
</ul>
<h5 id="10-1-5一致性维度和交叉探查"><a href="#10-1-5一致性维度和交叉探查" class="headerlink" title="10.1.5一致性维度和交叉探查"></a>10.1.5一致性维度和交叉探查</h5><ul>
<li>迭代式的构建数仓容易导致形成独立的数据集市，导致严重的不一致性。</li>
<li>构建一致性维度要根据总线架构合理划分。下面总结维度一致性的几种表现形式<ul>
<li>共享维表。比如在阿里的数仓中，商品，卖家，买家等维度有且只有一个，对这些公共维度进行交叉探查不会有问题。</li>
<li>一致性上卷，其中一个维度的维度属性是另一个维度的维度属性的子集，且两个维度的公共维度属性结构和内容相同。比如阿里的商品体系中，有商品维度和类目维度，其中类目维度的维度属性是商品维度的维度属性的子集，且具有相同维度属性和维度属性值。这样基于类目维度进行不同业务过程的交叉探查也不会存在任何问题</li>
<li>交叉属性，两个维度具有部分相同的维度属性，比如在商品维度中具有类目属性，在卖家维度中具有主营类目属性，两个维度具有相同的类目属性，则可以在相同的类目属性上进行不同业务过程的交叉探查。</li>
</ul>
</li>
</ul>
<h4 id="10-2维度设计高级主题"><a href="#10-2维度设计高级主题" class="headerlink" title="10.2维度设计高级主题"></a>10.2维度设计高级主题</h4><h5 id="10-2-1维度整合"><a href="#10-2-1维度整合" class="headerlink" title="10.2.1维度整合"></a>10.2.1维度整合</h5><ul>
<li>数据仓库是一个<strong>面向主题的，集成的，非易失的且随时间变化的数据集合</strong>，用来支持管理人员的<strong>决策</strong>。</li>
<li><strong>集成</strong>是数仓四个特性中最重要的，所以数据由面向应用的操作型环境进入数据仓库后，需要进行数据集成。<strong>将面向应用的数据转换为面向主题的数据仓库数据，本身就是一种集成</strong>。体现在以下几个方面：<ul>
<li>命名的规范统一。表名，字段名统一</li>
<li>字段类型的统一。相同和相似字段的字段类型统一</li>
<li>公共代码和代码值的统一。公共代码及标志性字段的数据类型，命名方式的统一</li>
<li>业务含义相同的表的统一。<strong>主要依据高内聚低耦合的理念，在物理实现中，将业务关系大，源系统影响差异小的表进行整合</strong>；<strong>将业务关系小，源系统影响差异大的表进行分而置之。</strong></li>
</ul>
</li>
</ul>
<h5 id="10-2-2-水平拆分"><a href="#10-2-2-水平拆分" class="headerlink" title="10.2.2 水平拆分"></a>10.2.2 水平拆分</h5><ul>
<li>维度属性通常可以按照类别或者类型进行细分。比如淘系商品表，根据业务线或行业等可以对商品进行细分，如淘宝的商品，天猫的商品，1688的商品。不同分类的商品，维度属性有相同的也有不同的；比如航旅的商品和普通的淘系商品，都有商品价格，标题，类型等维度属性，但是航旅的商品除了有这些公共属性外，还有酒店，景点，门票等自己独特的维度属性。</li>
<li>那么怎样设计维度才好呢？主要有两种解决方案<ul>
<li>方案1：将维度的不同分类实例化为不同的维度，同时在主维表中保存公共属性；</li>
<li>方案2：维护单一维度，包含所有可能的属性</li>
</ul>
</li>
<li>如何选择方案主要考虑以下三个原则：<ul>
<li>扩展性：当源系统业务逻辑变化时，能通过较少的成本快速扩展模型，保持核心模型的相对稳定性。高内聚，低耦合的思想就是其核心</li>
<li>效能：在性能和成本方面取得平衡。通过牺牲一定的存储成本，达到性能和逻辑的优化</li>
<li>易用性：模型可理解性高，访问复杂度低。用户能方便地从模型中找到对应地数据表，并能够方便地查询和分析。</li>
</ul>
</li>
</ul>
<h5 id="10-2-3-垂直拆分"><a href="#10-2-3-垂直拆分" class="headerlink" title="10.2.3 垂直拆分"></a>10.2.3 垂直拆分</h5><ul>
<li>处于扩展性，产出时间，易用性等方面地考虑，设计主从维表。主维表存放稳定，产出时间早，热度高的属性；从维表存放变化较快，产出时间晚，热度低的属性。比如阿里数仓中，设计了商品主维表和商品扩展维度，主维表1：30产出，商品扩展维表由于有冗余的产出时间较晚的商品品牌和标签信息，在每日的3：00产出。主维表先产出，对于数仓的稳定和下游应用的产出都有较大的意义。</li>
</ul>
<h5 id="10-2-4-历史归档"><a href="#10-2-4-历史归档" class="headerlink" title="10.2.4 历史归档"></a>10.2.4 历史归档</h5><ul>
<li>对于每天庞大的数据量，可以对历史数据进行归档</li>
</ul>
<h4 id="10-3-维度变化"><a href="#10-3-维度变化" class="headerlink" title="10.3 维度变化"></a>10.3 维度变化</h4><h5 id="10-3-1缓慢变化维"><a href="#10-3-1缓慢变化维" class="headerlink" title="10.3.1缓慢变化维"></a>10.3.1缓慢变化维</h5><ul>
<li>数仓的重要特点之一就是反应历史变化，所以如何处理维度的变化是维度设计的重要工作之一。维度一般会随时间发生变化。几种处理方式：<ul>
<li>重写维度值。采用此方式，不保留历史数据，始终取最新的数据</li>
<li>插入新的维度行。采用此种方式，保留历史数据</li>
<li>添加维度列。</li>
</ul>
</li>
</ul>
<h5 id="10-3-2-快照维表"><a href="#10-3-2-快照维表" class="headerlink" title="10.3.2 快照维表"></a>10.3.2 快照维表</h5><ul>
<li>在阿里数仓实践中，处理缓慢变化维的方式是采用快照方式。数仓的计算周期一般是每天一次。即每天保存一份全量快照数据。任意一天的事实均可以获取到当天的商品信息 ，也可以获取到最新的商品信息，通过限定日期，<strong>采用自然键进行关联</strong>即可。<ul>
<li>优点:简单，开发维护成本低。使用方便，易于理解</li>
<li>缺点:存储浪费太多。如果每天的变化很少，其实会浪费很多存储空间</li>
</ul>
</li>
</ul>
<h5 id="10-3-3-极限存储"><a href="#10-3-3-极限存储" class="headerlink" title="10.3.3 极限存储"></a>10.3.3 极限存储</h5><ul>
<li>其实就是采用拉链表的方式构建维度表，既有历史数据，又不会太浪费存储空间。还可以每月做历史数据的清理。缺点是可能对非数仓人员来说，模型理解上有一定门槛。</li>
<li>在实际生产中可以做一些额外处理<ul>
<li>在做极限存储前有一个全量存储表 ，<strong>全量存储表仅保留最近一段时间的全量分区数据</strong>，历史数据通过映射的方式关联到极限存储表。即用户只访问全量存储表，所以对用户来说极限存储是不可见的。</li>
<li><strong>对于部分变化频率频繁的宇段需要过滤</strong>。例如，用 户表中存在用户积分宇段，这种字段的值每天都在发生变化，如果不过滤的话，极限存储就相当于每个分区存储一份全量数据，起不到节约存储成本的效果。</li>
<li>其实就是可以做一张</li>
</ul>
</li>
</ul>
<h5 id="10-3-4-微型维度"><a href="#10-3-4-微型维度" class="headerlink" title="10.3.4 微型维度"></a>10.3.4 微型维度</h5><ul>
<li>如果维度表中的一些属性变化太频繁，可以把这些维度属性放置到新的维表中。</li>
<li>微型维度的创建是通过将一部分不稳定的属性从主维度中移出，并将它们放置到拥有自己代理键的新表中来实现的。</li>
</ul>
<h4 id="10-4-特殊维度"><a href="#10-4-特殊维度" class="headerlink" title="10.4 特殊维度"></a>10.4 特殊维度</h4><h5 id="10-4-1-递归层次"><a href="#10-4-1-递归层次" class="headerlink" title="10.4.1 递归层次"></a>10.4.1 递归层次</h5><ul>
<li>对于一些递归结构的维度，比如父子类目，省市区街道等，经常会进行上钻下钻的分析，使用递归sql成本较高。在维度模型中，可以对层次结构进行处理</li>
<li>1.层次结构扁平化。把层次打平<ul>
<li><a href="https://imgse.com/i/pkOP3Zj"><img src="https://s21.ax1x.com/2024/07/30/pkOP3Zj.png" alt="pkOP3Zj.png"></a></li>
<li>优先使用层次机构扁平化的方式吧，扩展性查但是易用性更好。</li>
</ul>
</li>
<li>2.层次桥接表<ul>
<li><a href="https://imgse.com/i/pkOPwyF"><img src="https://s21.ax1x.com/2024/07/30/pkOPwyF.png" alt="pkOPwyF.png"></a></li>
</ul>
</li>
</ul>
<h5 id="10-4-2-行为维度"><a href="#10-4-2-行为维度" class="headerlink" title="10.4.2 行为维度"></a>10.4.2 行为维度</h5><ul>
<li>对一些事实进行一些统计得到的一些维度，称为行为维度，或者事实衍生维度，按照加工方式可分为以下几种：<ul>
<li>另一个维度 的过去行为，如买家最近一次访问淘宝的时间、 买家最近 次发生淘宝交易的时间等。</li>
<li>快照事实行为维度，如买家从年初截至当前的淘宝交易金额、买家信用分值 、卖家信用分值等。</li>
<li>分组事实行为维度 ，将数值型事实转换为枚举值。如买家从年初截至当前的淘宝交易金额按照金额划分的等级 买家信用分值按照分数划分得到的信用等级等。</li>
<li>复杂逻辑事实行为维度，通过复杂算法加工或多个事实综合加工得到。如前面提到的卖家主营类目，商品热度根据访问、收藏、加入购物车、交易等情况综合计算得到。</li>
</ul>
</li>
<li>对于行为维度的处理一般有两种：1.冗余进现有的维表中。2.加工成单独的行为维表。主要参考两个原则：<ul>
<li>1.避免维度过快的增长。</li>
<li>比如对商品表进行了极限存储，如果将商品热度加入现有的商品维表中，则可能会使每日商品变更占比过高，从而导致极限存储效果较差。</li>
<li>2.避免耦合度过高。比如卖家主营类目，加工逻辑异常复杂，如果融合进现有的卖家维表中，那么过多的业务稠合会导致卖家维表刷新逻辑复杂、维护性差、产出延迟等。</li>
</ul>
</li>
</ul>
<h5 id="10-4-3-多值维度"><a href="#10-4-3-多值维度" class="headerlink" title="10.4.3 多值维度"></a>10.4.3 多值维度</h5><ul>
<li>一种情况是事实表的一条记录在某维表中有多条记录与之对应。</li>
<li>比如对于淘宝交易订单，买家一次购买了多种商品，如一件毛衣和两双袜子，称为交易父订单 对于每种商品的交易，称为交易子订单：此交易父订单有两个子订单与之对应。假设设计交易父订单事实表，则对于此事实表的每一条记录，在商品表中都有 一到多条记录与之对应。</li>
<li>处理方式<ul>
<li>第一种是降低事实表的粒度。比如将交易订单设计为子订单粒度</li>
<li>第二种是采用多字段。比如地产销售中，每次合同签订可能存在多个买受方的情况。对于合同签订事实表，每条记录可能对应多个买受方，由于合同已经是事实表中的最细粒度，无法通过降低粒度的方式来解决。但由于合同签订的买受人一般不会太多，所以一般采用多字段方式。<ul>
<li><a href="https://imgse.com/i/pkOcv7t"><img src="https://s21.ax1x.com/2024/07/31/pkOcv7t.png" alt="pkOcv7t.png"></a></li>
</ul>
</li>
<li>第三种是桥接表<ul>
<li><a href="https://imgse.com/i/pkOgnhT"><img src="https://s21.ax1x.com/2024/07/31/pkOgnhT.png" alt="桥接表的方式"></a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="10-4-4-多值属性"><a href="#10-4-4-多值属性" class="headerlink" title="10.4.4 多值属性"></a>10.4.4 多值属性</h5><ul>
<li>维表中某个属性字段同时有多个值，称为多值属性。一般有三种处理方式<ul>
<li>第一种：保持维度主键不变，将多值属性放在维度的一个属性字段中。比如商品属性，统一通过k-v对的形式存放在property字段中。这种方式扩展性好，但是使用数据做统计会比较麻烦。</li>
<li>第二种：保持维度主键不变，但是将多值属性拆开，将多值属性放在维度的多个属性字段中。</li>
<li>第三种：就是将多值属性炸开。维度的id和属性id做为联合主键，这种方式使用方便，但是数据会出现很多倍的膨胀。淘宝的商品属性表采用此种方式，数据量达到了几百亿的级别</li>
</ul>
</li>
</ul>
<h5 id="10-4-5-杂项维度"><a href="#10-4-5-杂项维度" class="headerlink" title="10.4.5 杂项维度"></a>10.4.5 杂项维度</h5><ul>
<li><p>杂项维度是由操作型系统中的指示符或者标志字段组合而成的，一般不在一致性维度之列。比如淘宝交易订单的交易类型字段，包括话费充值，司法拍卖，航旅等类型；支付状态，物流状态等，它们在源系统中直接保存在交易表中。</p>
</li>
<li><p>这时，通常的解决方案就是建立杂项维度 ，将这些字段建立到一个维表中，在事实表中只需保存一个外键 即可。多个字段的不同取值组成一条记录，生成代理键，存入维表中，并将该代理键保存到相应的事实表字段下。建议不要直接使用所有的组合生成完整 的杂项维表，在抽取</p>
<p>遇到新的组合时生成相应的记录即可。杂项维度 ETL 过程比一般维度略微复杂些。</p>
</li>
</ul>
<h3 id="第十一章事实表设计"><a href="#第十一章事实表设计" class="headerlink" title="第十一章事实表设计"></a>第十一章事实表设计</h3><h4 id="11-1-事实表基础"><a href="#11-1-事实表基础" class="headerlink" title="11.1 事实表基础"></a>11.1 事实表基础</h4><h5 id="11-1-1-事实表特性"><a href="#11-1-1-事实表特性" class="headerlink" title="11.1.1 事实表特性"></a>11.1.1 事实表特性</h5><ul>
<li>事实表作为维度建模的核心，紧紧<strong>围绕着业务过程</strong>来设计，通过获取描述业务过程的度量来表达业务过程，包含了<strong>引用的维度</strong>和业务过程有关的<strong>度量</strong>。</li>
<li>事实表中<strong>一条记录</strong>所表达的业务细节程度被称为<strong>粒度</strong>。通常粒度有两种表述方式：一种是<strong>维度属性组合</strong>所表示的细节程度；一种是所表示的具体<strong>业务含义</strong>。</li>
<li>作为度量业务过程的事实，一般为整型或浮点型。具有可加性，半可加性和不可加性。</li>
<li>事实表分为三种类型：事务型事实表，周期型快照事实表，累积快照事实表</li>
</ul>
<h5 id="11-1-2事实表设计原则"><a href="#11-1-2事实表设计原则" class="headerlink" title="11.1.2事实表设计原则"></a>11.1.2事实表设计原则</h5><ul>
<li>尽可能包含所有与业务过程相关的事实</li>
<li>只选择与业务过程相关的事实</li>
<li>分解不可加性事实为可加的组件</li>
<li>在选择维度和事实之前必须先声明粒度</li>
<li>在同一个事实表中不能有多种不同粒度的事实</li>
<li>事实的单位要保持一致</li>
<li>对事实的null值要处理</li>
<li>使用维度退化提高事实表的易用性</li>
</ul>
<h5 id="11-1-3-事实表设计方法"><a href="#11-1-3-事实表设计方法" class="headerlink" title="11.1.3 事实表设计方法"></a>11.1.3 事实表设计方法</h5><ul>
<li>Kimball的维度建模理论，对于维度模型设计采用四步设计方法：1.选择业务过程；2.声明粒度；3.确定维度；4.确定事实</li>
<li>OneData理论改进版：<ul>
<li>第一步：选择业务过程及确定事实表类型。<ul>
<li>明确业务需求之后，对业务过程进行划分，选择我们需要的业务过程，并确定事实表的类型</li>
</ul>
</li>
<li>第二步：声明粒度<ul>
<li>即确定事实表中一行数据所表示的业务含义</li>
</ul>
</li>
<li>第三步：确定维度<ul>
<li>完成声明粒度之后，也意味着确定了主键，对应的维度组合以及相关的维度字段就可以确定了。</li>
</ul>
</li>
<li>第四步：确定事实<ul>
<li>事实可以通过回到“过程的度量是什么‘来确定。应该选择与业务过程有关的所有事实，且事实的粒度要与所声明的事实表的粒度一致。</li>
</ul>
</li>
<li>第五步：冗余维度<ul>
<li>即维度退化，在事实表中冗余常用的维度字段，方便下游的查询</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="11-2-事务事实表"><a href="#11-2-事务事实表" class="headerlink" title="11.2 事务事实表"></a>11.2 事务事实表</h4><h5 id="11-2-1-设计过程"><a href="#11-2-1-设计过程" class="headerlink" title="11.2.1 设计过程"></a>11.2.1 设计过程</h5><ul>
<li>任何类型的事件都可以理解为一种事务。比如交易过程中的创建订单，买家付款，物流过程中的揽货，发货，签收，退款中的申请退款，申请小二介入等。都可以被理解成一种事务。事务事实表就是针对这些过程构建的一类事实表，用以跟踪定义业务过程的个体行为，提供丰富的分析能力，作为数据仓库原子的明细数据。</li>
<li>1.选择业务过程<ul>
<li>选择要分析的业务流程</li>
<li>Kimball维度建模理论任务，为了方便进行独立的分析研究，应该为每个业务过程建立一个事实表。但是将不同的业务过程放到同一个事实表也是一个可以考虑的实现方式。</li>
</ul>
</li>
<li>2.声明粒度<ul>
<li>业务过程选定之后，就要针对每个业务过程确定一个粒度，即确定事务事实表每一行所表达的细节层次。</li>
</ul>
</li>
<li>3.确定维度<ul>
<li>选定好业务过程且确定粒度后，就可以确定维度信息了。在淘宝交易事务事实表设计过程中，按照经常用于统计分析的场景，确定维度包含：买家，卖家，商品，商品类目，发货地区，收货地区，父订单维度以及杂项维度。由于订单的属性较多，比如订单的业务类型，是否无线交易，订单的attributes属性等。对于这些使用较多却又无法归属到上述买卖家或商品维度中的属性，则新建一个杂项维度进行存放。</li>
</ul>
</li>
<li>4.确定事实<ul>
<li>作为过程度量的核心，事实表应该包含与其描述过程有关的所有事实。以淘宝交易事务事实表为例，选定三个业务过程一一下单、支付和成功完结。不同的业务过程有不同的事实（度量值）。比如下单业务中的：下单金额，下单数量，下单分摊金额，在支付业务过程中，包含支付金额，分摊油费，折扣金额，红包金额，积分金额；在完结业务过程中包含确认收货金额等。由于粒度是子订单，所以对于一些父订单上的金额需要分摊到子订单上，比如父订单邮费，父订单折扣等。</li>
<li>按照kimball的维度建模理论，经过已上步骤，淘宝交易事务事实表已经成型<ul>
<li><a href="https://imgse.com/i/pA9ZTP0"><img src="https://s21.ax1x.com/2024/08/14/pA9ZTP0.png" alt="pA9ZTP0.png"></a></li>
</ul>
</li>
</ul>
</li>
<li>5.冗余维度(维度退化)<ul>
<li>OneData理论在已上四步的基础上增加了<strong>维度退化</strong>的一步。这个过程在kimball的维度建模理论中也有所提及；但是阿里数仓出于效率和资源的考虑，将常用维度全部退化到事实表中，使下游分析使用更加方便。</li>
<li>在确定维度时，包含了买卖家维度，商品维度，类目维度，收发货维度等。Kimball维度建模理论建议在事实表中只保存这些维表的外键。而淘宝交易事务事实表在kimball维度建模基础上做了进一步的优化，将买卖星级，标签，店铺名称，商品类型，商品特性，商品属性，类目层级等常用的维度属性冗余进事实表中，提高对事实表过滤查询，统计的效率。</li>
<li><a href="https://imgse.com/i/pA9epPx"><img src="https://s21.ax1x.com/2024/08/14/pA9epPx.png" alt="pA9epPx.png"></a></li>
</ul>
</li>
</ul>
<h5 id="11-2-2-单事务事实表"><a href="#11-2-2-单事务事实表" class="headerlink" title="11.2.2 单事务事实表"></a>11.2.2 单事务事实表</h5><ul>
<li><p>单事务事实表，就是针对每一个业务过程设计一个事实表。可以方便的对每个业务过程进行独立的分析。1688交易流程和淘宝类似，也是下单，支付，发货和完结。在这个四个关键流程中1688选择<strong>下单</strong>和<strong>支付</strong>两个业务过程分别设计事务事实表</p>
</li>
<li><p><a href="https://imgse.com/i/pA9l39A"><img src="https://s21.ax1x.com/2024/08/14/pA9l39A.png" alt="pA9l39A.png"></a></p>
</li>
<li><p><a href="https://imgse.com/i/pA9lGct"><img src="https://s21.ax1x.com/2024/08/14/pA9lGct.png" alt="pA9lGct.png"></a></p>
</li>
</ul>
<h5 id="11-2-3-多事务事实表"><a href="#11-2-3-多事务事实表" class="headerlink" title="11.2.3 多事务事实表"></a>11.2.3 多事务事实表</h5><ul>
<li><p>多事务事实表，将不同的事实放到同一个事实表中，即<strong>同一个事实表包含不同的业务过程。</strong></p>
</li>
<li><p>多事务事实表有两种设计方式：</p>
<ul>
<li>1.不同的业务过程的事实使用<strong>不同的事实字段存放</strong></li>
<li>2.不同的业务过程的事实使用<strong>同一个事实字段进行存放</strong>，但需要增加一个业务过程标签用于区分不同的业务过程。</li>
</ul>
</li>
<li><p>淘宝交易事务事实表（不同事实字段存放）</p>
<ul>
<li>淘宝交易事务事实表中包含了下单，支付，完结三个业务过程，三个业务过程的粒度都是子订单粒度。没有把发货放到此事务事实表中，是因为发货的粒度比子订单更细（比如拆分发货，聚合发货等多种情况），属于不同粒度上的业务过程。</li>
<li>确定好业务过程和粒度后，下一步就是确定维度和事实。一般来说，对于不同的业务过程和粒度，维度也不完全一致。但在设计淘宝交易事务事实表时，根据分析统计，常用维度比较一致，因此在维度层面可以保证这三个业务过程放到同一个事务事实表中。这里的维度也是在交易过程中比较常见的，如包括买家、卖家、商品、类目、店铺、收发货地区等，无论在哪一个业务过程中，都需要按照这些维度进行统计分析。</li>
<li>将多个业务过程放到同一个事实表中，需要面对如何处理多个事实的问题。交易事务事实表需要包含<strong>下单度量</strong>，<strong>支付度量</strong>，<strong>成功完结度量</strong>。这里的解决方案是针对每一个度量都使用一个字段进行保存,即不同的事实使用不同的字段进行存放。如果不是当前业务过程的度量，则采取<strong>零值处理方式</strong>。</li>
<li>同一个事实表中包含了多个业务过程，如何在表中进行区分标记呢？这里的解决方案是对每一个业务过程打一个标签，标记当天是否是这个业务过程。比如，针对下单打一个是否当天下单的标签；针对支付，打一个是否当天支付的标签；针对成功完结打一个是否当天成功完结的标签，标签之间互不相干。</li>
<li><a href="https://imgse.com/i/pA98XUU"><img src="https://s21.ax1x.com/2024/08/14/pA98XUU.png" alt="pA98XUU.png"></a></li>
</ul>
</li>
<li><p>收藏商品事务事实表</p>
<ul>
<li><p>收藏业务一般包含两个业务过程：收藏商品和删除商品。接下来是确定粒度，无论收藏还是删除收藏的商品都是用户对商品的一个操作。</p>
</li>
<li><p>确定好业务过程和粒度后，接下来是确定维度和事实，由于粒度是用户加上商品，所以维度主要是用户维度和商品维度。为了使事实表更加丰富，冗余了商品类目维度和商品所属卖家维度，收藏商品和删除商品业务过程所属的维度是一致的。</p>
</li>
<li><p>收藏和删除商品是两个不同的业务过程，但是确定了相同的粒度和维度，所以考虑设计多事务事实表，将这两个业务过程放到同一个事实表中，只是在不同业务过程的事实上进行区分。</p>
</li>
<li><p>这里的解决方案是<strong>使用同一个字段存放不同业务过程的事实</strong>，使用标签字段区分不同的业务过程。收藏商品和删除商品的事实主要是商品价格，不过收藏事务事实表更多的是无事实的事实表，一般用于统计收</p>
<p>藏或者删除的次数。</p>
</li>
<li><p><a href="https://imgse.com/i/pA9GYGQ"><img src="https://s21.ax1x.com/2024/08/14/pA9GYGQ.png" alt="pA9GYGQ.png"></a></p>
</li>
</ul>
</li>
<li><p>多事务事实表的选择</p>
<ul>
<li>当不同业务过程的度量比较相似，差异不大时，可以采用第二种多事务事实表的设计方式，使用同一个字段来表示度量数据。但这种方式存在 一个问题一一在同一个周期内会存在多条记录。</li>
<li>当不同业务过程的度量差异较大时，可以选择第一事务事实表的设计方式，将不同业务过程的度量使用不同字段冗余到表中，非当前业务过程则置零表示。这种方式所存在的问题是度量字段零值较多。</li>
</ul>
</li>
</ul>
<h5 id="11-2-4-两种事实表的对比"><a href="#11-2-4-两种事实表的对比" class="headerlink" title="11.2.4 两种事实表的对比"></a>11.2.4 两种事实表的对比</h5><ul>
<li>哪张设计方式更好，可以从下面这些方面考量。</li>
<li>1.业务过程</li>
<li>2.粒度和维度</li>
<li>3.事实</li>
<li>4.下游业务使用</li>
<li>5.计算存储成本</li>
</ul>
<h5 id="11-2-5-父子事实的处理方式"><a href="#11-2-5-父子事实的处理方式" class="headerlink" title="11.2.5 父子事实的处理方式"></a>11.2.5 父子事实的处理方式</h5><ul>
<li>以淘宝交易事务事实表来说，选择子订单粒度表示一行数据的含义。一次对于父子订单的情况来说，需要把父订单的下单总额或者支付总额分摊到每个子订单上。通过分摊父订单的金额，将所有业务过程的度量全部带进淘宝交易事务事实表中。</li>
</ul>
<h5 id="11-2-6-事实的设计准则"><a href="#11-2-6-事实的设计准则" class="headerlink" title="11.2.6 事实的设计准则"></a>11.2.6 事实的设计准则</h5><ul>
<li>1.事实完整性</li>
<li>2.事实一致性</li>
<li>3.事实可加性</li>
</ul>
<h4 id="11-3周期快照事实表"><a href="#11-3周期快照事实表" class="headerlink" title="11.3周期快照事实表"></a>11.3周期快照事实表</h4><ul>
<li>事务事实表可以很好地跟踪一个事件，并对其进行度量。但是，当需要一些状态度量时，比如账户余额，买卖家星级，商品库存，卖家累积交易额等。则需要聚集与之相关的事务才能进行识别计算。对于这些状态度量，事务事实表是无效率的，而这些度量也和事务度量一样是有用的。因此维度建模理论给出了第二种常见的事实表——周期快照事实表。</li>
<li>快照事实表在确定的时间间隔内对实体的度量进行抽样，这样可以很容易的研究实体的度量值，而不需要聚集长期的事务历史。</li>
<li>特性：快照事实表的粒度通常以维度形式声明；快照事实表是稠密的；快照模型将至少包含一个用来展示半可加性质的事实。</li>
<li>快照事实表的设计步骤可以归纳为：<ul>
<li>1.确定快照事实表的快照粒度</li>
<li>2.确定快照事实表采样的状态度量。</li>
</ul>
</li>
</ul>
<h5 id="11-3-2-案例"><a href="#11-3-2-案例" class="headerlink" title="11.3.2 案例"></a>11.3.2 案例</h5><ul>
<li>1.单维度的每天快照事实表<ul>
<li><a href="https://imgse.com/i/pAPariq"><img src="https://s21.ax1x.com/2024/08/19/pAPariq.png" alt="pAPariq.png"></a></li>
</ul>
</li>
<li>2.混合维度的每天快照事实表<ul>
<li><a href="https://imgse.com/i/pAPagQU"><img src="https://s21.ax1x.com/2024/08/19/pAPagQU.png" alt="pAPagQU.png"></a></li>
<li>已上的两类快照事实表都有一个特点——都可以从事务事实表进行汇总产出，这是周期快照事实表的一种常见产出模式。除此之外，还有一种直接使用ods的数据作为周期快照事实表的数据源进行加工。比如淘宝卖家星级，卖家DSR事实表等。</li>
<li><a href="https://imgse.com/i/pAPdekn"><img src="https://s21.ax1x.com/2024/08/19/pAPdekn.png" alt="pAPdekn.png"></a></li>
</ul>
</li>
<li>3.全量快照事实表<ul>
<li>还有一种特殊的快照事实表，即全量快照事实表。</li>
<li><a href="https://imgse.com/i/pAPdW1f"><img src="https://s21.ax1x.com/2024/08/19/pAPdW1f.png" alt="pAPdW1f.png"></a></li>
</ul>
</li>
</ul>
<h5 id="11-3-3-注意事项"><a href="#11-3-3-注意事项" class="headerlink" title="11.3.3 注意事项"></a>11.3.3 注意事项</h5><ul>
<li>1.事务与快照成对设计<ul>
<li>数仓维度建模中，对于事务事实表和快照事实表往往是成对设计的，相互补充，以满足更多的下游统计分析需求。特别是在事务事实表的基础上可以加工快照事实表，如淘宝卖家历史至今快照事实表，就是在事务事实表的基础上加工得到的，既丰富了星形模型又降低了下游分析的成本。</li>
</ul>
</li>
<li>2.附加事实<ul>
<li>可以附加一些上一个采样周期的状态度量</li>
</ul>
</li>
<li>3.周期到日期度量<ul>
<li>针对多种周期到日期的度量设计了不同的快照事实表，比如淘宝卖家财年至今的下单金额，淘宝商品自然年至今的收藏次数等。</li>
</ul>
</li>
</ul>
<h4 id="11-4累积快照事实表"><a href="#11-4累积快照事实表" class="headerlink" title="11.4累积快照事实表"></a>11.4累积快照事实表</h4><ul>
<li>对于研究事件时间间隔的需求，采用累积快照事实表可以很好的实现。</li>
<li>比如统计买家下单到支付的时长、买家支付到卖家发货的时长、买家从下单到确认收货的时长等。</li>
</ul>
<h5 id="11-4-1-设计过程"><a href="#11-4-1-设计过程" class="headerlink" title="11.4.1 设计过程"></a>11.4.1 设计过程</h5><ul>
<li>设计过程与事务事实表的设计是一样的。</li>
<li>1.选择业务过程<ul>
<li>在前面我们学习到，淘宝交易订单的流转过程有四个事件：买家下单，买家支付，卖家发货，买家确认收货的的四个业务过程。在事务事实表中，我们只关注了下单，支付和确认收货三个业务过程，而在统计事件时间间隔的需求中，卖家发货也是关键环节。所以针对淘宝交易累积快照事实表，我们选择四个业务过程</li>
</ul>
</li>
<li>2.声明粒度<ul>
<li>和前面类似，我们依旧是选择子订单粒度，但是对于累积快照事实表来说，一个子订单在此表中只有一条记录</li>
</ul>
</li>
<li>3.确定维度<ul>
<li>维度和上面类似，基本都是买家，卖家，店铺，商品，类目，发货地区，收货地区等。四个业务过程的时间字段要进行标明</li>
</ul>
</li>
<li>4.确定事实<ul>
<li>包含四个业务过程中对于的事实，即度量值</li>
</ul>
</li>
<li>5.维度退化</li>
</ul>
<ul>
<li>为方便分析，冗余一些常用的维度到事实表中</li>
</ul>
<ul>
<li><a href="https://imgse.com/i/pAPweDe"><img src="https://s21.ax1x.com/2024/08/19/pAPweDe.png" alt="pAPweDe.png"></a></li>
<li></li>
</ul>
<h5 id="11-4-2-特点"><a href="#11-4-2-特点" class="headerlink" title="11.4.2 特点"></a>11.4.2 特点</h5><ul>
<li>1.数据不断更新<ul>
<li>会随着业务过程的递进，数据会不断发生变化</li>
</ul>
</li>
<li>2.多业务过程日期<ul>
<li>会分别记录多个业务过程发生的时间</li>
</ul>
</li>
</ul>
<h5 id="11-4-3-特殊处理"><a href="#11-4-3-特殊处理" class="headerlink" title="11.4.3 特殊处理"></a>11.4.3 特殊处理</h5><ul>
<li>非线性过程<ul>
<li>对于发生的一些流转，循环过程。需要进行一些统一处理<ul>
<li>确定好业务过程要统一</li>
<li>针对关键业务过程构建全面流程。全流程是下单–&gt;支付–&gt;发货–&gt;确认收货。对于缺失的过程，时间字段和事实置为空</li>
<li>循环流程的处理。选择第一次时间还是最后一次时间要和业务方确定</li>
</ul>
</li>
</ul>
</li>
<li>多源过程<ul>
<li>对于淘宝交易累积快照事实表，除了上述提到的下单→支付→发货→确认收货流程，假设需要关注交易子订单退款业务或者物流业务，此时会涉及交易、售后、物流 多个业务源系统。</li>
<li>针对多源业务建模，主要考虑事实表的粒度问题。对于淘宝交易累积快照事实表，其粒度是交易子订单。对于退款，由于每个子订单可能存在多次退款，此时如果要将退款相关业务过程加入模型中，则需要和商业用户确定存在多次退款时如何取舍，确保模型粒度不变。</li>
</ul>
</li>
</ul>
<h5 id="11-4-4-物理实现"><a href="#11-4-4-物理实现" class="headerlink" title="11.4.4 物理实现"></a>11.4.4 物理实现</h5><ul>
<li><p>第一种是全量表的形式。此全量表一般为日期分区表，每天的分区存储昨天的全量数据和当天的增量数据合并的结果，保障每条记录的状态最新。此种方式适用于全量数据较少的情况。如果数据量很大，</p>
<p>此全量表数据量不断膨胀，存储了大量永远不再更新的历史数据，对ETL 和分析统计性能影响较大。</p>
</li>
<li><p>第二种是全量表的变化形式。此方式针对数据量大的情况。业务实体一般从生产到消亡都有一个相对较大的时间间隔。</p>
</li>
<li><p>比如针对交易订单，我们以 200 天作为订单从产生到消亡的最大间隔。设计最近 200 天的交易订单累积快照事实 ，每天 的分区存储最近 200 天的交易订单 200 天之前的订单则按照 gmt_create建分区存储在归档表中。</p>
</li>
<li><p>第三种方式是以业务实体的结束时间分区。每天的分区存放当天结束的数据，设计一个时间非常大的分区，比如 3000-12-31 ，存放截至当前未结束的数据。由于每天将当天结束的数据归档至当天分区中，时间非常大的分区数据量不会很大， ETL 性能较好；并且无存储的浪费，对于业务实体的某具体实例，在该表的全量数据中唯一。 比如对于交易订单，在交易累积快照事实表中唯一。</p>
</li>
<li><p>第三种方式，存在一些特殊情况，即业务系统无法标识业务实体的结束时间。</p>
</li>
</ul>
<h4 id="11-5三种事实表的比较"><a href="#11-5三种事实表的比较" class="headerlink" title="11.5三种事实表的比较"></a>11.5三种事实表的比较</h4><ul>
<li><a href="https://imgse.com/i/pAPfEFK"><img src="https://s21.ax1x.com/2024/08/20/pAPfEFK.png" alt="pAPfEFK.png"></a></li>
<li>由于事实表种许多日期是在首次加载时不知道的，而且这类事实表在数据加载完成后，可以对其数据进行更新，来补充业务状态变更时的日期信息和事实。</li>
</ul>
<h4 id="11-6-无事实的事实表"><a href="#11-6-无事实的事实表" class="headerlink" title="11.6 无事实的事实表"></a>11.6 无事实的事实表</h4><ul>
<li>不包含事实或度量的事实表称为无事实的事实表。主要有两类：<ul>
<li>第一种是事件类的，记录事件的发生。如用户浏览的日志</li>
<li>第二种是条件，范围或资格类的，记录维度与维度多对多之间的关系。如客户和销售人员的分配情况，产品的促销范围等。</li>
</ul>
</li>
</ul>
<h4 id="11-7-聚集型事实表"><a href="#11-7-聚集型事实表" class="headerlink" title="11.7 聚集型事实表"></a>11.7 聚集型事实表</h4><ul>
<li>数仓的性能是数仓建设是否成功的重要标准之一。聚集主要是通过汇总明细粒度数据来获得改进查询性能的效果。通过访问聚集数据，可以减少查询的复杂度，快速响应，同时也有利于减少不同用户访问明细数据带来的结果不一致问题。聚集带来良好的效益，但是对于ETL来说是更多工作内容和挑战。</li>
<li>阿里将使用频繁的公用数据，通过聚集进行沉淀，比如卖家最近一天的交易汇总表，卖家最近N天的交易汇总表等</li>
</ul>
<h5 id="11-7-1-聚集的基本原则"><a href="#11-7-1-聚集的基本原则" class="headerlink" title="11.7.1 聚集的基本原则"></a>11.7.1 聚集的基本原则</h5><ul>
<li>一致性<ul>
<li>聚集后的结果要和明细粒度的统计结果是一致的</li>
</ul>
</li>
<li>避免单一表设计<ul>
<li>不要在同一个表中存储不同层次的聚集数据，否则很可能导致重复计算的问题。</li>
<li>在聚集表中有些行按天汇总，有些按月汇总，会导致重复计算，可以对每一行数据加上标识，但是这样不易使用。最好的方式是使用不同的字段，即通过两列存放</li>
</ul>
</li>
<li>聚集粒度可不同<ul>
<li>聚集只关心需要查询的维度，可以根据需求随意聚合。</li>
<li>订单涉及的维度有商品、买家、卖家、地域等，比如可以按照商品汇总 天的交易额，可以按照卖家汇总一天的营业额（交易额） 可以按照商品与地域汇总一月的交易额。</li>
</ul>
</li>
</ul>
<h5 id="11-7-2-聚集的基本步骤"><a href="#11-7-2-聚集的基本步骤" class="headerlink" title="11.7.2 聚集的基本步骤"></a>11.7.2 聚集的基本步骤</h5><ul>
<li>第一步：确定聚集维度<ul>
<li>在原始明细模型中会存在多个描述事实的维度 ，如 日期、商品类卖家等，这时候需要确定根据什么维度聚集 ，如果只关心商品的交易额情况，那么就可以根据商品维度聚集数据。</li>
</ul>
</li>
<li>第二步：确定一致性上钻<ul>
<li>这时候要关心是按月汇总还是按天汇总，是按照商品汇总还是按照类目汇总，如果按照类目汇总，还需要关心是按照大类汇总还是小类汇总。当然，我们要做的只是了解用户需要什么，然后按照他们想要的进行聚集。</li>
</ul>
</li>
<li>第三步：确定聚集事实<ul>
<li>在原始明细模型中可能会有多个事实的度量，比如在交易中有交易额、交易数量等，这时候要明确是按照交易额汇总还是按照成交数量汇总。</li>
</ul>
</li>
</ul>
<h5 id="11-7-3-公共汇总层"><a href="#11-7-3-公共汇总层" class="headerlink" title="11.7.3 公共汇总层"></a>11.7.3 公共汇总层</h5><ul>
<li>1.基本原则：<ul>
<li>数据公用性。汇总的聚集会有第三者使用吗？基于某个维度的聚集是不是经常用于数据分析中？如果答案是肯定的，那么就有必要把明细数据经过汇总沉淀到聚集表中。</li>
<li>不跨数据域。数据域是在较高层次上对数据进行分类聚集的抽象。阿里巴巴以业务过程进行分类，如交易统一划到交易域下，商品的新增、修改放到商品域下。</li>
<li>区分统计周期。在表的命名上要能说明数据的统计周期，如 1d表示最近 天， td 表示截至当天， nd 表示最近 天。</li>
</ul>
</li>
<li>2.交易汇总表的设计<ul>
<li><a href="https://imgse.com/i/pAPX5Kx"><img src="https://s21.ax1x.com/2024/08/20/pAPX5Kx.png" alt="pAPX5Kx.png"></a></li>
<li>可以看到一个交易事务事实表就有如此多潜在聚集的方式。下面是一些案例</li>
<li><a href="https://imgse.com/i/pAPXoqK"><img src="https://s21.ax1x.com/2024/08/20/pAPXoqK.png" alt="pAPXoqK.png"></a></li>
</ul>
</li>
</ul>
<h5 id="11-7-4-聚集一些说明"><a href="#11-7-4-聚集一些说明" class="headerlink" title="11.7.4 聚集一些说明"></a>11.7.4 聚集一些说明</h5><ul>
<li>聚集是不跨越事实的<ul>
<li>聚集是针对原始星形模型进行的汇总，为了获取和查询与原始模型一致的结果，<strong>聚集的维度和度量必须与原始模型保持一致，因此聚集是不跨越事实的</strong>。横向钻取是针对多个事实基于一致性维度进行的分析，很多时候采用融合事实表，预先存放横向钻取的结果，从而提高查询性能。因此，融合事实表是 种导出模式而不是聚集。</li>
</ul>
</li>
<li>聚集带来的问题<ul>
<li>聚集会带来查询性能的提升，但聚集也会增加 ETL 维护的难度。当子类目对应的一级类目发生变更时，先前存在的、已经被汇总到聚集表中的数据需要被重新调整。这一额外工作随着业务复杂性的增加，会导致多数 ETL 人员选择简单强力的方法，删除并重新聚集数据。</li>
</ul>
</li>
</ul>
<h2 id="数据管理篇"><a href="#数据管理篇" class="headerlink" title="数据管理篇"></a>数据管理篇</h2><h3 id="第12章：元数据"><a href="#第12章：元数据" class="headerlink" title="第12章：元数据"></a>第12章：元数据</h3><h4 id="技术元数据"><a href="#技术元数据" class="headerlink" title="技术元数据"></a>技术元数据</h4><ul>
<li>技术元数据市存储关于数据仓库系统技术细节的数据，<strong>是用于开发和管理数据仓库使用的数据</strong>。常见的技术元数据有：<ul>
<li><strong>分布式计算系统存储元数据，如表，列，分区等信息。</strong>分区信息、责任人信息、文件大小、表类型，生命周期，以及列的字段名、字段类型、字段备注、是否是分区字段等信息。</li>
<li><strong>分布式计算系统运行元数据。</strong>如hive的Job日志，包括作业类型，实例名称，输入输出，sql，运行参数，执行时间，运行日志等信息</li>
<li><strong>数据开发平台中的同步任务，计算任务，任务调度信息。</strong>包括数据同步的输入输出表和字段，以及同步任务本身的节点信息：计算任务主要有输入输出、任务本身的节点信息 任务调度主要有任务的依赖类型、依赖关系等，以及不同类型调度任务的运行日志等。</li>
<li><strong>数据质量和运维相关元数据。</strong>如任务监控、运维报警、数据质量、故障等信息，包括任务监控运行日志、告警配置及运行日志、故障信息等。</li>
</ul>
</li>
</ul>
<h4 id="业务元数据"><a href="#业务元数据" class="headerlink" title="业务元数据"></a>业务元数据</h4><ul>
<li>业务元数据从业务角度描述了数据仓库中的数据。常见的元数据有：<ul>
<li><strong>维度建模元数据，如维度及属性，业务过程，指标等规范化定义，用于更好地管理和使用数据</strong></li>
<li><strong>数据应用元数据</strong>，如数据报表，数据产品等的配置和运行元数据。</li>
</ul>
</li>
</ul>
<h4 id="元数据体系建设"><a href="#元数据体系建设" class="headerlink" title="元数据体系建设"></a>元数据体系建设</h4><ul>
<li>元数据的质量直接影响到数据管理的准确性。元数据建设的目标是打通数据接入到加工，再到数据消费整个链路，规范元数据体系与模型，提供统一的元数据出口，保障元数据产出的稳定性和质量</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>书籍</tag>
      </tags>
  </entry>
  <entry>
    <title>flume</title>
    <url>/2021/06/21/flume/</url>
    <content><![CDATA[<h1 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h1><h2 id="Flume概述"><a href="#Flume概述" class="headerlink" title="Flume概述"></a>Flume概述</h2><ul>
<li><p>配置文档，自定义Source,Interceptor,sink就来看官方文档：<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#spooling-directory-source">官方文档</a></p>
</li>
<li><p>flume是一个高可用的，可靠的，分布式的<em><strong>海量日志采集，聚合，和传输的系统</strong></em>基于流式框架，灵活简单。</p>
</li>
<li><p>基础架构：</p>
<p><a href="https://imgtu.com/i/REvBm8"><img src="https://z3.ax1x.com/2021/06/21/REvBm8.png" alt="REvBm8.png"></a></p>
<ul>
<li>Agent:是一个JVM进程，它以事件的形式将数据从源头送至目的</li>
<li>source负责接收数据到Flume agent 的组件。可以处理各种类型，格式的日志数据。例如：spooling directory，netcat,avro,exec</li>
<li>sink不断轮询channel中的事件并且批量的移除他们，将这些事件批量写入到存储或索引系统，或者被发送到另一个flume agent。sink组件发送目的地包括：hdfs,logger,avro,file,HBase</li>
<li>Channel是位于source和sink之间缓冲区。Flume自带两种Channel：Memory Channel和File Channel。</li>
</ul>
</li>
<li><p>event:flume传输数据的基本单元。由Header和Body组成。Header用来存放该event的一些属性，为K-V结构，Body用来存放该条数据，形式为字节数组。</p>
</li>
</ul>
<h2 id="flume入门案例"><a href="#flume入门案例" class="headerlink" title="flume入门案例"></a>flume入门案例</h2><ul>
<li><p>实时监控Hive日志，并上传到HDFS中</p>
<figure class="highlight jboss-cli"><table><tr><td class="code"><pre><code class="hljs jboss-cli">1.第一种方式<br>flume-ng agent <span class="hljs-params">--name</span> agent的名字 <span class="hljs-params">--conf</span> 配置文件的目录 <span class="hljs-params">--conf-file</span> agent的配置文件（自己手动写）-Dflume.root.logger=INFO,console<br><br>需要写配置文件<br>基本步骤：<br><span class="hljs-comment">#1.agent的source,channel,sink组件</span><br><span class="hljs-comment">#2.source配置</span><br><span class="hljs-comment">#3.channel配置</span><br><span class="hljs-comment">#4.sink配置</span><br><span class="hljs-comment">#5.source,channel,sink之间的关系</span><br><span class="hljs-comment"># Name the components on this agent</span><br>a2.sources = r2<br>a2.sinks = k2<br>a2.channels = c2<br><br><span class="hljs-comment"># Describe/configure the source</span><br>a2.sources.r2.type = exec<br>a2.sources.r2.<span class="hljs-keyword">command</span> = tail -F <span class="hljs-string">/opt/module/hive/logs/hive.log</span><br>a2.sources.r2.shell = <span class="hljs-string">/bin/bash</span> -c<br><br><span class="hljs-comment"># Describe the sink</span><br>a2.sinks.k2.type = hdfs<br>a2.sinks.k2.hdfs.path = hdfs:<span class="hljs-string">//hadoop102</span><span class="hljs-function">:8020</span>/flume/%Y%m%d/%H<br><span class="hljs-comment">#上传文件的前缀</span><br>a2.sinks.k2.hdfs.filePrefix = logs-<br><span class="hljs-comment">#是否按照时间滚动文件夹</span><br>a2.sinks.k2.hdfs.round = <span class="hljs-literal">true</span><br><span class="hljs-comment">#多少时间单位创建一个新的文件夹</span><br>a2.sinks.k2.hdfs.roundValue = 1<br><span class="hljs-comment">#重新定义时间单位</span><br>a2.sinks.k2.hdfs.roundUnit = hour<br><span class="hljs-comment">#是否使用本地时间戳</span><br>a2.sinks.k2.hdfs.useLocalTimeStamp = <span class="hljs-literal">true</span><br><span class="hljs-comment">#积攒多少个Event才flush到HDFS一次</span><br>a2.sinks.k2.hdfs.<span class="hljs-keyword">batch</span>Size = 100<br><span class="hljs-comment">#设置文件类型，可支持压缩</span><br>a2.sinks.k2.hdfs.fileType = DataStream<br><span class="hljs-comment">#多久生成一个新的文件</span><br>a2.sinks.k2.hdfs.rollInterval = 60<br><span class="hljs-comment">#设置每个文件的滚动大小</span><br>a2.sinks.k2.hdfs.rollSize = 134217700<br><span class="hljs-comment">#文件的滚动与Event数量无关</span><br>a2.sinks.k2.hdfs.rollCount = 0<br><br><span class="hljs-comment"># Use a channel which buffers events in memory</span><br>a2.channels.c2.type = memory<br>a2.channels.c2.capacity = 1000<br>a2.channels.c2.transactionCapacity = 100<br><br><span class="hljs-comment"># Bind the source and sink to the channel</span><br>a2.sources.r2.channels = c2<br>a2.sinks.k2.channel = c2<br></code></pre></td></tr></table></figure>


</li>
<li><p>log4j.properties(日志架构)，类似于程序中的调试代码，输出一些东西，供编码的时候调试。但是项目上线后不能再直接输出，于是将这些调试输出输出到专门的日志文件中。</p>
</li>
</ul>
<h2 id="Flume进阶"><a href="#Flume进阶" class="headerlink" title="Flume进阶"></a>Flume进阶</h2><ul>
<li><p>flume事务。主要是为了保证数据不会丢失。</p>
<p><a href="https://imgtu.com/i/RZxhvR"><img src="https://z3.ax1x.com/2021/06/22/RZxhvR.png" alt="RZxhvR.png"></a></p>
</li>
<li><p>flume agent内部原理：</p>
<p><a href="https://imgtu.com/i/RZxsbV"><img src="https://z3.ax1x.com/2021/06/22/RZxsbV.png" alt="RZxsbV.png"></a></p>
</li>
<li><ol>
<li><p>Source接受数据，将数据以event的形式发给ChannnelProcessor</p>
</li>
<li><p>ChannelProcessor将事件传递给拦截器，在拦截其中可以对event数据进行更改。</p>
</li>
<li><p>拦截器再将事件返回到ChannelProcessor，ChannelProcessor将每个事件给Channel选择器。ChannelSelector的作用就是选出Event将要被发往哪个Channel。其共有两种类型，分别是Replicating（复制）和Multiplexing（多路复用）。</p>
</li>
<li><p>再返回到ChannelProcessor，根据选择器的选择，event进入不同的channel</p>
</li>
<li><p>SinkProcessor再决定channel中的enevt的走向。</p>
<p>SinkProcessor共有三种类型，分别是DefaultSinkProcessor、LoadBalancingSinkProcessor和FailoverSinkProcessor。</p>
<p>DefaultSinkProcessor对应的是单个的Sink，LoadBalancingSinkProcessor和FailoverSinkProcessor对应的是Sink Group，LoadBalancingSinkProcessor可以实现负载均衡的功能，FailoverSinkProcessor可以错误恢复的功能(故障转移）。</p>
</li>
</ol>
</li>
<li><p>flume拓扑结构</p>
<ul>
<li>简单串联。多个agent串起来</li>
<li>复制和多路复用。Flume支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel中，或者将不同数据分发到不同的channel中，sink可以选择传送到不同的目的地。</li>
<li>负载均衡和故障转移。Flume支持使用将多个sink逻辑上分到一个sink组，sink组配合不同的SinkProcessor可以实现负载均衡和错误恢复的功能。</li>
<li>聚合。将多台服务器部署一个flume日志采集，再将所有日志聚合到一个集中收集日志的flume，由此上传到HDFS,HBase，进行数据分析。</li>
</ul>
</li>
</ul>
<h3 id="API编程"><a href="#API编程" class="headerlink" title="API编程"></a>API编程</h3><ul>
<li>可以查看官网的开发者文档。</li>
<li>自定义Interceptor。定义类实现Interceptor接口，重写四个方法（初始化，单event修改，多event修改，关闭资源）还需要一个Builder静态内部类。</li>
</ul>
<h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="flume的source-sink-Channel的作用？"><a href="#flume的source-sink-Channel的作用？" class="headerlink" title="flume的source,sink,Channel的作用？"></a>flume的source,sink,Channel的作用？</h3><ul>
<li><p>作用：</p>
<p>（1）Source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy</p>
<p>（2）Channel组件对采集到的数据进行缓存，可以存放在Memory或File中。</p>
<p>（3）Sink组件是用于把数据发送到目的地的组件，目的地包括Hdfs、Logger、avro、thrift、ipc、file、Hbase、solr、自定义。</p>
</li>
</ul>
<h3 id="flume参数调优"><a href="#flume参数调优" class="headerlink" title="flume参数调优"></a>flume参数调优</h3><ul>
<li><p><em><strong>1. source</strong></em></p>
<p>增加Source个（使用Tair Dir Source时可增加FileGroups个数）可以增大Source的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个Source 以保证Source有足够的能力获取到新产生的数据。</p>
<p>batchSize参数决定Source一次批量运输到Channel的event条数，适当调大这个参数可以提高Source搬运Event到Channel时的性能。</p>
<p><em><strong>2. Channel:</strong></em></p>
<p>type 选择memory时Channel的性能最好，但是如果Flume进程意外挂掉可能会丢失数据。type选择file时Channel的容错性更好，但是性能上会比memory channel差。</p>
<p>使用file Channel时dataDirs配置多个不同盘下的目录可以提高性能。</p>
<p>Capacity 参数决定Channel可容纳最大的event条数。transactionCapacity 参数决定每次Source往channel里面写的最大event条数和每次Sink从channel里面读的最大event条数。transactionCapacity需要大于Source和Sink的batchSize参数。</p>
<p><em><strong>3. Sink</strong></em></p>
<p>增加Sink的个数可以增加Sink消费event的能力。Sink也不是越多越好够用就行，过多的Sink会占用系统资源，造成系统资源不必要的浪费。</p>
<p>batchSize参数决定Sink一次批量从Channel读取的event条数，适当调大这个参数可以提高Sink从Channel搬出event的性能。</p>
</li>
</ul>
<h3 id="flume数据是否会丢失？"><a href="#flume数据是否会丢失？" class="headerlink" title="flume数据是否会丢失？"></a>flume数据是否会丢失？</h3><ul>
<li>flume采用事务机制，Flume使用两个独立的事务分别负责从Soucrce到Channel，以及从Channel到Sink的事件传递。如果因为某种原因使得事件无法记录，那么事务将会回滚。</li>
<li>正是因为flume完善的事务机制，flume不会丢失数据。</li>
<li>唯一可能丢失数据的情况是Channel采用memoryChannel，agent宕机导致数据丢失，或者Channel存储数据已满，导致Source不再写入，未写入的数据丢失。</li>
<li>Flume不会丢失数据，但是有可能造成数据的重复，例如数据已经成功由Sink发出，但是没有接收到响应，Sink会再次发送数据，此时可能会导致数据的重复。</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>flume</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title>conclusion</title>
    <url>/2021/09/29/conclusion/</url>
    <content><![CDATA[<h1 id="重点知识点总结"><a href="#重点知识点总结" class="headerlink" title="重点知识点总结"></a>重点知识点总结</h1><h2 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h2><ul>
<li><p>linux常用命令</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>命令</th>
<th>命令解释</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>top</td>
<td>查看内存</td>
</tr>
<tr>
<td>2</td>
<td>df -h</td>
<td>查看磁盘存储情况</td>
</tr>
<tr>
<td>3</td>
<td>iotop</td>
<td>查看磁盘IO读写情况，要root权限</td>
</tr>
<tr>
<td>4</td>
<td>iotop -o</td>
<td>直接查看比较高的磁盘读写程序</td>
</tr>
<tr>
<td>5</td>
<td>netstat -tunlp | grep 端口号</td>
<td>查看端口占用情况</td>
</tr>
<tr>
<td>6</td>
<td>uptime</td>
<td>查看报告系统运行时长及平均负载</td>
</tr>
<tr>
<td>7</td>
<td>ps  -aux</td>
<td>查看进程</td>
</tr>
</tbody></table>
<p>ifconfig   查看IP地址</p>
<p>ps -aux    显示USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND</p>
<p>ps -ef | grep flume    查看进程和父进程的ID（flume进程的关闭就可以通过此方式查看进程号）</p>
<p>ps -ef 显示 UID    PID PPID C STIME TTY     TIME CMD</p>
<p><font color=red>PID是程序被操作系统加载到内存成为进程后动态分配的资源，</font>每次程序执行的时候，操作系统都会重新加载，PID在每次加载都是不同的。</p>
<p>PPID 是程序的父进程号</p>
</li>
</ul>
<h2 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h2><ul>
<li><p>框架启动关闭脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><br>case $1 in<br>&quot;start&quot;)&#123;<br>	for i in hadoop100 hadoop101 hadoop102<br>	do<br>		ssh $i &quot;启动命令，用绝对路径&quot;<br>	done<br>&#125;;;<br>&quot;stop&quot;)&#123;<br><br>&#125;;;<br>esac<br><br></code></pre></td></tr></table></figure>
</li>
<li><p>单引号和双引号的区别</p>
<ul>
<li>单引号：’$do_date’,在引号内部的变量不能解释里面变量对应的值</li>
<li>双引号：”$do_date”,在引号内部，能够取出变量的值</li>
<li>单引号，双引号嵌套：看谁在最外面</li>
<li>&#96;&#96;号表示，执行引号中的命令</li>
</ul>
</li>
</ul>
<h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><ul>
<li>Hadoop3.x常用端口<ul>
<li>namenode rpc 端口 8020    2.x是9000</li>
<li>namenode http 端口 9870    2.x是50070</li>
<li>resourcemanager webapp.address  端口8088</li>
<li>19888 历史服务器</li>
</ul>
</li>
<li>安装配置文件<ul>
<li>core-site.xml</li>
<li>yarn-site.xml</li>
<li>mapred-site.xml</li>
<li>hdfs.site.xml     zk信息在此配置</li>
<li>workers(3.x)                       slaves(2.x)</li>
</ul>
</li>
</ul>
<h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><ul>
<li>HDFS块大小2.x，3.x，默认都是128M   本地模式32M<ul>
<li>块大小主要看服务器之间的传输速率</li>
</ul>
</li>
<li>HDFS读写流程<ul>
<li>写数据流程（8步）<ul>
<li>客户端向NameNode请求写入数据</li>
<li>NameNode响应是否上传文件</li>
<li>客户端上传第一个Block的信息</li>
<li>namenode返回datanode节点（dn1,dn2,dn3）</li>
<li>客户端与datanode建立连接通道</li>
<li>datanode逐级应答</li>
<li>客户端向dn1传输数据（以Packet的形式64kb），dn1再向dn2传输，依次传输。<font color=red>dn1每传输一个packet会放入一个应答队列等待应答.</font></li>
<li>重复3-7步，传输block2</li>
</ul>
</li>
<li>读数据流程（4步）<ul>
<li>客户端向Namenode请求读数据</li>
<li>namenode返回元数据信息</li>
<li>找最近的datanode</li>
<li>datanode向客户端传输信息，以packet  的形式。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><ul>
<li>shuffle流程以及优化<ul>
<li>map方法之后，redece方法之前的数据处理过程</li>
<li>大致流程<ul>
<li>MapTask收集map()方法的K,V对，并将这些数据放入环形缓冲区中</li>
<li>当环形缓冲区(默认100m）达到80%后，会进行溢写操作，将数据反向溢写到磁盘。在环形缓冲区中会进行快速排序。如果数据阻塞了，会暂停向环形缓冲区写数据。</li>
<li>会将溢写到磁盘的文件进行归并排序，在溢写和归并的过程中，都要调用Partitioner进行分区和针对key进行排序。框架调用Partitioner分区器分区（指定分区规则），WritableComparable排序器排序（指定排序规则）。</li>
<li>ReduceTask 会根据分区号，去各个MapTask上取相应的结果分区数据。将数据写入内存</li>
<li>ReduceTask会取到同一分区来自不同MapTask的结果文件，内存不够时，也会触发溢写操作，将数据写入磁盘。</li>
<li>ReduceTask会将这些文件进行合并（归并排序）还可以将归并完成的文件看需求是否进行分组操作（GroupingComparator分组），合并成大文件后，shuffle过程结束。之后将数据按reduce 方法去处理。</li>
</ul>
</li>
<li>优化：<ul>
<li>自定义分区（减少数据倾斜）</li>
<li>可以设置环形缓冲区到90%，95%之后再进行溢写，减少溢写文件，提升速度</li>
<li>再不影响业务逻辑的情况下（如求和），可以进行Combiner操作。对MapTask的输出进行一个局部汇总</li>
<li>在MapTask中，默认一次归并10个文件，可以设置更多提高性能</li>
<li>MapTask输出可以进行压缩（snappy ,lzo），减少数据传输量,提升速度</li>
<li>ReduceTask一次默认拉取5个MapTask同一分区的数据，可以根据集群性能调整</li>
<li>ReduceTask从MapTask中拉取数据先放入内存中，可以适当提高内存，减少溢写，也增加传输速度</li>
</ul>
</li>
</ul>
</li>
<li>一般情况下1g内存   对应128M数据   比较合理</li>
<li>NM默认内存：8g（服务器调整到100g左右）</li>
<li>单个任务默认内存:8g</li>
<li>MapTask默认内存：1g</li>
<li>ReduceTask默认内存:1g</li>
<li>MapJoin:在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。mapjoin适合一张表很大，一张表很小的场景。将小表缓存<ul>
<li>在Mapper的setup阶段(重写setup方法)，将文件读取到缓存集合中</li>
<li>在驱动函数中加载缓存，缓存普通文件到Task运行节点</li>
<li>job.addCacheFile(new URL(“file:&#x2F;&#x2F;e:&#x2F;cache&#x2F;xxx.txt”))</li>
</ul>
</li>
</ul>
<h3 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h3><ul>
<li><p>Yarn是一个资源调度平台，负责为运算程序提供服务运算资源，相当于一个<font color=red>分布式的操作系统平台，</font>而MapReduce等运算则相当于运行于操作系统之上的程序。</p>
</li>
<li><p>yarn工作调度流程(8步)</p>
<ul>
<li><p>客户端执行job.waitForCompletion</p>
</li>
<li><p>客户端向RM申请运行一个Application</p>
</li>
<li><p>RM向client 返回Application的资源提交路径和job_id</p>
</li>
<li><p>client将资源(切片信息，配置信息，所需jar包)提交到HDFS上</p>
</li>
<li><p>资源提交完成，Application向RM申请运行AppMaster</p>
</li>
<li><p>AppMaster下载Client提交的资源到本地</p>
</li>
<li><p>APPMaster向RM申请多个任务资源，运行MapReduce程序</p>
</li>
<li><p>运行完成后，MR会向RM申请注销自己。</p>
</li>
</ul>
</li>
<li><p>yarn资源调度器</p>
<ul>
<li>FIFO<ul>
<li>队列形式，先进先出</li>
</ul>
</li>
<li>容量调度器(默认的资源调度器)<ul>
<li>几个并行的FIFO，当资源不紧张时，采用此方式</li>
<li>优先满足先进入的任务</li>
</ul>
</li>
<li>公平调度器 （能最大限度的利用集群资源）<ul>
<li>最大限度的利用集群资源</li>
<li>每个任务都公平的享有资源，并发度高</li>
<li>同一队列的作业按照其优先级分享整个队列的资源，并发执行</li>
<li>每个作业可以设置最小资源值，调度器会保证作业获得其以上的资源</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h3><ul>
<li>优化问题<ul>
<li>小文件问题<ul>
<li>har归档</li>
<li>使用CombineTextInputformat</li>
<li>JVM重用</li>
</ul>
</li>
<li>数据倾斜问题<ul>
<li>启用combiner ,减少数据的传输量</li>
<li>根据数据分布情况，自定义分区，将key 均匀分配到Reducer</li>
<li>重新设计key，有一种方案是在map阶段时给key加上一个随机数,有了随机数的key就不会被大量的分配到同一节点(小几率),待到reduce后再把随机数去掉即可。</li>
<li>增加Reducer，提升并行度</li>
<li>增加JVM内存</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><ul>
<li><p>Zookeeper从设计模式的角度理解，是一个基于观察者模式设计的分布式服务管理框架，<font color=red>它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应</font></p>
</li>
<li><p>Zookeeper&#x3D;文件系统+通知机制</p>
</li>
<li><p>选举机制</p>
<ul>
<li><p>奇数台</p>
<p>10台服务器：3台</p>
<p>20台服务器：5台</p>
<p>100台服务器：11台</p>
</li>
<li><p>台数不是越多越好，太多选举时间过长影响性能。</p>
</li>
<li><p>zk半数以上可以正常运行</p>
</li>
<li><p>ZooKeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目</p>
</li>
</ul>
</li>
<li><p>常用命令</p>
<ul>
<li>ls path 查看当前znode中所包含的内容</li>
<li>create 普通创建 -s含有序列  -e 临时（重启或超时消失）</li>
<li>get path 获得节点的值</li>
<li>set 设置节点的具体值</li>
<li>stat 查看节点状态</li>
</ul>
</li>
<li><p>HA</p>
<p><a href="https://imgtu.com/i/2lA1SS"><img src="https://z3.ax1x.com/2021/06/02/2lA1SS.png" alt="2lA1SS.png"></a></p>
</li>
</ul>
<h2 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h2><ul>
<li><p>架构</p>
<ul>
<li><p>flume是一个分布式的海量日志采集，聚合，和传输的系统</p>
</li>
<li><p>Flume agent 内部原理</p>
<ul>
<li><p>source接收数据，数据以event的形式传给ChannelProcessor</p>
</li>
<li><p>ChannelProcessor将事件传递给拦截器，在拦截器中可以对event数据进行更改</p>
</li>
<li><p>经过拦截器后，数据返回到ChannelProcessor，ChannelProcessor将每个event交给ChannelSelector，ChannelSelector的作用就是选出Event将要发往哪个Channel，有两种选择：分别是Replicating(复制)和Multiplexing（多路复用）</p>
</li>
<li><p>数据再返回到ChannelProcessor，根据选择器的选择，进入不同的Channel</p>
</li>
<li><p>SinkProcessor再决定channel中的event 的走向</p>
<ul>
<li><p>SinkProcessor共有三种类型，分别是DefaultSinkProcessor、LoadBalancingSinkProcessor和FailoverSinkProcessor</p>
<p>DefaultSinkProcessor对应的是单个的Sink，LoadBalancingSinkProcessor和FailoverSinkProcessor对应的是Sink Group，LoadBalancingSinkProcessor可以实现负载均衡的功能，FailoverSinkProcessor可以错误恢复的功能（故障转移）。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>event:flume传输数据的基本单位，由Header和Body组成，Header用来存放该event的一些属性，为K，V结构（Header不设置的话为空）；Body用来存放该条数据，形式为字节数组。</p>
</li>
<li><p>source</p>
<ul>
<li>Taildir Source:适用于监听多个实时追加的文件，并且能够实现断点续传。（可能会出现数据重复）</li>
<li>batchSize参数决定Source一次批量运输到Channel的event条数，适当调大这个参数可以提高Source搬运Event到Channel时的性能。</li>
</ul>
</li>
<li><p>channel</p>
<ul>
<li>File Channel  基于磁盘  可以存100万个event</li>
<li>Memory Channel 基于内存  可以存100个event</li>
<li>KafkaChannel  数据存储在Kafka里面 基于磁盘的  KafkaChannel传输性能大于MemoryChannel+KafkaSink</li>
</ul>
</li>
<li><p>sink</p>
<ul>
<li>hdfs sink<ul>
<li>针对小文件 hdfs sink 有优化的参数</li>
</ul>
</li>
</ul>
</li>
<li><p>拦截器</p>
<ul>
<li>自定义拦截器<ul>
<li>定义类实现Interceptor接口，重写四个方法（初始化，单event修改，多event修改，关闭资源） 还需要一个Builder静态内部类，实现Interceptor.Builder,在里面new一个自定义的拦截器返回。</li>
</ul>
</li>
</ul>
</li>
<li><p>选择器(Channel Selector)</p>
<ul>
<li>Multiplexing（多路复用）可以选择传到指定的Channel 上</li>
<li>Replicating(复制) event传到每个Channel上</li>
</ul>
</li>
<li><p>监控器</p>
<ul>
<li>监控资源的读写情况</li>
</ul>
</li>
<li><p>调优</p>
<ul>
<li>filechannel  能多磁盘就配置多磁盘（数据保存到不同磁盘） 能提高吞吐量</li>
<li>hdfs sink 小文件问题        设置  每个文件的滚动大小（128m）多久创建一个新的文件（1-2小时） event个数（0）设置为0表示文件的滚动与event 数量无关</li>
</ul>
</li>
</ul>
<h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><ul>
<li><p>基本信息</p>
<ul>
<li><p>Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列</p>
</li>
<li><p>架构</p>
<ul>
<li>producer:  消息生产者,向Kafak集群发消息的客户端</li>
<li>Kafka Cluster：Kafka集群</li>
<li>consumer: 消息消费者，向Kafka Broker取消息的客户端</li>
<li>consumer group:消费者组，每个消费者组由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者消费。消费者组之间互不影响，所有消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</li>
<li>broker ：Kafka集群中的每一台机器就是一个broker ，一个Kafka集群由多个broker 组成。</li>
<li>topic: 可以理解为一个消息队列，生产者和消费者面向的都是一个个topic。一台broker 可以容纳多个topic</li>
<li>partition：一个topic可以分为多个partition，每个partition都是一个有序的队列。可以实现很好的扩展性，一个非常大的topic可以分布到多个broker上。</li>
<li>replica：副本，保证数据的可靠性。即当集群中的某个节点发生故障时，为保证数据的完整性，kfak提供了副本机制，一个topic的每个partition都有若干个副本，一个leader和若干个follower</li>
<li>leader:副本的”主“，生产者发送数据的对象，以及消费者消费数据的对象都是leader </li>
<li>follower: 副本中的”从“，实时从leader中同步数据，保持和leader数据的同步，leader发生故障时，会选出某个follower成为新的leader</li>
</ul>
</li>
<li><p>分区策略</p>
<ul>
<li>分区原因（为什么将一个topic分为多个partition）：1. 方便在集群中扩展。2. 以partition为单位进行读写，可以提高并发。</li>
</ul>
</li>
<li><p>分区分配策略</p>
<ul>
<li>确定哪个partition由哪个consumer来消费</li>
<li>round-robin：当分区数大于consumer数量时，轮询分配，类似于斗地主一张一张的发牌。</li>
<li>range:类似于斗地主一次性发几张牌</li>
</ul>
</li>
</ul>
</li>
<li><p>挂了</p>
<ul>
<li>短时间  会存储在flume channel里</li>
<li>长时间  日志服务器有30天数据</li>
</ul>
</li>
<li><p>丢了</p>
<ul>
<li>ack  的几种情况（三种）</li>
<li>producer向Kafka集群发送信息是异步通信，为保证数据的可靠性，需要向producer发送ACK（acknowledgement 回信），如果producer收到ack，就进行下一轮的发送，否则重新发送数据。此时会出现三种情况</li>
<li>0  收到后还没开始写就发送，最不安全</li>
<li>1 leader写完后发送</li>
<li>all(-1) ISR中所有副本都写完后发送ack </li>
<li>ISR队列（在第三中情况下，为了防止某个follower长时间未向leader同步数据，该follower会被踢出ISR，leader故障则会重选leader）<ul>
<li>和leader保持同步的follower在ISR中,一定时间不同步会被提出ISR，时间阈值由replica.lag.time.max.ms参数设定，leader故障则重新选举leader。（Leader选举由Zookeeper来完成）</li>
<li><a href="https://imgtu.com/i/RgXWRJ"><img src="https://z3.ax1x.com/2021/07/03/RgXWRJ.png" alt="RgXWRJ.png"></a></li>
<li>当follower故障时会被临时踢出ISR，待待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该<em><strong>follower的LEO大于等于该Partition的HW</strong></em>，即follower追上leader之后，就可以重新加入ISR了。</li>
<li>leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。</li>
<li>这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复</li>
<li>At Least Once可以保证数据不丢失，但是不能保证数据不重复</li>
</ul>
</li>
</ul>
</li>
<li><p>重复了</p>
<ul>
<li>（自己解决）幂等性，事务，ack&#x3D;-1</li>
<li>（其它框架帮忙解决）数仓里面处理，</li>
</ul>
</li>
<li><p>积压了</p>
<ul>
<li>如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费者组的消费者数量，消费者数&#x3D;分区数。</li>
<li>增加下一级的消费速度</li>
</ul>
</li>
<li><p>优化</p>
<ul>
<li>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</li>
<li>将offset放在Kafka内置的一个topic中，__consumer_offsets。由50个分区。</li>
<li>Broker 参数配置，保留三天，也可以更短</li>
<li>Replica相关配置 ，默认副本一个</li>
<li>网络不稳定时，可以将网络通信延时</li>
<li>producer优化 采用压缩</li>
<li>增加kafka内存 生产环境下尽量不要超过6G</li>
</ul>
</li>
<li><p>其它</p>
<ul>
<li><p>Exactly once语义：At Least Once + 幂等性 &#x3D; Exactly Once。</p>
</li>
<li><p>幂等性就是指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条。</p>
</li>
<li><p>将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据，即At Least Once语义。</p>
</li>
<li><p>要启用幂等性，只需要将Producer的参数中enable.idompotence设置为true即可。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带Sequence Number。而Broker端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。</p>
</li>
<li><p>局限性：但是PID重启就会变化，同时不同的Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once。</p>
</li>
<li><p>要实现跨分区的Exactly Once，需要Transaction Coordinator，用于管理produce发送的信息的事务型。</p>
<ul>
<li>该<code>Transaction Coordinator</code>维护<code>Transaction Log</code>，该log存于一个内部的Topic内。由于Topic数据具有持久性，因此事务的状态也具有持久性。</li>
<li>应用程序必须提供一个稳定的（重启后不变）唯一的ID，也即<code>Transaction ID</code>。<code>Transactin ID</code>与<code>PID</code>可能一一对应。区别在于<code>Transaction ID</code>由用户提供，而<code>PID</code>是内部的实现对用户透明。</li>
<li>有了<code>Transaction ID</code>后，Kafka可保证：<ul>
<li>跨Session的数据幂等发送。当具有相同<code>Transaction ID</code>的新的Producer实例被创建且工作时，旧的且拥有相同<code>Transaction ID</code>的Producer将不再工作。</li>
<li>跨Session的事务恢复。如果某个应用实例宕机，新的实例可以保证任何未完成的旧的事务要么Commit要么Abort，使得新实例从一个正常状态开始工作。</li>
</ul>
</li>
</ul>
</li>
<li><p>Kafka高效读写的原因</p>
<ul>
<li><p>Kafka是分布式，还可以设置分区，并发度高</p>
</li>
<li><p>采用顺序读写，topic在磁盘中顺序存储</p>
</li>
<li><p>零拷贝技术</p>
<p><a href="https://imgtu.com/i/4fbbm8"><img src="https://z3.ax1x.com/2021/09/28/4fbbm8.png" alt="4fbbm8.png"></a></p>
</li>
<li><p>直接将文件从内核层拷贝到目标地址。速度更快</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Zookeeper在Kafka中的作用</p>
<ul>
<li>Kafka集群中有一个broker会被选举为Controller，<font color=red>负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。</font>Controller的管理工作是依赖于Zookeeper的。</li>
</ul>
</li>
</ul>
<h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><ul>
<li><p>组成</p>
</li>
<li><p>数据类型转换</p>
<ul>
<li><p>CAST： CAST ( expression AS data_type )</p>
</li>
<li><pre><code class="sql">select
    cast(1000 as varchar(10))
    cast(1000 as varchar)
    cast(&#39;1000&#39; as int)
<figure class="highlight mel"><table><tr><td class="code"><pre><code class="hljs mel"><br>- CONVERT： CONVERT (data_type[(length)], <span class="hljs-keyword">expression</span> [, style])<br><br>  <span class="hljs-string">``</span><span class="hljs-string">``</span>sqlite<br>  <span class="hljs-keyword">select</span> convert(datetime,<span class="hljs-string">&#x27;2017-01-01&#x27;</span>)<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p>与Mysql的区别</p>
<ul>
<li>数据量   hive擅长大数据场景     mysql擅长小数据场景（数据超过一千万条后速度就不行了）</li>
<li>速度      大数据场景下 hive快       小数据场景下  mysql 快</li>
</ul>
</li>
<li><p>内部表和外部表的区别</p>
<ul>
<li>内部表  删除后，元数据和表中的数据（原始数据）都删除了</li>
<li>外部表  删除后，只删除了元数据</li>
</ul>
</li>
<li><p>插入数据</p>
<ul>
<li>insert into :以追加数据的方式插入到表或分区，原有数据不会删除</li>
<li>insert overwrite:会覆盖表或分区中已经存在的数据</li>
</ul>
</li>
<li><p>4个by</p>
<ul>
<li>order by  全局排序（都会进入一个Reduce）很容易造成数据倾斜</li>
<li>sort by   局部排序（对每个reduce排序）</li>
<li>distribute  by   分区，按照分区进入不同的reduce</li>
<li>cluster by  当sort by 和distribute by  排序的字段相同时可以使用 cluster by 代替（默认只能是升序）</li>
</ul>
</li>
<li><p>系统函数</p>
<ul>
<li><p>常用内置函数</p>
</li>
<li><p>date_add</p>
<ul>
<li>返回日期的后n天的日期</li>
<li>select date_sub(‘2015-04-09’,4)；输出2015-04-13</li>
</ul>
</li>
<li><p>date_sub  </p>
<ul>
<li>返回日期前n天的日期</li>
<li>select date_sub(‘2015-04-09’,4);输出：2015-04-05；</li>
</ul>
</li>
<li><p>next_day </p>
<ul>
<li>返回当前时间的下一个星期几对应的日期</li>
<li>select next_day(‘2018-02-27 10:03:01’, ‘TU’); –2018-03-06</li>
<li>说明，输入日期为2-27，下个星期的周二为03-06，如果想要知道下周一的日期就是MO，周日就是SU，以此类推。</li>
<li>注意：西方都是任务周日是每周的第一天</li>
</ul>
</li>
<li><p>date_format</p>
<ul>
<li><p>‘yyyy-MM-dd HH:mm:ss’ 日期格式：年月日 时分秒</p>
</li>
<li><p>日期格式化，把字符串或者日期转成指定格式的日期</p>
</li>
<li><p>date_format(“2016-06-22”,”yyyy-MM-dd”)&#x3D;2016-06-22</p>
</li>
</ul>
</li>
<li><p>last_day</p>
<ul>
<li>返回这个月的最后一天的日期</li>
<li>select last_day(“2021-10-04”);&#x3D;2021-10-31</li>
</ul>
</li>
<li><p>解析json(get_json_object)</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> get_json_object(<span class="hljs-string">&#x27;[&#123;&quot;name&quot;:&quot;大郎&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;25&quot;&#125;,&#123;&quot;name&quot;:&quot;西门庆&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;47&quot;&#125;]&#x27;</span>,&quot;$[0].age&quot;);<br><br><span class="hljs-operator">-</span> 结果：<span class="hljs-number">25</span><br><span class="hljs-keyword">select</span> get_json_object(<span class="hljs-string">&#x27;[&#123;&quot;name&quot;:&quot;大郎&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;25&quot;&#125;,&#123;&quot;name&quot;:&quot;西门庆&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;47&quot;&#125;]&#x27;</span>,<span class="hljs-string">&#x27;$[0]&#x27;</span>);<br><br><span class="hljs-operator">-</span> 结果：&#123;&quot;name&quot;:&quot;大郎&quot;,&quot;sex&quot;:&quot;男&quot;,&quot;age&quot;:&quot;25&quot;&#125;<br><br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>自定义函数</p>
<ul>
<li>UDF  解析公共字段（定义类  继承UDF，重写evaluate 方法）</li>
<li>UDTF  解析事件字段  （定义类  继承UDTF，重写3个方法，分别是初始化（定义返回值的名称和类型），process,关闭）</li>
</ul>
</li>
<li><p>窗口函数</p>
<ul>
<li>窗口函数：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变化变化。（查询的数据会随着where,gruopby,having等条件的加入过滤掉一些数据）</li>
<li>rank()排名会重复，重复数据的话，排名数不会连续，排名数和总数一样，dense_rank() 相同的排名一样，数据重复的话排名数会比总数少。row_number() 直接排名，相同排名也不一样，排名数和总数一样。</li>
<li>over((partition by xxx order by xxx)</li>
<li>手写topn</li>
</ul>
</li>
<li><p>优化</p>
<ul>
<li><p>sql优化</p>
<ul>
<li>大表对大表：尽量减少数据集，可以通过分区表，避免扫描全表或者全字段</li>
<li>大表对小表：设置自动识别小表，将小表放入内存中去执行</li>
</ul>
</li>
<li><p>mapjoin  默认打开 ，不要关闭</p>
</li>
<li><p>先执行where再执行join,可以通过先where过滤一部分数据，再进行连接</p>
</li>
<li><p>创建分区表（天）</p>
</li>
<li><p>小文件相关处理</p>
<ul>
<li>CombinehiveInputformat &#x3D;&#x3D;&#x3D;&gt;减少切片个数，减少maptask个数</li>
<li>JVM重用</li>
<li>使用concatenate命令   alter table B partition(day&#x3D;20201224) concatenate;</li>
<li>ALTER TABLE A ARCHIVE PARTITION(dt&#x3D;’2021-05-07’, hr&#x3D;’12’);</li>
</ul>
</li>
<li><p>不影响业务逻辑的情况下可以开启map端combiner</p>
</li>
<li><p>merge 如果是map only任务，默认打开，执行完任务后，会产生大量小文件，默认会帮你开启一个job，将小于16M的文件，合并到256M。如果是mapreduce任务，需要将该功能开启</p>
</li>
<li><p>压缩</p>
</li>
<li><p>列式存储</p>
</li>
<li><p>替换引擎</p>
<ul>
<li>MR</li>
<li>TEZ</li>
<li>SPARK</li>
</ul>
</li>
<li><p>union会去重，union all 不会去重</p>
</li>
<li><p>hive数据倾斜</p>
<ul>
<li><pre><code class="kotlin">set hive.groupby.skewindata=true
生成两个MRJOB，第一个MR，先随机打散key，减少数据倾斜。第二个MR，再根据预处理的数据结果按照Group By Key分布到reduce中，最终完成z
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Java-SE"><a href="#Java-SE" class="headerlink" title="Java SE"></a>Java SE</h2><ul>
<li>HashMap的底层源码，数据结构<ul>
<li>在JDK1.8中，HashMap由数组+链表+红黑树实现</li>
<li>链表长度大于8，且表的长度大于64的时候将链表转化为红黑树</li>
<li><a href="https://imgtu.com/i/4b4dAK"><img src="https://z3.ax1x.com/2021/10/02/4b4dAK.png" alt="4b4dAK.png"></a></li>
<li>红黑树其实就是一种<strong>自平衡</strong>的二叉查找树。</li>
</ul>
</li>
<li>HashMap和HashTable<ul>
<li>线程安全性不同<ul>
<li>HashMap是线程不安全的，HashTable是线程安全的，其中的方法是Synchronize的，在多线程并发的情况下，可以直接使用HashTable，但是使用HashMap是必须自己增加同步处理。</li>
</ul>
</li>
<li>key和value是否允许null值<ul>
<li>HashTable中，key和value都不允许出现null值。HashMap中，null可以作为键，这样的键只有一个，可以有一个或多个value值为null。</li>
</ul>
</li>
<li>数组初始化和扩容机制<ul>
<li>HashTable在不指定容量的情况下默认容量为11，而HashMap为16。</li>
<li>HashTable扩容时，将容量变为原来的2倍加1，而HashMap扩容时，将容量变为原来的两倍。</li>
</ul>
</li>
</ul>
</li>
<li>StringBuffer和StingBuilder的区别<ul>
<li>StringBuffer是线程安全的，而StringBuilder是线程不安全的</li>
<li>单线程程序下，StringBuilder效率更快，因为它不需要加锁，而StringBuffer则每次都需要判断锁，效率相对更低。</li>
</ul>
</li>
<li>String和StringBuffer的区别<ul>
<li>String类是final修饰的，它是一个字符串常量，因此它一旦创建，其内容和长度不可改变，StringBuffer类它的内容和长度都是可变的。</li>
<li>String类重写了equals()方法，StringBuffer没有</li>
<li>String类对象之间可以用“+”号连接，StringBuffer对象之间不能。</li>
</ul>
</li>
<li>Final，Finally，Finalize<ul>
<li>final：修饰符（关键字）有三种用法：修饰类，变量和方法。修饰类时，意味着它不能被继承。修饰变量时，该变量使用中不被改变，必须在声明时给定初值。修饰方法时，同样只能使用，不能在子类中被重写。</li>
<li>finally：通常放在try{}catch 的后面构造最终的执行代码块。这就意味着程序无论正常执行还是发生异常，这里的代码只要JVM不关闭都能执行。</li>
<li>finalize：Object类中定义的方法，Java中允许使用finalize()方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。这个方法是由垃圾收集器在销毁对象时调用的，通过重写finalize()方法可以整理系统资源或者执行其它清理工作。</li>
</ul>
</li>
<li>&#x3D;&#x3D;和equals的区别<ul>
<li>&#x3D;&#x3D;：如果是比较基本数据类型，那么比较的是变量的值。如果比较的是引用数据类型，那么比较的是地址值（两个对象是否指向同一块内存）</li>
<li>equals：如果类没重写equals方法比较的是两个对象的地址值，如果类重写了equals方法后我们往往比较的是对象中属性的内容。</li>
<li>equals方法是从Object类中继承的，默认的实现就是使用&#x3D;&#x3D;</li>
</ul>
</li>
<li>常见执行顺序<ul>
<li>父类静态代码块——&gt;子类静态代码块——&gt;父类代码块——&gt;父类构造方法——&gt;子类代码块——&gt;子类构造方法</li>
</ul>
</li>
<li>ArrayList,HashMap扩容机制<ul>
<li>ArrayList初始化大小为0（第一次调用add 方法后长度变为10），当节点不够用时，就会扩容。扩容后的大小&#x3D;原始大小*1.5</li>
<li>HashMap:初始化大小是16，扩容因子默认为0.75（可以指定初始化大小，和扩容因子）<ul>
<li>扩容机制：当前大小和当前容量的比例超过了扩容因子，就会扩容，扩容大小增大一倍。</li>
</ul>
</li>
</ul>
</li>
<li>Java接口和抽象类<ul>
<li>含有抽象方法的类必须定义为抽象类，但抽象类中可以不包含任何抽象方法。抽象类不能被实例化，只能通过子类来实现。</li>
<li>接口将抽象进行的更加彻底，在JDK1.8中，接口中除了可以有抽象方法外，还可以有常量，默认方法和静态方法（类方法），默认方法用default修饰，静态方法用static修饰，且这两种方法都允许有方法体。</li>
</ul>
</li>
<li>volatile关键字能否保证线程安全<ul>
<li>单纯使用volatile关键字是不能保证线程安全的</li>
<li>volatile只提供了一种弱的同步机制，用来确保将变量的更新操作通知到其他线程</li>
</ul>
</li>
</ul>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><ul>
<li><p>Spark架构与作业提交流程</p>
<ul>
<li><p><a href="https://imgtu.com/i/5EtOoR"><img src="https://z3.ax1x.com/2021/10/10/5EtOoR.png" alt="5EtOoR.png"></a></p>
</li>
<li><p>Yarn Cluster模式</p>
<p><a href="https://imgtu.com/i/5ENNlT"><img src="https://z3.ax1x.com/2021/10/10/5ENNlT.png" alt="5ENNlT.png"></a></p>
<ul>
<li>脚本启动执行</li>
<li>调用Client类中的main方法并执行</li>
<li>调度器调度到该任务后，向RM申请资源运行ApplicationMaster，</li>
<li>选择一台NodeManager申请资源，启动AM</li>
<li>AM启动Driver线程执行用户的作业</li>
<li>Driver向RM申请资源</li>
<li>在NM中创建Excutor对象</li>
</ul>
</li>
</ul>
</li>
<li><p>常见算子</p>
<ul>
<li>ReduceByKey和GroupByKey的区别<ul>
<li>ReduceByKey按照Key进行聚合，会在shuffle之前有combine（预聚合）操作，返回结果是RDD[k,v]</li>
<li>GroupByKey：按照Key进行分组，直接进行shuffle</li>
<li>ReduceByKey会有预聚合的操作，所以在不影响业务逻辑的情况下优先使用ReduceByKey。</li>
</ul>
</li>
<li>Repartition和Coalesce的关系与区别<ul>
<li>关系：两者都是用来改变RDD的partition数量的，Repartition底层调用coalesce((numPartitions, shuffle &#x3D; true)</li>
<li>区别：Repartition一定会发生shuffle，coalesce根据传入的参数来判断是否发生shuffle。</li>
<li>一般增大RDD的partition数量时使用repartition，减少partition数量时使用coalesce</li>
<li>coalesce   一般用于缩减分区，默认不执行shuffle</li>
<li>reparation  一般用于扩大分区，默认执行shuffle，底层调用的就是coalesce</li>
</ul>
</li>
</ul>
<h3 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h3><ul>
<li><a href="https://imgtu.com/i/5Eyca8"><img src="https://z3.ax1x.com/2021/10/10/5Eyca8.png" alt="5Eyca8.png"></a></li>
<li>在划分stage时，最后一个stage称为finalStage，它本质上是一个ResultStage对象，前面的所有stage被称为ShuffleMapStage。</li>
<li>ShuffleMapStage的结束伴随着shuffle文件的写磁盘</li>
<li>ResultStage基本上对应代码中的action算子，即将一个函数应用在RDD的各个partition的数据集上，意味着一个job的运行结束。</li>
</ul>
</li>
<li><p>Task个数：</p>
<ul>
<li>map端的task个数等于分区数</li>
<li>reduce端的stage默认取spark.default.parallelism这个配置项的值作为分区数，如果没有配置，则以map端的最后一个RDD的分区数作为其分区数（也就是N），那么分区数就决定了reduce端的task的个数。</li>
</ul>
</li>
<li><p>Reduce端读取数据</p>
<ol>
<li><p>map task 执行完毕后会将计算状态以及磁盘小文件位置等信息封装到MapStatus对象中，然后由本进程中的MapOutPutTrackerWorker对象将mapStatus对象发送给Driver进程的MapOutPutTrackerMaster对象；</p>
</li>
<li><p><font color=red>在reduce task开始执行之前会先让本进程中的MapOutputTrackerWorker向Driver进程中的MapoutPutTrakcerMaster发动请求，请求磁盘小文件位置信息；</font></p>
</li>
<li><p><font color=red>当所有的Map task执行完毕后，Driver进程中的MapOutPutTrackerMaster就掌握了所有的磁盘小文件的位置信息。</font>此时MapOutPutTrackerMaster会告诉MapOutPutTrackerWorker磁盘小文件的位置信息；</p>
</li>
<li><p>完成之前的操作之后，由BlockTransforService去Executor0所在的节点拉数据，默认会启动五个子线程。每次拉取的数据量不能超过48M（reduce task每次最多拉取48M数据，将拉来的数据存储到Executor内存的20%内存中）。</p>
</li>
</ol>
</li>
<li><p>HashShuffle（已经弃用了）</p>
<ul>
<li>未经优化的HashShuffle<ul>
<li>对相同的key执行hash算法，从而将相同的key写入同一个文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，再溢写到磁盘文件中去。</li>
<li>下一个stage的task有多少个，当前Excutor的每个task就要创建多少份磁盘文件。如果分区数很多，则会造成大量的文件</li>
<li><a href="https://imgtu.com/i/5Ecfun"><img src="https://z3.ax1x.com/2021/10/10/5Ecfun.png" alt="5Ecfun.png"></a></li>
</ul>
</li>
<li>优化的HashShuffle<ul>
<li><a href="https://imgtu.com/i/5EgrrR"><img src="https://z3.ax1x.com/2021/10/10/5EgrrR.png" alt="5EgrrR.png"></a></li>
<li>优化后的HashShuffle，会将一个Excutor上所有task的文件都根据key分别写入到几个磁盘文件中。几个磁盘文件取决于下层task的数量。</li>
<li>优化后的Hashshuffle减少了大量的磁盘文件。</li>
</ul>
</li>
<li>不管是不是优化的hashShuffle都产生了大量的磁盘文件(都与下游的task的数量有关)。所以在高版本的spark中已经被弃用。</li>
</ul>
</li>
<li><p>SortShuffle</p>
<ul>
<li><p>SortShuffle</p>
<ul>
<li><p><a href="https://imgtu.com/i/5EfCdI"><img src="https://z3.ax1x.com/2021/10/10/5EfCdI.png" alt="5EfCdI.png"></a></p>
</li>
<li><p>在该模式下，<font color=red>数据会先写入一个内存数据结构中，</font>此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，<font color=red>每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。</font></p>
</li>
<li><p><font color=red>在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。</font>默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。</p>
</li>
<li><p><font color=red>一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。</font>此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。</p>
</li>
<li><p>SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件</p>
</li>
</ul>
</li>
<li><p>ByPassSortShuffle</p>
<ul>
<li><a href="https://imgtu.com/i/5VFM7j"><img src="https://z3.ax1x.com/2021/10/11/5VFM7j.png" alt="5VFM7j.png"></a></li>
<li>ByPassSortShuffle运行机制的触发条件（同时满足两个条件触发）<ul>
<li>不是聚合类的算子</li>
<li>shuffle上一个阶段的最后一个RDD的分区数小于参数spark.shuffle.sort.bypassMergeThreshold（默认为200）（减少中间小文件产生）</li>
</ul>
</li>
<li>此时，每个task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件中。写入磁盘文件也都是先写到内存缓存中，缓存写满后再溢写到磁盘文件中去。最后，会将所有的临时文件进行合并，合并成一个磁盘文件，并创建一个单独的索引文件。<font color=red>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因此都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。</font>因此，该机制比未经优化的HashShuffle来说shuffle read的性能会更好。</li>
<li>与普通的SortShuffle比较：<ul>
<li>磁盘写机制不一样。ByPassSortShuffle每个Task会为下游都创建一个临时磁盘文件。普通的SortShuffle临时文件是溢写出来的，与下游task数量无关。</li>
<li>不会进行排序。(因此，该机制的好处就是shuffle write过程中，不需要进行数据的排序的操作，也就节省掉了这部分的性能开销。)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Spark优化"><a href="#Spark优化" class="headerlink" title="Spark优化"></a>Spark优化</h3><ul>
<li>常规优化<ul>
<li>增加资源。增加Excutor个数，增加每个Excutor的CPU core个数，增加每个Excutor的内存量。</li>
<li>RDD持久化，RDD序列化。RDD尽可能早的filter操作。</li>
<li>并行度的调节，Spark官方推荐，task数量应该设置为Spark作业总CPU core数量的2~3倍。</li>
<li>使用广播变量，序列化</li>
</ul>
</li>
<li>算子调优<ul>
<li>mapPartitions算子对RDD中的每个分区进行操作。提高了并行度。</li>
<li>在生产环境中，通常使用foreachPartition算子来完成数据库的写入，通过foreachPartition算子的特性，可以优化写数据库的性能。</li>
<li>ReduceByKey算子map端预聚合</li>
</ul>
</li>
<li>shuffle调优<ul>
<li>调节map端内存缓冲区的大小</li>
<li>调节Reduce端拉取数据内存缓冲区大小</li>
<li>调节sortshuffle排序操作阈值，如果确定业务不需要进行排序操作，可以将此参数调大一些，大于shuffle read task的数量，那么此时map端就不会进行排序了。</li>
</ul>
</li>
<li>故障排除<ul>
<li>控制reduce端缓冲区大小避免OOM,如果缓冲区过大，导致运算的内存不足，可能会出现OOM</li>
<li>checkpoint+cache同时使用。</li>
</ul>
</li>
</ul>
<h3 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h3><ul>
<li><p>定位数据倾斜问题：</p>
<ul>
<li>查看代码中的shuffle算子，例如：distinct，reparation, reduceByKey,countByKey,GroupByKey,join等算子，根据代码逻辑判断此处是否会出现数据倾斜。</li>
<li>查看Spark作业的log文件</li>
</ul>
</li>
<li><p>解决数据倾斜：</p>
<ul>
<li><p>聚合原数据</p>
<ul>
<li>避免shuffle过程，如果避免了shuffle过程，那么就从根本上消除了发生数据倾斜问题的可能。</li>
<li>增大key粒度，比如，原来是统计每个区的一个统计量，现在将粒度增大，变成统计一个省的某些数据。</li>
</ul>
</li>
<li><p>提高reduce的并行度</p>
</li>
<li><p>使用随机key实现双重聚合</p>
<ul>
<li>当使用了类似于groupByKey，reduceByKey这样的算子时，可以考虑使用随机key实现双重聚合。</li>
<li>首先，<font color=red>通过map算子给每个数据的key添加随机前缀，对key进行打散，将原先一样的key变成不一样的key，然后进行第一次聚合，这样就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，随后，去掉每个key的前缀，再次进行聚合。</font></li>
<li>此方法仅适用于聚合类的shuffle操作</li>
<li><a href="https://imgtu.com/i/5V0IgI"><img src="https://z3.ax1x.com/2021/10/11/5V0IgI.png" alt="5V0IgI.png"></a></li>
</ul>
</li>
<li><p>将reduce join转换成map join</p>
<ul>
<li>采用广播小RDD全量数据+map算子。（RDD是不能进行广播的，只能将RDD内部的数据通过collect拉取到Driver内存然后再进行广播）</li>
<li>核心思路：<font color=red>将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key比较，如果连接key相同，那么就将两个RDD根据需求连接起来。</font></li>
<li><a href="https://imgtu.com/i/5VrVj1"><img src="https://z3.ax1x.com/2021/10/11/5VrVj1.png" alt="5VrVj1.png"></a></li>
<li>此方法适合小RDD和大RDD之间的join操作。与MapReduce中的mapjoin操作类似。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="数仓项目分层"><a href="#数仓项目分层" class="headerlink" title="数仓项目分层"></a>数仓项目分层</h2><ul>
<li>ODS<ul>
<li>保持数据原貌，（数据备份的作用）</li>
<li>采用压缩    LZO  &#x3D;&#x3D;》减少磁盘空间  100g &#x3D;&#x3D;&gt;10g</li>
<li>创建分区表  ：&#x3D;&#x3D;》防止后续计算时的全表扫描</li>
</ul>
</li>
<li>DWD<ul>
<li>数据清洗   <ul>
<li>解析数据</li>
<li>核心字段不能为空</li>
<li>过期数据清除</li>
<li>重复数据，进行过滤</li>
</ul>
</li>
<li>采用压缩，创建分区表，采用列式存储</li>
<li><strong>维度退化</strong></li>
<li><strong>维度建模</strong><ul>
<li>选择业务过程<ul>
<li>关心的事实表（下单，支付，点赞，收藏）</li>
</ul>
</li>
<li>声明粒度<ul>
<li>一行信息表示什么含义：1次，1h ,1day…..</li>
<li>根据实际情况选择，最小粒度可以统计所有指标</li>
</ul>
</li>
<li>确定维度<ul>
<li>维度：时间，地区，用户，商品，活动，优惠卷</li>
<li>维度退化（维度建模当中的星型模型，让事实表周围只有一级维度）</li>
<li>商品表+商品SPU表+商品SKU表+三级分类+二级分类+一级分类&#x3D;&#x3D;》商品表</li>
<li>省份表+地区表&#x3D;&#x3D;》地区表</li>
<li>时间表+假期表&#x3D;&#x3D;》时间表</li>
<li>活动表+活动规则表&#x3D;&#x3D;》活动表</li>
</ul>
</li>
<li>确定事实<ul>
<li>确定事实表的度量值（次数，个数，金额之类的可累加的统计值）</li>
<li>度量值的特点就是可以累加</li>
</ul>
</li>
<li>通过上面的四步，结合数仓的业务事实，得出业务总线矩阵图</li>
</ul>
</li>
</ul>
</li>
<li>DWS<ul>
<li>站在维度表的角度去看待事实，主要看事实表的度量值</li>
</ul>
</li>
<li>DWT<ul>
<li>站在维度表的角度去看待事实</li>
<li>主要关心事实表的度量值的累加值、和累积一段时间的值，还有行为的开始时间，结束时间</li>
</ul>
</li>
<li>ADS<ul>
<li>一些结果指标</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>Hadoop</tag>
        <tag>Flume</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>flink</title>
    <url>/2021/10/31/flink/</url>
    <content><![CDATA[<h1 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h1><h2 id="Flink简介"><a href="#Flink简介" class="headerlink" title="Flink简介"></a>Flink简介</h2><ul>
<li><p>Apache Flink是为分布式，高性能，随时可用以及准确的流处理应用程序打造的开源流处理框架。</p>
<p><a href="https://imgtu.com/i/IEp9K0"><img src="https://z3.ax1x.com/2021/11/03/IEp9K0.png" alt="IEp9K0.png"></a></p>
</li>
<li><p>流处理架构的演变</p>
<ul>
<li>lambda 架构（批处理+流处理 ，由twitter提出）<ul>
<li><a href="https://imgtu.com/i/ImBDQx"><img src="https://z3.ax1x.com/2021/11/04/ImBDQx.png" alt="ImBDQx.png"></a></li>
</ul>
</li>
<li>flink 流批统一（同时保证低延迟和结果正确）</li>
</ul>
</li>
</ul>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li><p>Flink的重要特点：</p>
<ul>
<li>事件型驱动（Event-driven）事件驱动型应用是一类具有状态的应用，它从一个或多个事件流提取数据，并根据到来的事件触发计算，状态更新或其他外部动作。比较典型的就是以Kafka为代表的消息队列几乎都是事件驱动型应用。</li>
</ul>
</li>
<li><p>与Spark对比：</p>
<ul>
<li>在spark的世界观中，一切都是由批次组成的，离线数据是一个大批次，而实时数据是由一个个无限的小批次组成的，最本质上来说还是批处理的。</li>
<li><strong>在Flink的世界观中，一切都是由流组成的，离线数据是有界限的流，实时数据是一个没有界限的流</strong>，这就是所谓的有界流和无界流。</li>
</ul>
</li>
<li><p>分层API</p>
<p><a href="https://imgtu.com/i/IEiil9"><img src="https://z3.ax1x.com/2021/11/03/IEiil9.png" alt="IEiil9.png"></a></p>
<ul>
<li>最底层的抽象仅仅提供了有状态流，它将通过过程函数（Process Function）被嵌入到DataStream API中，底层过程函数（Process Function）与DataStream API相集成，使其可以对某些特定的操作进行底层的抽象，它允许用户可以自由地处理来自一个或多个数据流的事件，并使用一致的容错的状态。</li>
<li>目前Flink作为批处理还不是主流，不如Spark成熟，所以DataSet使用的并不是很多。Flink Table API和Flink SQL也并不完善，大多都由各大厂商自己定制。所以我们主要学习DataStream API的使用。实际上Flink作为最接近Google DataFlow模型的实现，是流批统一的观点，所以基本上使用DataStream就可以了。</li>
</ul>
</li>
</ul>
<h2 id="快速上手"><a href="#快速上手" class="headerlink" title="快速上手"></a>快速上手</h2><ul>
<li><p>搭建maven工程，添加Scala框架</p>
<ul>
<li><p>参考pom文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">project</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span><br><span class="hljs-tag">         <span class="hljs-attr">xmlns:xsi</span>=<span class="hljs-string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span><br><span class="hljs-tag">         <span class="hljs-attr">xsi:schemaLocation</span>=<span class="hljs-string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">modelVersion</span>&gt;</span>4.0.0<span class="hljs-tag">&lt;/<span class="hljs-name">modelVersion</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>com.zt.flink<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>Flink<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.0<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br><br>    <span class="hljs-tag">&lt;<span class="hljs-name">dependencies</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.flink<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>flink-scala_2.11<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.10.0<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br>        <span class="hljs-comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-scala --&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.flink<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>flink-streaming-scala_2.11<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.10.0<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">dependencies</span>&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">build</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">plugins</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 该插件用于将Scala代码编译成class文件 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">plugin</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>net.alchim31.maven<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>scala-maven-plugin<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>3.4.6<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">executions</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">execution</span>&gt;</span><br>                <span class="hljs-comment">&lt;!-- 声明绑定到maven的compile阶段 --&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">goals</span>&gt;</span><br>                    <span class="hljs-tag">&lt;<span class="hljs-name">goal</span>&gt;</span>compile<span class="hljs-tag">&lt;/<span class="hljs-name">goal</span>&gt;</span><br>                <span class="hljs-tag">&lt;/<span class="hljs-name">goals</span>&gt;</span><br>            <span class="hljs-tag">&lt;/<span class="hljs-name">execution</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">executions</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">plugin</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">plugin</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>3.0.0<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">descriptorRefs</span>&gt;</span><br>                    <span class="hljs-tag">&lt;<span class="hljs-name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="hljs-tag">&lt;/<span class="hljs-name">descriptorRef</span>&gt;</span><br>                <span class="hljs-tag">&lt;/<span class="hljs-name">descriptorRefs</span>&gt;</span><br>            <span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">executions</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">execution</span>&gt;</span><br>                    <span class="hljs-tag">&lt;<span class="hljs-name">id</span>&gt;</span>make-assembly<span class="hljs-tag">&lt;/<span class="hljs-name">id</span>&gt;</span><br>                    <span class="hljs-tag">&lt;<span class="hljs-name">phase</span>&gt;</span>package<span class="hljs-tag">&lt;/<span class="hljs-name">phase</span>&gt;</span><br>                    <span class="hljs-tag">&lt;<span class="hljs-name">goals</span>&gt;</span><br>                        <span class="hljs-tag">&lt;<span class="hljs-name">goal</span>&gt;</span>single<span class="hljs-tag">&lt;/<span class="hljs-name">goal</span>&gt;</span><br>                    <span class="hljs-tag">&lt;/<span class="hljs-name">goals</span>&gt;</span><br>                <span class="hljs-tag">&lt;/<span class="hljs-name">execution</span>&gt;</span><br>            <span class="hljs-tag">&lt;/<span class="hljs-name">executions</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">plugin</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">plugins</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">build</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">project</span>&gt;</span><br><br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>批处理wordcount。记得导包：import org.apache.flink.api.scala._  否则会报错no implicits found for parameter evidence</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><br><span class="hljs-keyword">import</span> org.apache.flink.api.scala._<br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">PiChuLiWordCount</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-comment">// 创建执行环境</span><br>    <span class="hljs-keyword">val</span> env = <span class="hljs-type">ExecutionEnvironment</span>.getExecutionEnvironment<br>    <span class="hljs-comment">// 从文件中读取数据</span><br>    <span class="hljs-keyword">val</span> inputPath = <span class="hljs-string">&quot;E:\\idea\\flink\\src\\main\\Scala\\word.txt&quot;</span><br>    <span class="hljs-keyword">val</span> inputDS: <span class="hljs-type">DataSet</span>[<span class="hljs-type">String</span>] = env.readTextFile(inputPath)<br>    <span class="hljs-comment">// 分词之后，对单词进行groupby分组，然后用sum进行聚合</span><br>    <span class="hljs-keyword">val</span> wordCountDS: <span class="hljs-type">AggregateDataSet</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Int</span>)] =<br>      inputDS.flatMap(_.split(<span class="hljs-string">&quot; &quot;</span>))<br>          .map((_,<span class="hljs-number">1</span>))<br>          .groupBy(<span class="hljs-number">0</span>)<br>          .sum(<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment">// 打印输出</span><br>    wordCountDS.print()<br><br>  &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure>
</li>
<li><p>流处理的wordcount</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.scala._<br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">StreamWordCount</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-comment">//创建流处理环境</span><br>    <span class="hljs-keyword">val</span> env: <span class="hljs-type">StreamExecutionEnvironment</span> = <span class="hljs-type">StreamExecutionEnvironment</span>.getExecutionEnvironment<br>    <span class="hljs-comment">//接收socket 文本流</span><br>    <span class="hljs-keyword">val</span> textDstream: <span class="hljs-type">DataStream</span>[<span class="hljs-type">String</span>] = env.socketTextStream(<span class="hljs-string">&quot;hadoop100&quot;</span>, <span class="hljs-number">7777</span>)<br><br>    <span class="hljs-keyword">import</span> org.apache.flink.api.scala._<br>    <span class="hljs-keyword">val</span> dataStream: <span class="hljs-type">DataStream</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Int</span>)] = textDstream.flatMap(_.split(<span class="hljs-string">&quot;\\s&quot;</span>))<br>      .filter(_.nonEmpty)<br>      .map((_, <span class="hljs-number">1</span>))<br>      .keyBy(<span class="hljs-number">0</span>)<br>      .sum(<span class="hljs-number">1</span>)<br>    dataStream.print().setParallelism(<span class="hljs-number">1</span>);<br><br>    <span class="hljs-comment">//启动excutor ,执行任务</span><br>    env.execute(<span class="hljs-string">&quot;Socket stream word count&quot;</span>)<br><br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="Flink部署"><a href="#Flink部署" class="headerlink" title="Flink部署"></a>Flink部署</h3><ul>
<li>standalone模式<ul>
<li>解压缩,修改flink&#x2F;conf&#x2F;flink-conf.yaml文件，jobmanager.rpc.address:hadoop100</li>
<li>修改flink&#x2F;conf&#x2F;slaves文件  加上从机器的机器名hadoop101,hadoop102</li>
<li>分发flink文件</li>
</ul>
</li>
<li>Yarn模式<ul>
<li>Session-Cluster<ul>
<li><a href="https://imgtu.com/i/IEGJHK"><img src="https://z3.ax1x.com/2021/11/03/IEGJHK.png" alt="IEGJHK.png"></a></li>
</ul>
</li>
<li>Per-Job-Cluster<ul>
<li><a href="https://imgtu.com/i/IEJSDx"><img src="https://z3.ax1x.com/2021/11/03/IEJSDx.png" alt="IEJSDx.png"></a></li>
<li>Per-job模式在Flink1.15中已经被弃用</li>
</ul>
</li>
<li>application模式<ul>
<li><strong>flink-1.11</strong> 引入了一种新的部署模式，即 <strong>Application</strong> 模式。目前，flink-1.11 已经可以支持基于 Yarn 和 Kubernetes 的 Application 模式。</li>
<li><strong>Session模式</strong>：所有作业共享集群资源，隔离性差，JM 负载瓶颈，main 方法在<strong>客户端执行</strong>。</li>
<li><strong>Per-Job模式</strong>：每个作业单独启动集群，隔离性好，JM <a href="https://zhida.zhihu.com/search?q=%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1&zhida_source=entity&is_preview=1">负载均衡</a>，main 方法在<strong>客户端执行</strong>。</li>
<li>缺点：<ul>
<li>1.main方法都在客户端执行，在客户端执行 main() 方法来获取 flink 运行时所需的依赖项，并生成 JobGraph，提交到集群的操作都会在实时平台所在的机器上执行，那么将会给服务器造成很大的压力。尤其在大量用户共享客户端时，问题更加突出。</li>
<li>2.提交任务时会把本地flink的所有jar包先上传到hdfs上相应的临时目录，这个也会带来大量的网络开销，如果任务特别多的情况下，平台的吞吐量将会直线下降。</li>
</ul>
</li>
<li>Application 模式下，用户程序的 main 方法将在集群中而不是客户端运行，用户将程序逻辑和依赖打包进一个可执行的 jar 包里，集群的入口程序 (ApplicationClusterEntryPoint) 负责调用其中的 main 方法来生成 JobGraph。</li>
<li>Yarn Application 与Per-Job 模式类似，只是提交任务不需要客户端进行提交，直接由JobManager来进行任务提交，每个Flink Application对应一个Flink集群，如果该Flink Application有多个job任务，所有job任务共享该集群资源，TaskManager也是根据提交的Application所需资源情况动态进行申请。</li>
</ul>
</li>
</ul>
</li>
<li>Kubernetes部署<ul>
<li>容器化部署是是目前业界很流行的一项技术，基于Docker镜像运行能够让用户更加方便地对应用进行管理和运维。现在最流行地就是K8s了，flink也支持k8s部署模式</li>
</ul>
</li>
</ul>
<h2 id="运行架构（重点）"><a href="#运行架构（重点）" class="headerlink" title="运行架构（重点）"></a>运行架构（重点）</h2><h3 id="运行时的组件"><a href="#运行时的组件" class="headerlink" title="运行时的组件"></a>运行时的组件</h3><ul>
<li>Flink运行时架构主要包括四个不同的组件，它们会在运行流处理应用程序时协同工作:作业管理器（JobManager），资源管理器（ResourceManager）,任务管理器（TaskManager）,以及分发器（Dispatcher）。Flink是由Java和Scala实现的，因此所有的组件都会运行在Java虚拟机中。</li>
<li>作业管理器（JobManager）<ul>
<li>控制一个应用程序执行的主进程，也就是说，每个应用程序都会被一个不同的JobManager 所控制执行。JobManager会先接收到要执行的应用程序，这个应用程序包括：作业图（JobGraph）,逻辑数据流图（logical dataflow graph）和打包了所有的类，库和其他资源的JAR包。JobManager会把作业图转换成一个物理层面的数据流图，这个图被叫做”执行图“（ExecutionGraph），包含了所有可以并发执行的任务。作业管理器会向资源管理器请求执行任务必要的资源，也就是任务管理器（TaskManager）上的插槽（slot）。一旦作业管理器获取到了足够的资源，就会将执行图分发到真正运行他们的任务管理器（TaskManager）上。而在运行过程中，JobManager会负责所有需要中央协调的操作，比如检查点（checkpoint）的协调。</li>
</ul>
</li>
<li>资源管理器(ResourceManager)<ul>
<li>主要负责管理任务管理器（TaskManager）的插槽（slot），TaskManager插槽是Flink中定义的处理资源单位。Flink为不同的环境和资源管理工具提供了不同的资源管理器，比如Yarn，K8s,standalone。当JobManager申请插槽资源时，ResourceManager会将有空闲插槽的TaskManager分配给JobManager 。如果ResourceManager没有足够的插槽来满足JobManager的请求，它还可以向资源提供平台发起会话，以提供启动TaskManager进程的容器，另外，ResourceManager还负责终止空闲的TaskManager,释放计算资源。</li>
</ul>
</li>
<li>任务管理器（TaskManager）<ul>
<li>通常一个Flink中会有多个TaskManager运行，每一个TaskManager都包含了一定数量的插槽（slots）。插槽的数量限制了TaskManager能执行的任务数量。任务管理器启动之后会向资源管理器注册它的插槽；收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给任务管理器（JobManager）调用。任务管理器就可以向插槽分配任务（tasks）来执行。在执行过程中，一个任务管理器可以跟其它运行同一应用程序的TaskManager交换数据。</li>
</ul>
</li>
<li>分发器（Dispatcher）<ul>
<li>可以跨作业运行，它为应用提交提供了REST接口。当一个应用被提交执行时，分发器就会启动并将应用移交给一个JobManager。由于是REST接口，所以Dispatcher可以作为集群的一个HTTP接入点，这样就能够不受防火墙阻挡。Dispatcher也会启动一个Web UI，用来方便的展示和监控作业执行的信息。Dispatcher 在架构中可能并不是必须的，这取决于应用提交运行的方式。</li>
<li>REST接口：REST 用来规范应用如何在 HTTP 层与 API 提供方进行数据交互 。REST 描述了 HTTP 层里客户端和服务器端的数据交互规则；客户端通过向服务器端发送 HTTP（s）请求，接收服务器的响应，完成一次 HTTP 交互。这个交互过程中，REST 架构约定两个重要方面就是 HTTP 请求所采用的方法，以及请求的链接。</li>
</ul>
</li>
</ul>
<h3 id="任务提交流程"><a href="#任务提交流程" class="headerlink" title="任务提交流程"></a>任务提交流程</h3><ul>
<li>Yarn环境下任务的提交流程<ul>
<li><a href="https://imgtu.com/i/IEOkxf"><img src="https://z3.ax1x.com/2021/11/03/IEOkxf.png" alt="IEOkxf.png"></a></li>
<li>Client向HDFS上传Flink的Jar包和配置</li>
<li>Client向ResourceManager提交任务</li>
<li>ResourceManager分配Container资源并通知对应的NodeManager启动ApplicationMaster</li>
<li>ApplicationMaster启动后下载Client提交的资源和配置构建环境，启动JobManager</li>
<li>ApplicationMaster向ResourceManager申请Container资源启动TaskManager。（NodeManager加载flink的jar包和配置构建环境并启动TaskManager）</li>
<li>TaskManager启动后向JobManager发送心跳包，并等待JobManager向其分配任务</li>
</ul>
</li>
</ul>
<h3 id="任务调度原理"><a href="#任务调度原理" class="headerlink" title="任务调度原理"></a>任务调度原理</h3><ul>
<li><p>客户端（Client）不是运行时和程序执行的一部分，但它用于准备并发送dataflow(JobGraph)给Master（JobManager），然后，客户端断开连接或者维持连接以等待接收计算结果。</p>
</li>
<li><p>Client,JobManager,TaskManager三者均为独立的JVM进程。</p>
</li>
<li><p>TaskManager与Slots</p>
<ul>
<li>TaskManager是一个<strong>JVM进程</strong>，它可能会在<strong>独立的线程</strong>上执行一个或多个subtask。为了控制一个TaskManager能接收多少个task，Task Manager通过task slot来进行控制。（一个Task Manager至少有一个task slot）</li>
<li>每一个task slot表示TaskManager拥有资源的<strong>一个固定大小的子集</strong>。假如一个TaskManager有三个slot，那么他会将其管理的内存分成三份给各个slot。</li>
<li>一个TaskManager多个slot意味着更多的subtask可以共享一个JVM。</li>
<li><strong>Task Slot是静态的概念，是指TaskManager具有的并发执行的能力</strong>，可以通过参数配置，而<strong>并行度parallelism是动态概念</strong>，即TaskManager运行程序时实际使用的并发能力。可以通过参数配置</li>
</ul>
</li>
<li><p>程序与数据流</p>
<ul>
<li><p>所有flink都是由三部分组成的</p>
</li>
<li><p>Source</p>
<ul>
<li>负责读取数据源</li>
</ul>
</li>
<li><p>Transformation</p>
<ul>
<li>利用各种算子进行处理加工</li>
</ul>
</li>
<li><p>Sink</p>
<ul>
<li>负责进行输出</li>
</ul>
</li>
<li><p>在运行时，Flink 上运行的程序会被映射成”逻辑数据流“（dataflows），它包含了这三部分，<strong>每一个dataflow以一个或多个source开始以一个或多个sinks结束</strong>dataflow类似于任意的有向无环图（DAG）</p>
</li>
</ul>
</li>
<li><p>执行图</p>
<ul>
<li>由Flink程序直接映射成的数据流图是StreamGraph，也被称为逻辑流图，因为它们表示的是计算逻辑的高级视图。为了执行一个流处理程序，Flink需要将逻辑流图转换为物理数据流图（也叫执行图），详细说明程序的执行方式。</li>
<li><strong>StreamGraph</strong>：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。</li>
<li><strong>JobGraph</strong>：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化&#x2F;反序列化&#x2F;传输消耗。</li>
<li><strong>ExecutionGraph</strong>：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。</li>
<li><strong>物理执行图</strong>：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</li>
</ul>
</li>
<li><p>并行度</p>
<ul>
<li><p>在flink程序执行过程中，一个流（stream）包含一个或多个分区（stream partition），而每个算子（operator）可以包含一个或多个子任务（operator subtask）</p>
</li>
<li><p>一个特定算子的子任务（subtask）的个数称作它的并行度</p>
</li>
<li><p>stream算子之间传输数据的形式可以是one-to-one(forwarding)的模式也可以是redistributing的模式</p>
</li>
<li><p><strong>One-to-one</strong>：stream(比如在source和map operator之间)维护着分区以及元素的顺序。那意味着map 算子的子任务看到的元素的个数以及顺序跟source 算子的子任务生产的元素的个数、顺序相同，map、fliter、flatMap等算子都是one-to-one的对应关系。类似于spark中的窄依赖</p>
</li>
<li><p><strong>Redistributing</strong>：stream(map()跟keyBy&#x2F;window之间或者keyBy&#x2F;window跟sink之间)的分区会发生改变。每一个算子的子任务依据所选择的transformation发送数据到不同的目标任务。例如，keyBy() 基于hashCode重分区、broadcast和rebalance会随机重新分区，这些算子都会引起redistribute过程，而redistribute过程就类似于Spark中的shuffle过程。类似于spark中的宽依赖。</p>
</li>
</ul>
</li>
<li><p>任务链</p>
<ul>
<li>相同并行度的one to one操作，Flink这样相连的算子链接在一起形成一个task，原来的算子成为里面的一部分。将算子链接成task是非常有效的优化，它能减少线程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量。</li>
</ul>
</li>
</ul>
<h2 id="Flink-流处理API"><a href="#Flink-流处理API" class="headerlink" title="Flink 流处理API"></a>Flink 流处理API</h2><h3 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h3><ul>
<li>ExcutionEnvironment.getExcutionEnvironment</li>
<li>val env &#x3D; StreamExecutionEnvironment.createLocalEnvironment(1)</li>
<li>val env &#x3D; ExecutionEnvironment.createRemoteEnvironment(“jobmanage-hostname”, 6123,”YOURPATH&#x2F;&#x2F;wordcount.jar”)</li>
</ul>
<h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><ul>
<li>从集合读取数据</li>
<li>从文件读取数据</li>
<li>以Kafka消息队列的数据作为来源</li>
<li>自定义Source</li>
</ul>
<h3 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h3><ul>
<li>map</li>
<li>flatMap</li>
<li>Filter</li>
<li>KeyBy</li>
<li>滚动聚合算子</li>
<li>Reduce</li>
<li>Split和Select</li>
<li>Connect和CoMap</li>
<li>Union</li>
</ul>
<h3 id="支持的数据类型"><a href="#支持的数据类型" class="headerlink" title="支持的数据类型"></a>支持的数据类型</h3><ul>
<li>基础数据类型</li>
<li>Java和Scala元组</li>
<li>Scala样例类</li>
<li>Java简单对象</li>
<li>Flink对Java和Scala中的一些特殊目的的类型也都是支持的，比如Java的ArrayList，HashMap，Enum等等。</li>
</ul>
<h3 id="实现UDF函数"><a href="#实现UDF函数" class="headerlink" title="实现UDF函数"></a>实现UDF函数</h3><ul>
<li>函数类（Function Classes）</li>
<li>匿名函数（Lambda Functions）</li>
<li>富函数（Rich Functions）<ul>
<li>富函数是DataStream API提供的一个函数类接口，所有Flink函数类都有其Rich版本</li>
<li>富函数与常规函数的不同在于，可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。</li>
</ul>
</li>
</ul>
<h3 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h3><ul>
<li>Kafka</li>
<li>Redis</li>
<li>Elasticsearch</li>
<li>JDBC自定义sink</li>
</ul>
<h2 id="Flink中的Window（重点）"><a href="#Flink中的Window（重点）" class="headerlink" title="Flink中的Window（重点）"></a>Flink中的Window（重点）</h2><ul>
<li><p>flink中处理一般是分流–》开窗–》聚合 </p>
</li>
<li><p>窗口就是将无限流切割成为有限流的一种方式，它会将流数据分发到有限大小的桶中进行分析</p>
</li>
<li><p>窗口类型：时间窗口，计数窗口</p>
</li>
<li><p>时间窗口</p>
<ul>
<li>滚动时间窗口</li>
<li>滑动时间窗口</li>
<li>会话窗口（flink支持）</li>
</ul>
</li>
<li><p>计数窗口</p>
<ul>
<li>滚动计数窗口</li>
<li>滑动计数窗口</li>
</ul>
</li>
<li><p>增量聚合函数 AggregateFunction</p>
<ul>
<li>来一条数据进行一次运算，只保存一个简单的状态（累加器）</li>
<li>当窗口闭合的时候，增量聚合完成</li>
<li>处理时间：当机器时间超过窗口结束时间的时候，窗口闭合</li>
<li>来一条数据计算一次</li>
</ul>
</li>
<li><p>全窗口聚合函数 ProcessWindowFunction</p>
<ul>
<li>先把窗口所有的数据收集起来，等到计算的时候会遍历所有数据</li>
<li>可以访问到窗口信息</li>
</ul>
<p>可以将增量窗口聚合函数和全窗口聚合函数结合使用，ProcessWindowFunction提供更多对数据流的访问权限（比如窗口开始时间，窗口结束时间），但是耗性能，先用AggregateFunction处理，最后传一个结果值给ProcessWindowFunction。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.<span class="hljs-type">AggregateFunction</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.scala._<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="hljs-type">ProcessWindowFunction</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="hljs-type">Time</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="hljs-type">TimeWindow</span><br><span class="hljs-keyword">import</span> org.apache.flink.util.<span class="hljs-type">Collector</span><br><span class="hljs-comment">//最低最高温度</span><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">MinMaxTemp</span> </span>&#123;<br><br>  <span class="hljs-comment">//样例类</span><br>  <span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MinMaxTemp</span>(<span class="hljs-params">id: <span class="hljs-type">String</span>, min: <span class="hljs-type">Double</span>, max: <span class="hljs-type">Double</span>, endTs: <span class="hljs-type">Long</span></span>)</span><br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-keyword">val</span> env: <span class="hljs-type">StreamExecutionEnvironment</span> = <span class="hljs-type">StreamExecutionEnvironment</span>.getExecutionEnvironment<br>    env.setParallelism(<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">val</span> stream: <span class="hljs-type">DataStream</span>[<span class="hljs-type">SensorReading</span>] = env.addSource(<span class="hljs-keyword">new</span> <span class="hljs-type">SensorSource</span>)<br>    stream.keyBy(_.id).timeWindow(<span class="hljs-type">Time</span>.seconds(<span class="hljs-number">5</span>))<br>      .aggregate(<span class="hljs-keyword">new</span> <span class="hljs-type">HighAndLowAgg</span>, <span class="hljs-keyword">new</span> <span class="hljs-type">WindowResult</span>)<br>      .print()<br>    env.execute()<br>  &#125;<br><span class="hljs-comment">//增量聚合函数</span><br>  <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HighAndLowAgg</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">AggregateFunction</span>[<span class="hljs-type">SensorReading</span>, (<span class="hljs-type">String</span>, <span class="hljs-type">Double</span>, <span class="hljs-type">Double</span>), (<span class="hljs-type">String</span>, <span class="hljs-type">Double</span>, <span class="hljs-type">Double</span>)] </span>&#123;<br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">createAccumulator</span></span>(): (<span class="hljs-type">String</span>, <span class="hljs-type">Double</span>, <span class="hljs-type">Double</span>) = (<span class="hljs-string">&quot;&quot;</span>, <span class="hljs-type">Double</span>.<span class="hljs-type">MaxValue</span>, <span class="hljs-type">Double</span>.<span class="hljs-type">MinValue</span>)<br><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">add</span></span>(value: <span class="hljs-type">SensorReading</span>, accumulator: (<span class="hljs-type">String</span>, <span class="hljs-type">Double</span>, <span class="hljs-type">Double</span>)): (<span class="hljs-type">String</span>, <span class="hljs-type">Double</span>, <span class="hljs-type">Double</span>) =<br>      (value.id, value.temperature.min(accumulator._2), value.temperature.max(accumulator._3))<br><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getResult</span></span>(accumulator: (<span class="hljs-type">String</span>, <span class="hljs-type">Double</span>, <span class="hljs-type">Double</span>)): (<span class="hljs-type">String</span>, <span class="hljs-type">Double</span>, <span class="hljs-type">Double</span>) = accumulator<br><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge</span></span>(a: (<span class="hljs-type">String</span>, <span class="hljs-type">Double</span>, <span class="hljs-type">Double</span>), b: (<span class="hljs-type">String</span>, <span class="hljs-type">Double</span>, <span class="hljs-type">Double</span>)): (<span class="hljs-type">String</span>, <span class="hljs-type">Double</span>, <span class="hljs-type">Double</span>) =<br>      (a._1, a._2.min(b._2), a._3.max(b._3))<br>  &#125;<br><span class="hljs-comment">//全窗口聚合函数</span><br>  <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WindowResult</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">ProcessWindowFunction</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Double</span>, <span class="hljs-type">Double</span>), <span class="hljs-type">MinMaxTemp</span>, <span class="hljs-type">String</span>, <span class="hljs-type">TimeWindow</span>] </span>&#123;<br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process</span></span>(key: <span class="hljs-type">String</span>, context: <span class="hljs-type">Context</span>, elements: <span class="hljs-type">Iterable</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Double</span>, <span class="hljs-type">Double</span>)], out: <span class="hljs-type">Collector</span>[<span class="hljs-type">MinMaxTemp</span>]): <span class="hljs-type">Unit</span> = &#123;<br>      <span class="hljs-keyword">val</span> minMax: (<span class="hljs-type">String</span>, <span class="hljs-type">Double</span>, <span class="hljs-type">Double</span>) = elements.head<br>      out.collect(<span class="hljs-type">MinMaxTemp</span>(key, minMax._2, minMax._3, context.window.getEnd))<br><br>    &#125;<br>  &#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="Flink中的时间语义和Watermark（重点）"><a href="#Flink中的时间语义和Watermark（重点）" class="headerlink" title="Flink中的时间语义和Watermark（重点）"></a>Flink中的时间语义和Watermark（重点）</h2><h3 id="时间语义"><a href="#时间语义" class="headerlink" title="时间语义"></a>时间语义</h3><ul>
<li>事件时间（event time）<ul>
<li>真正正确的时间，时间语义完全正确。事件创建的事件（必须包含在数据源中的元素里面）</li>
<li>是事件创建的时间。它通常由事件中的时间戳描述，例如采集的日志数据中，每一条日志都会记录自己的生成时间，Flink通过时间戳分配器访问事件时间戳。</li>
<li>flink事件事件单位是ms，代码中需要注意</li>
</ul>
</li>
<li>机器时间<ul>
<li>Ingestion Time（摄入时间） :数据进入Flink的时间,与机器相关</li>
<li>Processing Time（处理时间）: 是每一个执行基于时间操作的算子的本地系统时间，与机器相关，默认的时间属性就是Processing Time</li>
</ul>
</li>
</ul>
<h3 id="Watermark"><a href="#Watermark" class="headerlink" title="Watermark"></a>Watermark</h3><ul>
<li>当Flink以Event Time模式处理数据流时，它会根据数据里的时间戳来处理基于时间的算子</li>
<li>由于网络，分布式等原因，会导致乱序数据的产生</li>
<li>乱序数据会让窗口计算不准确</li>
<li>窗口的区间是左闭右开区间</li>
<li>Watermark的特点<ul>
<li>水位线：系统认为时间戳小于水位线的事件都已经达到了</li>
<li>水位线是一种特殊的事件，在source算子后面插入到流中</li>
<li>事件时间窗口的闭合触发规则：水位线大于等于窗口结束时间的时候</li>
<li>水位线由程序员编程插入到流中，水位线是一个特殊的事件</li>
</ul>
</li>
<li>水位线几个重要的概念<ul>
<li>Flink默认每隔200ms（机器时间）向数据流中插入一次Watermark</li>
<li><font color=red>水位线产生的公式：水位线&#x3D;系统观察到的最大事件时间 - 最大延迟时间</font></li>
<li><font color=red>最大延迟时间由程序员自己设定</font></li>
<li>分配时间戳和水位线一定要在keyBy之前进行</li>
</ul>
</li>
<li>为了保证事件时间的正确性：Flink有三重保障<ul>
<li>watermark</li>
<li>允许迟到时间  allowedLateness()</li>
<li>侧输出流  sideOutputLateData()</li>
</ul>
</li>
</ul>
<h2 id="ProcessFunction-API-底层-API"><a href="#ProcessFunction-API-底层-API" class="headerlink" title="ProcessFunction API(底层 API)"></a>ProcessFunction API(底层 API)</h2><ul>
<li><p>之前学习的转换算子是无法访问事件的时间戳信息和水位线信息的。基于此 DataStream API提供了一系列的Low-Level转换算子。可以访问时间戳，watermark以及注册定时事件。还可以输出特定的一些事件，例如超时事件等。ProcessFunction用来构建事件驱动的应用以及实现自定义的业务逻辑（使用之前的window 函数和转换算子无法实现）。例如，Flink SQL就是使用Process Function实现的。</p>
</li>
<li><p>常用的几个</p>
<ul>
<li><strong>KeyedProcessFunction</strong>：KeyBy以后的流，没有开窗口</li>
<li>ProcessFunction：既没有分流也没有开窗</li>
<li><strong>ProcessWindowFunction</strong>：分流和开窗口以后的流</li>
<li>ProcessAllWindowFunction：没有分流，但是开了窗口的流</li>
<li><strong>CoProcessFunction</strong>：两条联合的流（connect）</li>
<li><strong>AggregateFunction</strong>：窗口的增量聚合函数</li>
<li>Trigger: 窗口聚合函数的底层实现，可以自由的控制窗口计算的时机</li>
</ul>
</li>
<li><p>常用函数的案例：</p>
<ul>
<li><p>KeyedProcessFunction</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.flink.api.common.state.<span class="hljs-type">ValueStateDescriptor</span><br><span class="hljs-keyword">import</span> org.apache.flink.api.scala.typeutils.<span class="hljs-type">Types</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.functions.<span class="hljs-type">KeyedProcessFunction</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.scala._<br><span class="hljs-keyword">import</span> org.apache.flink.util.<span class="hljs-type">Collector</span><br><br><span class="hljs-comment">//检查连续1s内温度上升</span><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">TempIncreaseAlert</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-keyword">val</span> env = <span class="hljs-type">StreamExecutionEnvironment</span>.getExecutionEnvironment<br>    env.setParallelism(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">val</span> stream = env.addSource(<span class="hljs-keyword">new</span> <span class="hljs-type">SensorSource</span>)<br>      .keyBy(_.id)<br>      .process(<span class="hljs-keyword">new</span> <span class="hljs-type">TempIncreaseAlertFunction</span>)<br><br>    stream.print()<br><br>    env.execute()<br>  &#125;<br><br>  <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TempIncreaseAlertFunction</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">KeyedProcessFunction</span>[<span class="hljs-type">String</span>, <span class="hljs-type">SensorReading</span>, <span class="hljs-type">String</span>] </span>&#123;<br>    <span class="hljs-comment">//初始化一个状态变量</span><br>    <span class="hljs-comment">//懒加载，惰性赋值</span><br>    <span class="hljs-comment">//当执行到process算子时，才会初始化，所以是懒加载</span><br>    <span class="hljs-comment">//通过配置，状态变量可以通过检查点操作，保存在hdfs里面</span><br>    <span class="hljs-comment">//当程序故障时，可以从最近一次检查点恢复</span><br>    <span class="hljs-comment">//所以要有一个名字last-temp和变量的类型（需要明确告诉flink状态变量的类型）</span><br>    <span class="hljs-comment">//状态变量只会被初始化一次，允许程序时，如果没有这个状态变量，就初始化一个，如果有这个状态变量，直接读取，是单例模式</span><br>    <span class="hljs-keyword">lazy</span> <span class="hljs-keyword">val</span> lastTemp = getRuntimeContext.getState(<br>      <span class="hljs-keyword">new</span> <span class="hljs-type">ValueStateDescriptor</span>[<span class="hljs-type">Double</span>](<span class="hljs-string">&quot;last-temp&quot;</span>, <span class="hljs-type">Types</span>.of[<span class="hljs-type">Double</span>])<br>    )<br>    <span class="hljs-comment">//用来保存报警定时器的时间戳，默认是0L</span><br>    <span class="hljs-keyword">lazy</span> <span class="hljs-keyword">val</span> timerTs = getRuntimeContext.getState(<br>      <span class="hljs-keyword">new</span> <span class="hljs-type">ValueStateDescriptor</span>[<span class="hljs-type">Long</span>](<span class="hljs-string">&quot;ts&quot;</span>, <span class="hljs-type">Types</span>.of[<span class="hljs-type">Long</span>])<br>    )<br><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">processElement</span></span>(value: <span class="hljs-type">SensorReading</span>, ctx: <span class="hljs-type">KeyedProcessFunction</span>[<span class="hljs-type">String</span>, <span class="hljs-type">SensorReading</span>, <span class="hljs-type">String</span>]#<span class="hljs-type">Context</span>, out: <span class="hljs-type">Collector</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>      <span class="hljs-comment">//获取最近一次温度，调用&#x27;.valeu()&#x27;方法</span><br>      <span class="hljs-comment">//如果来的是第一条温度，那么prevTemp是0.0</span><br>      <span class="hljs-keyword">val</span> prevTemp = lastTemp.value()<br>      <span class="hljs-comment">//将来的温度值更新到lastTemp状态变量，使用update方法</span><br>      lastTemp.update(value.temperature)<br><br>      <span class="hljs-keyword">val</span> curTimerTs = timerTs.value()<br><br>      <span class="hljs-keyword">if</span> (prevTemp == <span class="hljs-number">0.0</span> || value.temperature &lt; prevTemp) &#123;<br>        <span class="hljs-comment">//如果来的温度是第一条温度，或者来的温度小于最近一次温度</span><br>        <span class="hljs-comment">//删除报警定时器</span><br>        ctx.timerService().deleteProcessingTimeTimer(curTimerTs)<br>        <span class="hljs-comment">//情况保存定时器时间戳的状态变量，使用clear方法</span><br>        timerTs.clear()<br>      &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (value.temperature &gt; prevTemp &amp;&amp; curTimerTs == <span class="hljs-number">0</span>L) &#123;<br>        <span class="hljs-comment">//来的温度大于最近一次温度，并且我们没有注册报警定时器，因为curTimerTs等于0L</span><br>        <span class="hljs-keyword">val</span> ts = ctx.timerService().currentProcessingTime() + <span class="hljs-number">1000</span>L<br>        ctx.timerService().registerProcessingTimeTimer(ts)<br>        timerTs.update(ts)<br>      &#125;<br>    &#125;<br><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">onTimer</span></span>(timestamp: <span class="hljs-type">Long</span>, ctx: <span class="hljs-type">KeyedProcessFunction</span>[<span class="hljs-type">String</span>, <span class="hljs-type">SensorReading</span>, <span class="hljs-type">String</span>]#<span class="hljs-type">OnTimerContext</span>, out: <span class="hljs-type">Collector</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>      out.collect(<span class="hljs-string">&quot;传感器ID为&quot;</span> + ctx.getCurrentKey + <span class="hljs-string">&quot;的传感器温度连续1s上升&quot;</span>)<br>      timerTs.clear() <span class="hljs-comment">//清空定时器</span><br>    &#125;<br>  &#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure>
</li>
<li><p>ProcessFunction</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.functions.<span class="hljs-type">ProcessFunction</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.scala._<br><span class="hljs-keyword">import</span> org.apache.flink.util.<span class="hljs-type">Collector</span><br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">FreezingAlarm</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-keyword">val</span> env: <span class="hljs-type">StreamExecutionEnvironment</span> = <span class="hljs-type">StreamExecutionEnvironment</span>.getExecutionEnvironment<br>    env.setParallelism(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">val</span> stream = env.addSource(<span class="hljs-keyword">new</span> <span class="hljs-type">SensorSource</span>)<br>      <span class="hljs-comment">//没有KeyBy,没有开窗口函数</span><br>      .process(<span class="hljs-keyword">new</span> <span class="hljs-type">FreezingAlarmFunction</span>)<br>    <span class="hljs-comment">//stream.print()//打印常规输出</span><br>    <span class="hljs-comment">//打印侧输出流</span><br>    <span class="hljs-comment">//侧输出标签的名字必须是一样的</span><br>    stream.getSideOutput(<span class="hljs-keyword">new</span> <span class="hljs-type">OutputTag</span>[<span class="hljs-type">String</span>](<span class="hljs-string">&quot;freezing-alarm&quot;</span>)).print()<br>    env.execute()<br>  &#125;<br><br>  <span class="hljs-comment">//ProcessFunction处理的是没有KeyBy的流</span><br>  <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FreezingAlarmFunction</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">ProcessFunction</span>[<span class="hljs-type">SensorReading</span>, <span class="hljs-type">SensorReading</span>] </span>&#123;<br>    <span class="hljs-comment">//定义一个侧输出标签，实际上就是侧输出流的名字</span><br>    <span class="hljs-comment">//侧输出流中的元素泛型是String</span><br>    <span class="hljs-keyword">lazy</span> <span class="hljs-keyword">val</span> freezingAlarmOut = <span class="hljs-keyword">new</span> <span class="hljs-type">OutputTag</span>[<span class="hljs-type">String</span>](<span class="hljs-string">&quot;freezing-alarm&quot;</span>)<br><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">processElement</span></span>(value: <span class="hljs-type">SensorReading</span>, ctx: <span class="hljs-type">ProcessFunction</span>[<span class="hljs-type">SensorReading</span>, <span class="hljs-type">SensorReading</span>]#<span class="hljs-type">Context</span>, out: <span class="hljs-type">Collector</span>[<span class="hljs-type">SensorReading</span>]): <span class="hljs-type">Unit</span> = &#123;<br>      <span class="hljs-keyword">if</span> (value.temperature &lt; <span class="hljs-number">32.0</span>) &#123;<br>        <span class="hljs-comment">//第一个参数是侧输出标签，第二个参数是发送的数据</span><br>        ctx.output(freezingAlarmOut, value.id + <span class="hljs-string">&quot;的传感器低温报警&quot;</span>)<br>      &#125;<br>      <span class="hljs-comment">//将所以读数发送到常规输出</span><br>      out.collect(value)<br>    &#125;<br>  &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure>
</li>
<li><p>CoProcessFunction</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.flink.api.common.state.<span class="hljs-type">ValueStateDescriptor</span><br><span class="hljs-keyword">import</span> org.apache.flink.api.scala.typeutils.<span class="hljs-type">Types</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.functions.co.<span class="hljs-type">CoProcessFunction</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.scala._<br><span class="hljs-keyword">import</span> org.apache.flink.util.<span class="hljs-type">Collector</span><br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">CoProcessFunctionExample</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-keyword">val</span> env = <span class="hljs-type">StreamExecutionEnvironment</span>.getExecutionEnvironment<br>    env.setParallelism(<span class="hljs-number">1</span>)<br>    <span class="hljs-comment">//第一条流,无限流</span><br>    <span class="hljs-keyword">val</span> stream1 = env.addSource(<span class="hljs-keyword">new</span> <span class="hljs-type">SensorSource</span>)<br>    <span class="hljs-comment">//第二条流，有限流，只有一个元素，用来做开关，对Sensor_7的数据放行10s</span><br>    <span class="hljs-keyword">val</span> stream2 = env.fromElements(<br>      (<span class="hljs-string">&quot;Sensor_7&quot;</span>, <span class="hljs-number">5</span> * <span class="hljs-number">1000</span>L)<br>    )<br>    <span class="hljs-keyword">val</span> result = stream1<br>      .connect(stream2)<br>      .keyBy(_.id, _._1) <span class="hljs-comment">//on stream1.id=stream2._1</span><br>      .process(<span class="hljs-keyword">new</span> <span class="hljs-type">ReadingFilter</span>)<br><br>    result.print()<br><br>    env.execute()<br><br>  &#125;<br><br>  <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ReadingFilter</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">CoProcessFunction</span>[<span class="hljs-type">SensorReading</span>, (<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>), <span class="hljs-type">SensorReading</span>] </span>&#123;<br>    <span class="hljs-comment">//初始值为false</span><br>    <span class="hljs-keyword">lazy</span> <span class="hljs-keyword">val</span> forwardEnabled = getRuntimeContext.getState(<br>      <span class="hljs-keyword">new</span> <span class="hljs-type">ValueStateDescriptor</span>[<span class="hljs-type">Boolean</span>](<span class="hljs-string">&quot;switch&quot;</span>, <span class="hljs-type">Types</span>.of[<span class="hljs-type">Boolean</span>])<br>    )<br><br>    <span class="hljs-comment">//处理来自传感器的流数据</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">processElement1</span></span>(value: <span class="hljs-type">SensorReading</span>, ctx: <span class="hljs-type">CoProcessFunction</span>[<span class="hljs-type">SensorReading</span>, (<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>), <span class="hljs-type">SensorReading</span>]#<span class="hljs-type">Context</span>, out: <span class="hljs-type">Collector</span>[<span class="hljs-type">SensorReading</span>]): <span class="hljs-type">Unit</span> = &#123;<br>      <span class="hljs-comment">//如果开关为true，就允许数据流向下发送</span><br>      <span class="hljs-keyword">if</span> (forwardEnabled.value()) &#123;<br>        out.collect(value)<br>      &#125;<br>    &#125;<br><br>    <span class="hljs-comment">//处理来自开关流的数据</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">processElement2</span></span>(value: (<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>), ctx: <span class="hljs-type">CoProcessFunction</span>[<span class="hljs-type">SensorReading</span>, (<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>), <span class="hljs-type">SensorReading</span>]#<span class="hljs-type">Context</span>, out: <span class="hljs-type">Collector</span>[<span class="hljs-type">SensorReading</span>]): <span class="hljs-type">Unit</span> = &#123;<br>      <span class="hljs-comment">//打开开关</span><br>      forwardEnabled.update(<span class="hljs-literal">true</span>)<br>      <span class="hljs-comment">//开关元组的第二个值是放行时间</span><br>      <span class="hljs-keyword">val</span> ts = ctx.timerService().currentProcessingTime() + value._2<br>      <span class="hljs-comment">//注册一个定时器</span><br>      ctx.timerService().registerProcessingTimeTimer(ts)<br>    &#125;<br><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">onTimer</span></span>(timestamp: <span class="hljs-type">Long</span>, ctx: <span class="hljs-type">CoProcessFunction</span>[<span class="hljs-type">SensorReading</span>, (<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>), <span class="hljs-type">SensorReading</span>]#<span class="hljs-type">OnTimerContext</span>, out: <span class="hljs-type">Collector</span>[<span class="hljs-type">SensorReading</span>]): <span class="hljs-type">Unit</span> = &#123;<br>      <span class="hljs-comment">//关闭开关</span><br>      forwardEnabled.clear()<br>    &#125;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>水位线+迟到事件处理+侧输出流处理</p>
<ul>
<li>每200ms系统默认插入一个水位线</li>
<li>窗口是一个左闭右开的区间</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><br><span class="hljs-keyword">import</span> org.apache.flink.api.common.state.<span class="hljs-type">ValueStateDescriptor</span><br><span class="hljs-keyword">import</span> org.apache.flink.api.scala.typeutils.<span class="hljs-type">Types</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.<span class="hljs-type">TimeCharacteristic</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.functions.timestamps.<span class="hljs-type">BoundedOutOfOrdernessTimestampExtractor</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.scala._<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="hljs-type">ProcessWindowFunction</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="hljs-type">Time</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="hljs-type">TimeWindow</span><br><span class="hljs-keyword">import</span> org.apache.flink.util.<span class="hljs-type">Collector</span><br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">UpdateWindowResultWithLateElement</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-keyword">val</span> env = <span class="hljs-type">StreamExecutionEnvironment</span>.getExecutionEnvironment<br>    env.setParallelism(<span class="hljs-number">1</span>)<br>    <span class="hljs-comment">//设置事件时间</span><br>    env.setStreamTimeCharacteristic(<span class="hljs-type">TimeCharacteristic</span>.<span class="hljs-type">EventTime</span>)<br>    <span class="hljs-keyword">val</span> stream = env<br>      .socketTextStream(<span class="hljs-string">&quot;hadoop100&quot;</span>, <span class="hljs-number">9999</span>, &#x27;\n&#x27;)<br>      .map(line =&gt; &#123;<br>        <span class="hljs-keyword">val</span> arr = line.split(<span class="hljs-string">&quot; &quot;</span>)<br>        (arr(<span class="hljs-number">0</span>), arr(<span class="hljs-number">1</span>).toLong * <span class="hljs-number">1000</span>L)<br>      &#125;)<br>      .assignTimestampsAndWatermarks(<br>        <span class="hljs-comment">//最大延迟时间5s</span><br>        <span class="hljs-keyword">new</span> <span class="hljs-type">BoundedOutOfOrdernessTimestampExtractor</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>)](<span class="hljs-type">Time</span>.seconds(<span class="hljs-number">5</span>)) &#123;<br>          <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">extractTimestamp</span></span>(element: (<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>)): <span class="hljs-type">Long</span> = element._2<br>        &#125;<br>      )<br>      .keyBy(_._1)<br>      .timeWindow(<span class="hljs-type">Time</span>.seconds(<span class="hljs-number">5</span>))<br>      <span class="hljs-comment">//允许迟到时间</span><br>      .allowedLateness(<span class="hljs-type">Time</span>.seconds(<span class="hljs-number">5</span>))<br>      <span class="hljs-comment">//将迟到事件发送到侧输出流中去</span><br>      .process(<span class="hljs-keyword">new</span> <span class="hljs-type">UpdateWindowResult</span>)<br>    stream.print()<br>    env.execute()<br>  &#125;<br><br>  <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">UpdateWindowResult</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">ProcessWindowFunction</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>), <span class="hljs-type">String</span>, <span class="hljs-type">String</span>, <span class="hljs-type">TimeWindow</span>] </span>&#123;<br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process</span></span>(key: <span class="hljs-type">String</span>, context: <span class="hljs-type">Context</span>, elements: <span class="hljs-type">Iterable</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>)], out: <span class="hljs-type">Collector</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>      <span class="hljs-comment">//当第一次对窗口进行求值时，也就是水位线超过窗口结束时间的时候</span><br>      <span class="hljs-comment">//会第一次调用process函数</span><br>      <span class="hljs-comment">//这是isUpdate为默认值false</span><br>      <span class="hljs-comment">//窗口内初始化一个状态变量使用windowState,只对当前窗口可见</span><br>      <span class="hljs-keyword">val</span> isUpdate = context.windowState.getState(<br>        <span class="hljs-keyword">new</span> <span class="hljs-type">ValueStateDescriptor</span>[<span class="hljs-type">Boolean</span>](<span class="hljs-string">&quot;update&quot;</span>, <span class="hljs-type">Types</span>.of[<span class="hljs-type">Boolean</span>])<br>      )<br>      <span class="hljs-keyword">if</span> (!isUpdate.value()) &#123;<br>        <span class="hljs-comment">//当水位线超过窗口结束时间，第一次调用</span><br>        out.collect(<span class="hljs-string">&quot;窗口第一次求值了！元素共有&quot;</span> + elements.size + <span class="hljs-string">&quot; 个&quot;</span>)<br>        <span class="hljs-comment">//第一次调用完process后，将isUpdate赋值为true</span><br>        isUpdate.update(<span class="hljs-literal">true</span>)<br>      &#125; <span class="hljs-keyword">else</span> &#123;<br>        out.collect(<span class="hljs-string">&quot;迟到元素来了，更新的元素数量为&quot;</span> + elements.size + <span class="hljs-string">&quot; 个&quot;</span>)<br>      &#125;<br>    &#125;<br>  &#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure>
</li>
<li><p>触发器Trigger</p>
</li>
<li><p>每次调用Trigger都会产生一个TriggerResult来决定窗口接下来发生什么。TriggerResult可以取以下结果:</p>
<ul>
<li>CONTINUE：什么都不做</li>
<li>FIRE：如果window operator有ProcessWindowFunction这个参数，将会调用ProcessWindowFunction，如果窗口仅有增量聚合函数（ReduceFunction或者AggregateFunction）作为参数，那么当前的聚合结果将会被发送，窗口的state不变</li>
<li>PURGE：窗口所以内容包括窗口的元数据都将被丢弃</li>
<li>FIRE_AND_PURGE:先对窗口求值，再将窗口中的内容丢弃</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.flink.api.common.state.<span class="hljs-type">ValueStateDescriptor</span><br><span class="hljs-keyword">import</span> org.apache.flink.api.scala.typeutils.<span class="hljs-type">Types</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.<span class="hljs-type">TimeCharacteristic</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.scala._<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="hljs-type">ProcessWindowFunction</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="hljs-type">Time</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.windowing.triggers.<span class="hljs-type">Trigger</span>.<span class="hljs-type">TriggerContext</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.windowing.triggers.&#123;<span class="hljs-type">Trigger</span>, <span class="hljs-type">TriggerResult</span>&#125;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="hljs-type">TimeWindow</span><br><span class="hljs-keyword">import</span> org.apache.flink.util.<span class="hljs-type">Collector</span><br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">TriggerExample</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-keyword">val</span> env = <span class="hljs-type">StreamExecutionEnvironment</span>.getExecutionEnvironment<br>    env.setParallelism(<span class="hljs-number">1</span>)<br>    env.setStreamTimeCharacteristic(<span class="hljs-type">TimeCharacteristic</span>.<span class="hljs-type">EventTime</span>)<br>    <span class="hljs-keyword">val</span> stream = env<br>      .socketTextStream(<span class="hljs-string">&quot;hadoop100&quot;</span>, <span class="hljs-number">9999</span>, &#x27;\n&#x27;)<br>      .map(line =&gt; &#123;<br>        <span class="hljs-keyword">val</span> arr = line.split(<span class="hljs-string">&quot; &quot;</span>)<br>        (arr(<span class="hljs-number">0</span>), arr(<span class="hljs-number">1</span>).toLong)<br>      &#125;)<br>      .assignAscendingTimestamps(_._2)<br>      .keyBy(_._1)<br>      .timeWindow(<span class="hljs-type">Time</span>.seconds(<span class="hljs-number">10</span>))<br>      .trigger(<span class="hljs-keyword">new</span> <span class="hljs-type">OneSecondIntervalTrigger</span>)<br>      .process(<span class="hljs-keyword">new</span> <span class="hljs-type">WindowCount</span>)<br><br>    stream.print()<br>    env.execute()<br>  &#125;<br><br>  <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">OneSecondIntervalTrigger</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Trigger</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>), <span class="hljs-type">TimeWindow</span>] </span>&#123;<br>    <span class="hljs-comment">//每来一条元素都要调用一次</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">onElement</span></span>(element: (<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>), timestamp: <span class="hljs-type">Long</span>, window: <span class="hljs-type">TimeWindow</span>, ctx: <span class="hljs-type">TriggerContext</span>): <span class="hljs-type">TriggerResult</span> = &#123;<br>      <span class="hljs-comment">//默认值为false</span><br>      <span class="hljs-comment">//当第一条事件来的时候将firstSeen置为false</span><br>      <span class="hljs-keyword">val</span> firstSeen = ctx.getPartitionedState(<br>        <span class="hljs-keyword">new</span> <span class="hljs-type">ValueStateDescriptor</span>[<span class="hljs-type">Boolean</span>](<span class="hljs-string">&quot;first-seen&quot;</span>, <span class="hljs-type">Types</span>.of[<span class="hljs-type">Boolean</span>])<br>      )<br>      <span class="hljs-comment">//当第一条数据来的时候，！firstSeen.value()为true</span><br>      <span class="hljs-comment">//仅对第一条数据注册定时器</span><br>      <span class="hljs-comment">//这里的定时器指的是：onEventTime函数</span><br>      <span class="hljs-keyword">if</span> (!firstSeen.value()) &#123;<br>        <span class="hljs-comment">//如果当前水位线为1234，那么t=1234+(1000-1234%1000)=2000</span><br>        println(<span class="hljs-string">&quot;第一条数据进入时的水位线是：&quot;</span>+ctx.getCurrentWatermark)<br>        <span class="hljs-keyword">val</span> t = ctx.getCurrentWatermark + (<span class="hljs-number">1000</span> - (ctx.getCurrentWatermark % <span class="hljs-number">1000</span>))<br>        println(<span class="hljs-string">&quot;第一次注册的定时器时间是：&quot;</span>+ t)<br>        ctx.registerEventTimeTimer(t) <span class="hljs-comment">//第一条数据的时间戳之后的整数秒注册一个定时器</span><br>        println(<span class="hljs-string">&quot;第一次注册的窗口结束时间定时器时间是：&quot;</span>+window.getEnd)<br>        ctx.registerEventTimeTimer(window.getEnd) <span class="hljs-comment">//在窗口结束时间注册一个定时器</span><br>        firstSeen.update(<span class="hljs-literal">true</span>)<br>      &#125;<br>      <span class="hljs-type">TriggerResult</span>.<span class="hljs-type">CONTINUE</span> <span class="hljs-comment">//什么都不敢(fire=false,purge=false)</span><br>    &#125;<br><br>    <span class="hljs-comment">//我们使用的是事件事件，所以onProcessingTime什么都不用做</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">onProcessingTime</span></span>(time: <span class="hljs-type">Long</span>, window: <span class="hljs-type">TimeWindow</span>, ctx: <span class="hljs-type">TriggerContext</span>): <span class="hljs-type">TriggerResult</span> = &#123;<br>      <span class="hljs-type">TriggerResult</span>.<span class="hljs-type">CONTINUE</span><br>    &#125;<br>    <span class="hljs-comment">//定时器函数，在水位线到达time时触发</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">onEventTime</span></span>(time: <span class="hljs-type">Long</span>, window: <span class="hljs-type">TimeWindow</span>, ctx: <span class="hljs-type">TriggerContext</span>): <span class="hljs-type">TriggerResult</span> = &#123;<br>      println(<span class="hljs-string">&quot;当前水位线是&quot;</span>+ctx.getCurrentWatermark)<br>      <span class="hljs-comment">//在onElement函数中，我们注册过窗口结束时间的定时器</span><br>      <span class="hljs-keyword">if</span> (time == window.getEnd) &#123;<br>        <span class="hljs-comment">//在窗口闭合时，触发计算并清空窗口</span><br>        <span class="hljs-type">TriggerResult</span>.<span class="hljs-type">FIRE_AND_PURGE</span><br>      &#125; <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-keyword">val</span> t = ctx.getCurrentWatermark + (<span class="hljs-number">1000</span> - (ctx.getCurrentWatermark % <span class="hljs-number">1000</span>))<br>        <span class="hljs-keyword">if</span> (t &lt; window.getEnd) &#123;<br>          ctx.registerEventTimeTimer(t)<br>          println(<span class="hljs-string">&quot;注册的定时器时间是：&quot;</span>+t)<br>        &#125;<br>        <span class="hljs-type">TriggerResult</span>.<span class="hljs-type">FIRE</span><br>      &#125;<br>    &#125;<br><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">clear</span></span>(window: <span class="hljs-type">TimeWindow</span>, ctx: <span class="hljs-type">TriggerContext</span>): <span class="hljs-type">Unit</span> = &#123;<br>      <span class="hljs-comment">//状态变量是一个单例</span><br>      <span class="hljs-keyword">val</span> firstSeen = ctx.getPartitionedState(<br>        <span class="hljs-keyword">new</span> <span class="hljs-type">ValueStateDescriptor</span>[<span class="hljs-type">Boolean</span>](<span class="hljs-string">&quot;first-seen&quot;</span>, <span class="hljs-type">Types</span>.of[<span class="hljs-type">Boolean</span>])<br>      )<br><br>      firstSeen.clear()<br>    &#125;<br>  &#125;<br><br>  <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WindowCount</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">ProcessWindowFunction</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>), <span class="hljs-type">String</span>, <span class="hljs-type">String</span>, <span class="hljs-type">TimeWindow</span>] </span>&#123;<br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process</span></span>(key: <span class="hljs-type">String</span>, context: <span class="hljs-type">Context</span>, elements: <span class="hljs-type">Iterable</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>)], out: <span class="hljs-type">Collector</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>      out.collect(<span class="hljs-string">&quot;窗口中有&quot;</span> + elements.size + <span class="hljs-string">&quot;条数据！ &quot;</span> + <span class="hljs-string">&quot;窗口结束时间是&quot;</span> + context.window.getEnd)<br>    &#125;<br>  &#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="Flink状态编程和容错机制"><a href="#Flink状态编程和容错机制" class="headerlink" title="Flink状态编程和容错机制"></a>Flink状态编程和容错机制</h2><h3 id="状态编程"><a href="#状态编程" class="headerlink" title="状态编程"></a>状态编程</h3><ul>
<li><p>流式计算分为无状态编程和有状态编程两种情况。无状态计算观察每个独立事件，并根据最后一个事件输出结果。</p>
</li>
<li><p>无状态计算例子：</p>
<ul>
<li>流处理应用程序从传感器接收温度读数，并在温度超过90度时发出警告。</li>
</ul>
</li>
<li><p>有状态计算例子：</p>
<ul>
<li>所有窗口类型，例如，计算过去一小时的平均温度，就是有状态的计算</li>
<li>所有用于复杂事件处理的状态机。例如，若在一分钟内收到两个相差20度以上的温度读数，则发出警告，这就是有状态的计算。</li>
<li>流与流之间的所有关联操作，以及流与静态表或动态表之间的关联操作，都是有状态的计算。</li>
</ul>
</li>
<li><p>可以认为状态就是一个本地变量，可以被任务的业务逻辑访问</p>
</li>
<li><p>无状态流处理每次只转换一条输入记录，并且仅根据最新的输入记录输出结果，有状态流处理维护所有已经处理记录的状态值，并根据每条新输入的记录更新状态，因此输出记录反映的是综合考虑多个事件之后的结果。</p>
</li>
<li><p>在Flink中，状态始终与特定算子相关联。总的来说，有两种类型的状态：</p>
<ul>
<li>算子状态（operator state）<ul>
<li>算子状态的作用范围限定为算子任务。这意味着同一并行任务所处理的所有数据都可以访问到相同的状态，状态对于同一任务而言是共享的，算子状态不能由相同或不同算子的另一个任务访问。</li>
</ul>
</li>
<li>键控状态  (keyed state)<ul>
<li>键控状态是根据输入数据流中定义的键（key）来维护和访问的。Fink为每个键值维护一个状态实例，并将具有相同键的所有数据，都分区到同一个算子任务中，这个任务会维护和处理这个key对应的状态。当任务处理一条数据时，它会自动将状态的访问范围限定为当前数据的key。因此，具有相同key的所有数据都会访问相同的状态。Keyed State很类似于一个分布式的key-value map数据结构，只能用于KeyedStream (KeyBy算子处理之后)</li>
<li><a href="https://imgtu.com/i/IwrVfS"><img src="https://z3.ax1x.com/2021/11/11/IwrVfS.png" alt="IwrVfS.png"></a></li>
<li>Flink的Keyed State 支持以下数据类型：<ul>
<li>ValueState[T]保存单个的值，值的类型为T。<ul>
<li>get操作: ValueState.value()</li>
<li>set操作: ValueState.update(value: T)</li>
</ul>
</li>
<li>ListState[T]保存一个列表，列表里的元素的数据类型为T。基本操作如下：</li>
<li>o ListState.add(value: T)</li>
<li>ListState.addAll(values: java.util.List[T])</li>
<li>ListState.get()返回Iterable[T]</li>
<li>ListState.update(values: java.util.List[T])</li>
<li>MapState[K,V]保存Key-Value对<ul>
<li>MapState.get(key:K)</li>
<li>MapState.put(key:K,value:V)</li>
<li>MapState.contains(key:K)</li>
<li>MapState.remove(key:K)</li>
</ul>
</li>
<li>ReducingState[T]</li>
<li>AggregatingState[I,O]</li>
<li>State.clear()是清空操作</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="hljs-type">ListStateDescriptor</span>, <span class="hljs-type">ValueStateDescriptor</span>&#125;<br><span class="hljs-keyword">import</span> org.apache.flink.api.scala.typeutils.<span class="hljs-type">Types</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.functions.<span class="hljs-type">KeyedProcessFunction</span><br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.scala._<br><span class="hljs-keyword">import</span> org.apache.flink.util.<span class="hljs-type">Collector</span><br><br><span class="hljs-keyword">import</span> scala.collection.mutable.<span class="hljs-type">ListBuffer</span><br><br><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">ListStateExample</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-keyword">val</span> env = <span class="hljs-type">StreamExecutionEnvironment</span>.getExecutionEnvironment<br>    env.setParallelism(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">val</span> stream = env<br>      .addSource(<span class="hljs-keyword">new</span> <span class="hljs-type">SensorSource</span>)<br>      .filter(_.id.equals(<span class="hljs-string">&quot;Sensor_1&quot;</span>))<br>      .keyBy(_.id)<br>      .process(<span class="hljs-keyword">new</span> <span class="hljs-type">Keyed</span>)<br><br>    stream.print()<br><br>    env.execute()<br>  &#125;<br><br>  <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Keyed</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">KeyedProcessFunction</span>[<span class="hljs-type">String</span>, <span class="hljs-type">SensorReading</span>, <span class="hljs-type">String</span>] </span>&#123;<br>    <span class="hljs-keyword">lazy</span> <span class="hljs-keyword">val</span> listState = getRuntimeContext.getListState(<br>      <span class="hljs-keyword">new</span> <span class="hljs-type">ListStateDescriptor</span>[<span class="hljs-type">SensorReading</span>](<span class="hljs-string">&quot;list-state&quot;</span>, <span class="hljs-type">Types</span>.of[<span class="hljs-type">SensorReading</span>])<br>    )<br>    <span class="hljs-keyword">lazy</span> <span class="hljs-keyword">val</span> timer = getRuntimeContext.getState(<br>      <span class="hljs-keyword">new</span> <span class="hljs-type">ValueStateDescriptor</span>[<span class="hljs-type">Long</span>](<span class="hljs-string">&quot;timer&quot;</span>, <span class="hljs-type">Types</span>.of[<span class="hljs-type">Long</span>])<br>    )<br><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">processElement</span></span>(value: <span class="hljs-type">SensorReading</span>, ctx: <span class="hljs-type">KeyedProcessFunction</span>[<span class="hljs-type">String</span>, <span class="hljs-type">SensorReading</span>, <span class="hljs-type">String</span>]#<span class="hljs-type">Context</span>, out: <span class="hljs-type">Collector</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>      listState.add(value) <span class="hljs-comment">//添加value到列表状态变量中</span><br>      <span class="hljs-keyword">if</span> (timer.value() == <span class="hljs-number">0</span>L) &#123;<br>        <span class="hljs-keyword">val</span> ts = ctx.timerService().currentProcessingTime() + <span class="hljs-number">10</span> * <span class="hljs-number">1000</span>L<br>        ctx.timerService().registerProcessingTimeTimer(ts)<br>        timer.update(ts)<br>      &#125;<br>    &#125;<br><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">onTimer</span></span>(timestamp: <span class="hljs-type">Long</span>, ctx: <span class="hljs-type">KeyedProcessFunction</span>[<span class="hljs-type">String</span>, <span class="hljs-type">SensorReading</span>, <span class="hljs-type">String</span>]#<span class="hljs-type">OnTimerContext</span>, out: <span class="hljs-type">Collector</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>      <span class="hljs-comment">//不能直接对列表状态变量进行计数</span><br>      <span class="hljs-keyword">val</span> readings: <span class="hljs-type">ListBuffer</span>[<span class="hljs-type">SensorReading</span>] = <span class="hljs-keyword">new</span> <span class="hljs-type">ListBuffer</span>()<br>      <span class="hljs-comment">//隐式类型转换必须导入</span><br>      <span class="hljs-keyword">import</span> scala.collection.<span class="hljs-type">JavaConversions</span>._<br>      <span class="hljs-keyword">for</span> (r &lt;- listState.get()) &#123;<br>        readings += r;<br>      &#125;<br>      out.collect(<span class="hljs-string">&quot;当前时刻列表状态变量里面共有&quot;</span> + readings.size + <span class="hljs-string">&quot;条数据&quot;</span>)<br>      timer.clear() <span class="hljs-comment">//清空定时器</span><br>    &#125;<br>  &#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure>


</li>
<li><p>状态后端(State Backends)</p>
<ul>
<li>每传入一条数据，有状态的算子任务都会读取和更新状态</li>
<li>由于有效的状态访问对于处理数据的低延迟至关重要，因此每个并行任务都会在本地维护其状态，以确保快速的状态访问。</li>
<li>状态的存储，访问以及维护，由一个可插入的组件决定这个组件就叫做状态后端。</li>
<li>状态后端主要负责两件事情，本地状态管理，以及将检查点（checkpoint）状态写入远程存储（HDFS）。</li>
</ul>
</li>
<li><p>状态后端的选择</p>
<ul>
<li>MemoryStateBackend<ul>
<li>内存级的状态后端，会将键控状态作为内存中的对象进行管理，将他们存储在TaskManager的JVM堆上，而将checkpoint存储在JobManager的内存中</li>
<li>特定：快速，但不稳定，一般用于测试</li>
</ul>
</li>
<li>FsStateBackend<ul>
<li>将checkpoint存到远程的持久化文件系统（FileSystem）上，而对于本地状态，跟MemoryStateBackend一样，也会存储在TaskManager的JVM堆上</li>
<li>特点：同时拥有内存级的本地访问速度，和更好的容错保证</li>
</ul>
</li>
<li>RocksDBStateBackend<ul>
<li>将所有状态序列化后，存入本地的RocksDB中存储。</li>
</ul>
</li>
</ul>
</li>
<li><p>Flink的一个重大价值在于，它既保证了exactly-once，也具有低延迟和高吞吐的处理能力</p>
</li>
</ul>
<h3 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h3><ul>
<li>Flink故障恢复机制的核心就是<strong>应用状态的一致性检查点</strong></li>
<li>有状态应用的一致检查点，其实就是所有任务的状态，在某个时间点的一份拷贝（一份快照）；这个时间点，应该是所有任务都恰好处理完一个相同的输入数据的时候。</li>
<li>从检查点恢复状态过程：<ul>
<li>遇到故障之后，第一步就是重启应用</li>
<li>第二步是从checkpoint中读取状态，将状态重置，从检查点重新启动应用程序后，其内部状态与检查点完成时的状态完全相同。</li>
<li>第三步：开始消费并处理检查点到发生故障之间的所有数据</li>
</ul>
</li>
<li>这种检查点的保存和恢复机制可以为应用程序提供“精确一次”（exactly-once）的一致性，因为所有算子都会保存检查点并恢复其所有状态，这样一来所有的输入流都会被重置到检查点完成时的位置。</li>
<li>保存点：flink提供了可以自定义的镜像保存功能，就是保存点（savepoints）<ul>
<li>原则上，创建保存点使用的算法与检查点完全相同，因此保存点可以认为就是具有一些额外元数据的检查点</li>
<li>flink不会自动创建保存点，需要用户手动执行的（在程序中编写相应的代码）</li>
</ul>
</li>
</ul>
<h4 id="状态一致性"><a href="#状态一致性" class="headerlink" title="状态一致性"></a>状态一致性</h4><ul>
<li><p>状态一致性级别</p>
</li>
<li><p>flink 端到端（end-to-end）状态一致性</p>
<ul>
<li><p>内部保证 checkpoint</p>
</li>
<li><p>source端  可重设数据的读取位置   (Kafka,FileSystem)</p>
</li>
<li><p>sink 端      从故障恢复时，数据不会重复写入外部系统</p>
<ul>
<li><p>幂等写入：   幂等操作就是指：一个操作，可以重复执行很多次，但只导致一次结果更改，也就是说，后面再重复执行就不起作用了</p>
</li>
<li><p>事物写入：  具有原子性，一个事务中的一系列操作要么全部成功，要么一个都不做</p>
<ul>
<li>实现思想：构建的事务对应着checkpoint，等到checkpoint 真正完成的时候，才把所有对应的结果写入sink系统中</li>
<li>实现方式：<ul>
<li>预写日志   (宕机会导致不能实现exactly-once)</li>
<li>两阶段提交（真正实现exactly-once）外部sink系统必须要提供事务</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>不同sink的一致性保证</p>
<ul>
<li><a href="https://imgse.com/i/ppOOckT"><img src="https://s1.ax1x.com/2023/04/12/ppOOckT.png" alt="ppOOckT.png"></a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Flink-CEP"><a href="#Flink-CEP" class="headerlink" title="Flink CEP"></a>Flink CEP</h2><ul>
<li><p>一个或多个由简单事件构成的事件流通过一定的规则匹配，然后输出用户想得到的数据，满足规则的复杂事件。</p>
<ul>
<li>目标：从有序的简单事件流中发现一些高阶特征</li>
<li>输入：一个或多个由简单事件构成的事件流</li>
<li>处理：识别简单事件之间的内在联系，多个符合一定规则的简单事件构成复杂事件</li>
<li>输出：满足规则的复杂事件</li>
</ul>
</li>
<li><p>CEP用于分析低延迟、频繁产生的不同来源的事件流。CEP可以帮助在复杂的、不相关的事件流中找出有意义的模式和复杂的关系，以接近实时或准实时的获得通知并阻止一些行为。</p>
<p>CEP支持在流上进行模式匹配，根据模式的条件不同，分为连续的条件或不连续的条件；模式的条件允许有时间的限制，当在条件范围内没有达到满足的条件时，会导致模式匹配超时。</p>
</li>
<li><p>功能</p>
<ul>
<li>输入的流数据，尽快产生结果</li>
<li>在2个event流上，基于时间进行聚合类的计算</li>
<li>提供实时&#x2F;准实时的警告和通知</li>
<li>在多样的数据源中产生关联并分析模式</li>
<li>高吞吐、低延迟的处理</li>
</ul>
</li>
</ul>
<h2 id="Flink-SQL"><a href="#Flink-SQL" class="headerlink" title="Flink SQL"></a>Flink SQL</h2><ul>
<li>SQL的几个优点<ul>
<li>声明式（Declarative）</li>
<li>自动调优（Optimized）</li>
<li>易于理解（Understandable）</li>
<li>稳定（Stable）</li>
<li>流与批的统一（Unify）</li>
</ul>
</li>
<li>Flink SQL的流与批统一总结起来就一句话：One Query，One Result</li>
<li><a href="!https://nightlies.apache.org/flink/flink-docs-release-1.20/zh/docs/dev/table/sql/queries/select/">官方文档</a></li>
</ul>
<h3 id="Flink-SQL核心概念"><a href="#Flink-SQL核心概念" class="headerlink" title="Flink SQL核心概念"></a>Flink SQL核心概念</h3><ul>
<li>动态表&amp;流表二像性<ul>
<li>传统的SQL是定义在表上的，为了能在流上定义SQL，我们也需要一个表的概念。这里就需要引入一个非常重要的概念：动态表（Dynamic Table）。所谓动态表，就是数据会随着时间变化的表，可以想象成就是数据库中一张被不断更新的表。我们发现流与表有非常紧密的关系，流表可以看作动态表，动态表可以看做流。我们称之为流表二象性。<ul>
<li><a href="https://imgtu.com/i/TrLqK0"><img src="https://s4.ax1x.com/2021/12/28/TrLqK0.png" alt="TrLqK0.png"></a></li>
</ul>
</li>
</ul>
</li>
<li>连续查询<ul>
<li>动态表是流的另一种表现形式，有了动态表后，我们就可以在流上定义SQL了，流式SQL可以想象成连续查询（Continous Query）。传统的查询是只运行一次的SQL，产生一个结果表就结束了。连续查询会一直运行在那里，当每个数据到来，都会持续增量地更新计算结果，从而产生另一个动态表。而这个结果动态表（也就是流）会作为另一个SQL（连续查询）的输入接着计算，从而串起整个数据流图。</li>
<li><a href="https://imgtu.com/i/TrOOSA"><img src="https://s4.ax1x.com/2021/12/28/TrOOSA.png" alt="TrOOSA.png"></a></li>
</ul>
</li>
</ul>
<h3 id="Flink-SQL核心功能"><a href="#Flink-SQL核心功能" class="headerlink" title="Flink SQL核心功能"></a>Flink SQL核心功能</h3><ul>
<li><a href="https://imgtu.com/i/TrXKkF"><img src="https://s4.ax1x.com/2021/12/28/TrXKkF.png" alt="TrXKkF.png"></a></li>
<li>高级功能<ul>
<li>双流join<ul>
<li>双流join功能是将两条流进行关联，用来补齐流上的字段。双流join又分为无限流的双流JOIN和带窗口的双流JOIN</li>
</ul>
</li>
<li>维表join<ul>
<li>维表JOIN功能是流与表的关联，也是用来为数据流补齐字段，只是补齐的维表字段是在外部存储的维表中的。</li>
</ul>
</li>
<li>TopN<ul>
<li>全局TopN，分组TopN</li>
</ul>
</li>
<li>Window<ul>
<li>支持滚动窗口，滑动窗口，会话窗口，以及传统数据库中的OVER窗口</li>
</ul>
</li>
<li>多路输入，多路输出</li>
<li>MiniBatch 优化<ul>
<li>对于有状态的算子，每个进入算子的元素都需要对状态做序列化&#x2F;反序列化的操作，频繁的状态序列化&#x2F;反序列化操作占了性能开销的大半。MiniBatch的核心思想是，对进入算子的元素进行攒批，一批数据只需要对状态序列化&#x2F;反序列化一次即可，极大地提升了性能。</li>
</ul>
</li>
<li>Retraction 撤回机制<ul>
<li>撤回机制是Flink SQL中一个非常重要地基石，它解决了early-fire导致的结果正确性问题（所有的GroupBy都是early-fire的）。而利用好撤回机制有时候能够很巧妙地帮助业务解决一些特殊需求。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Flink-SQL-维表JOIN与异步优化"><a href="#Flink-SQL-维表JOIN与异步优化" class="headerlink" title="Flink SQL 维表JOIN与异步优化"></a>Flink SQL 维表JOIN与异步优化</h3><ul>
<li><p>维表JOIN语法</p>
<ul>
<li>假设我们有一个Orders订单数据流，希望根据产品ID补全流上地产品维度信息，所以需要跟Products维度表进行关联。Orders 和 Products的DDL声明语句如下：</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> Orders (<br>  orderId <span class="hljs-type">VARCHAR</span>,          <span class="hljs-comment">-- 订单 id</span><br>  productId <span class="hljs-type">VARCHAR</span>,        <span class="hljs-comment">-- 产品 id</span><br>  units <span class="hljs-type">INT</span>,                <span class="hljs-comment">-- 购买数量</span><br>  orderTime <span class="hljs-type">TIMESTAMP</span>       <span class="hljs-comment">-- 下单时间</span><br>) <span class="hljs-keyword">with</span> (<br>  type <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;tt&#x27;</span>,              <span class="hljs-comment">-- tt 日志流</span><br>  ...<br>)<br><br><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> Products (<br>  productId <span class="hljs-type">VARCHAR</span>,        <span class="hljs-comment">-- 产品 id</span><br>  name <span class="hljs-type">VARCHAR</span>,             <span class="hljs-comment">-- 产品名称</span><br>  unitPrice <span class="hljs-keyword">DOUBLE</span>          <span class="hljs-comment">-- 单价</span><br>  <span class="hljs-keyword">PERIOD</span> <span class="hljs-keyword">FOR</span> <span class="hljs-built_in">SYSTEM_TIME</span>,   <span class="hljs-comment">-- 这是一张随系统时间而变化的表，用来声明维表</span><br>  <span class="hljs-keyword">PRIMARY</span> KEY (productId)   <span class="hljs-comment">-- 维表必须声明主键</span><br>) <span class="hljs-keyword">with</span> (<br>  type <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;alihbase&#x27;</span>,        <span class="hljs-comment">-- HBase 数据源</span><br>  ...<br>)<br><br><span class="hljs-comment">-- JOIN当前维表</span><br><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> Orders <span class="hljs-keyword">AS</span> o<br>[<span class="hljs-keyword">LEFT</span>] <span class="hljs-keyword">JOIN</span> Products <span class="hljs-keyword">FOR</span> <span class="hljs-built_in">SYSTEM_TIME</span> <span class="hljs-keyword">AS</span> <span class="hljs-keyword">OF</span> PROCTIME() <span class="hljs-keyword">AS</span> p<br><span class="hljs-keyword">ON</span> o.productId <span class="hljs-operator">=</span> p.productId<br><br><br><span class="hljs-comment">-- Flink SQL支持LEFT JOIN和INNER JOIN的维表关联。只是Products维表后面需要跟上FOR SYSTEM_TIME AS OF PROCTIME()的关键字，其含义是每条到达的数据所关联上的是到达时刻的维表快照，也就是说，当数据到达时，我们会根据数据上的Key去查询远程数据库，拿到匹配的结果后关联输出。这里的PROCTIME()即processing time。使用JOIN当前维表功能需要注意的是，如果维表插入了一条数据能匹配上之前左表的数据时，JOIN的结果流，不会发出更新的数据以弥补之前的未匹配。JOIN行为只发生在处理时间（processing time）,即使维表中的数据都被删了，之前JOIN流已经发出的关联上的数据也不会被撤回或改变。</span><br></code></pre></td></tr></table></figure>


</li>
<li><p>JOIN当前维表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> Orders <span class="hljs-keyword">AS</span> o<br>[<span class="hljs-keyword">LEFT</span>] <span class="hljs-keyword">JOIN</span> Products <span class="hljs-keyword">FOR</span> <span class="hljs-built_in">SYSTEM_TIME</span> <span class="hljs-keyword">AS</span> <span class="hljs-keyword">OF</span> PROCTIME() <span class="hljs-keyword">AS</span> p<br><span class="hljs-keyword">ON</span> o.productId <span class="hljs-operator">=</span> p.productId<br></code></pre></td></tr></table></figure>

<ul>
<li>Flink SQL支持LEFT JOIN和INNER JOIN的维表关联。只是Products维表后面需要跟上<strong>FOR SYSTEM_TIME AS OF PROCTIME()<strong>的关键字，其含义是每条到达的数据所关联上的是到达时刻的维表快照，也就是说，当数据到达时，我们会根据数据上的Key去查询远程数据库，拿到匹配的结果后关联输出。这里的</strong>PROCTIME()<strong>即</strong>processing time</strong>。使用JOIN当前维表功能需要注意的是，如果维表插入了一条数据能匹配上之前左表的数据时，JOIN的结果流，不会发出更新的数据以弥补之前的未匹配。JOIN行为只发生在处理时间（processing time）,即使维表中的数据都被删了，之前JOIN流已经发出的关联上的数据也不会被撤回或改变。</li>
</ul>
</li>
<li><p>JOIN历史维表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> Orders <span class="hljs-keyword">AS</span> o<br>[<span class="hljs-keyword">LEFT</span>] <span class="hljs-keyword">JOIN</span> Products <span class="hljs-keyword">FOR</span> <span class="hljs-built_in">SYSTEM_TIME</span> <span class="hljs-keyword">AS</span> <span class="hljs-keyword">OF</span> o.orderTime <span class="hljs-keyword">AS</span> p<br><span class="hljs-keyword">ON</span> o.productId <span class="hljs-operator">=</span> p.productId<br></code></pre></td></tr></table></figure>

<ul>
<li>有时候想关联上的维度数据，并不是当前时刻的值，而是某个时刻的值。比如，产品的价格一直在发生变化，订单流希望补全的是下单时的价格，而不是当前的价格，那就是JOIN历史维表。语法上只需要将上文的PROCTIME()改成o.orderTime即可。</li>
</ul>
</li>
<li><p>Flink 在获取维度数据时，会根据左流的时间去查对应时刻的快照数据。因此JOIN历史维表需要外部存储支持多版本存储，如HBase，或者存储的数据中带有多版本信息。</p>
</li>
<li><p>维表优化</p>
<ul>
<li>提高吞吐。维表的IO请求严重阻塞了数据流的计算处理。解决办法：异步IO，原始的维表JOIN是同步访问的方式，来一条数据，去数据库查询一次，等待返回后输出关联结果。可以发现网络等待时间极大地阻碍了吞吐和延迟。为了解决同步访问地问题，异步模式可以并发地处理多个请求和回复，从而连续地请求之间不需要阻塞等待。</li>
<li>降低维表数据库读压力。如HBase也只能承受单机每秒20万的读请求。解决办法：进行缓存，LRU和ALL（维表较小，可以将整个维表缓存到本地）</li>
<li>通过 <code>cache=&#39;LRU&#39;</code>参数可以开启 LRU 缓存优化</li>
<li>ALL cache 可以通过 <code>cache=&#39;ALL&#39;</code>参数开启</li>
<li>通过<code>cacheTTLMs</code>控制缓存的刷新间隔。</li>
</ul>
</li>
</ul>
<h3 id="Flink-sql-支持的语法"><a href="#Flink-sql-支持的语法" class="headerlink" title="Flink sql 支持的语法"></a>Flink sql 支持的语法</h3><ul>
<li><pre><code class="sql">insert:
  INSERT INTO tableReference
  query

query:
  values
  | &#123;
      select
      | selectWithoutFrom
      | query UNION [ ALL ] query
      | query EXCEPT query
      | query INTERSECT query
    &#125;
    [ ORDER BY orderItem [, orderItem ]* ]
    [ LIMIT &#123; count | ALL &#125; ]
    [ OFFSET start &#123; ROW | ROWS &#125; ]
    [ FETCH &#123; FIRST | NEXT &#125; [ count ] &#123; ROW | ROWS &#125; ONLY]

orderItem:
  expression [ ASC | DESC ]

select:
  SELECT [ ALL | DISTINCT ]
  &#123; * | projectItem [, projectItem ]* &#125;
  FROM tableExpression
  [ WHERE booleanExpression ]
  [ GROUP BY &#123; groupItem [, groupItem ]* &#125; ]
  [ HAVING booleanExpression ]
  [ WINDOW windowName AS windowSpec [, windowName AS windowSpec ]* ]

selectWithoutFrom:
  SELECT [ ALL | DISTINCT ]
  &#123; * | projectItem [, projectItem ]* &#125;

projectItem:
  expression [ [ AS ] columnAlias ]
  | tableAlias . *

tableExpression:
  tableReference [, tableReference ]*
  | tableExpression [ NATURAL ] [ LEFT | RIGHT | FULL ] JOIN tableExpression [ joinCondition ]

joinCondition:
  ON booleanExpression
  | USING &#39;(&#39; column [, column ]* &#39;)&#39;

tableReference:
  tablePrimary
  [ [ AS ] alias [ &#39;(&#39; columnAlias [, columnAlias ]* &#39;)&#39; ] ]

tablePrimary:
  [ TABLE ] [ [ catalogName . ] schemaName . ] tableName
  | LATERAL TABLE &#39;(&#39; functionName &#39;(&#39; expression [, expression ]* &#39;)&#39; &#39;)&#39;
  | UNNEST &#39;(&#39; expression &#39;)&#39;

values:
  VALUES expression [, expression ]*

groupItem:
  expression
  | &#39;(&#39; &#39;)&#39;
  | &#39;(&#39; expression [, expression ]* &#39;)&#39;
  | CUBE &#39;(&#39; expression [, expression ]* &#39;)&#39;
  | ROLLUP &#39;(&#39; expression [, expression ]* &#39;)&#39;
  | GROUPING SETS &#39;(&#39; groupItem [, groupItem ]* &#39;)&#39;

windowRef:
    windowName
  | windowSpec

windowSpec:
    [ windowName ]
    &#39;(&#39;
    [ ORDER BY orderItem [, orderItem ]* ]
    [ PARTITION BY expression [, expression ]* ]
    [
        RANGE numericOrIntervalExpression &#123;PRECEDING&#125;
      | ROWS numericExpression &#123;PRECEDING&#125;
    ]
    &#39;)&#39;
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><br>  <br><br>#### <span class="hljs-keyword">Group</span> <span class="hljs-keyword">Window</span><br><br><span class="hljs-operator">*</span> 根据窗口数据划分的不同，目前Flink有如下三种Bounded <span class="hljs-keyword">Window</span><br><br>  <span class="hljs-operator">-</span> Tumble,滚动窗口，窗口数据有固定的大小，窗口数据无叠加<br><br>    ````sqlite<br>    <span class="hljs-keyword">SELECT</span> <br>        [gk],<br>        [TUMBLE_START(timeCol, size)], <br>        [TUMBLE_END(timeCol, size)], <br>        agg1(col1), <br>        ... <br>        aggn(colN)<br>    <span class="hljs-keyword">FROM</span> Tab1<br>    <span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> [gk], TUMBLE(timeCol, size)<br>    <br>    <span class="hljs-comment">/*</span><br><span class="hljs-comment">    [gk] 决定了是否需要按照字段进行聚合；</span><br><span class="hljs-comment">    TUMBLE_START 代表窗口开始时间；</span><br><span class="hljs-comment">    TUMBLE_END 代表窗口结束时间；</span><br><span class="hljs-comment">    timeCol 是流表中表示时间字段；</span><br><span class="hljs-comment">    size 表示窗口的大小，如 秒、分钟、小时、天。</span><br><span class="hljs-comment">    */</span><br>    <span class="hljs-comment">-- 举个例子，假如我们要计算每个人每天的订单量，按照 user 进行聚合分组：</span><br>    <br>    <span class="hljs-keyword">SELECT</span> <span class="hljs-keyword">user</span>, <br>    TUMBLE_START(rowtime, <span class="hljs-type">INTERVAL</span> ‘<span class="hljs-number">1</span>’ <span class="hljs-keyword">DAY</span>) <span class="hljs-keyword">as</span> wStart, <br>    <span class="hljs-built_in">SUM</span>(amount) <br>    <span class="hljs-keyword">FROM</span> Orders <br>    <span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> TUMBLE(rowtime, <span class="hljs-type">INTERVAL</span> ‘<span class="hljs-number">1</span>’ <span class="hljs-keyword">DAY</span>), <span class="hljs-keyword">user</span>;<br></code></pre></td></tr></table></figure>

  

- Hop,滑动窗口，窗口数据有固定大小，有固定的窗口重建频率，窗口数据有叠加

  <figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <br>    [gk], <br>    [HOP_START(timeCol, slide, size)] ,  <br>    [HOP_END(timeCol, slide, size)],<br>    agg1(col1), <br>    ... <br>    aggN(colN) <br><span class="hljs-keyword">FROM</span> Tab1<br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> [gk], HOP(timeCol, slide, size)<br><br><span class="hljs-comment">/*</span><br><span class="hljs-comment">[gk] 决定了是否需要按照字段进行聚合；</span><br><span class="hljs-comment">HOP_START 表示窗口开始时间；</span><br><span class="hljs-comment">HOP_END 表示窗口结束时间；</span><br><span class="hljs-comment">timeCol 表示流表中表示时间字段；</span><br><span class="hljs-comment">slide 表示每次窗口滑动的大小；</span><br><span class="hljs-comment">size 表示整个窗口的大小，如 秒、分钟、小时、天。</span><br><span class="hljs-comment">*/</span><br><span class="hljs-comment">-- 我们要每过一小时计算一次过去 24 小时内每个商品的销量：</span><br><span class="hljs-keyword">SELECT</span> product, <br><span class="hljs-built_in">SUM</span>(amount) <br><span class="hljs-keyword">FROM</span> Orders <br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> HOP(rowtime, <span class="hljs-type">INTERVAL</span> <span class="hljs-string">&#x27;1&#x27;</span> <span class="hljs-keyword">HOUR</span>, <span class="hljs-type">INTERVAL</span> <span class="hljs-string">&#x27;1&#x27;</span> <span class="hljs-keyword">DAY</span>), product<br></code></pre></td></tr></table></figure>

  

- Session, 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加

  <figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <br>    [gk], <br>    SESSION_START(timeCol, gap) <span class="hljs-keyword">AS</span> winStart,  <br>    SESSION_END(timeCol, gap) <span class="hljs-keyword">AS</span> winEnd,<br>    agg1(col1),<br>     ... <br>    aggn(colN)<br><span class="hljs-keyword">FROM</span> Tab1<br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> [gk], SESSION(timeCol, gap)<br><br><span class="hljs-comment">/*</span><br><span class="hljs-comment">[gk] 决定了是否需要按照字段进行聚合；</span><br><span class="hljs-comment">SESSION_START 表示窗口开始时间；</span><br><span class="hljs-comment">SESSION_END 表示窗口结束时间；</span><br><span class="hljs-comment">timeCol 表示流表中表示时间字段；</span><br><span class="hljs-comment">gap 表示窗口数据非活跃周期的时长。</span><br><span class="hljs-comment">*/</span><br><span class="hljs-comment">-- 例如，我们需要计算每个用户访问时间 12 小时内的订单量：</span><br><span class="hljs-keyword">SELECT</span> <br><span class="hljs-keyword">user</span>, <br>SESSION_START(rowtime, <span class="hljs-type">INTERVAL</span> ‘<span class="hljs-number">12</span>’ <span class="hljs-keyword">HOUR</span>) <span class="hljs-keyword">AS</span> sStart, <br>SESSION_ROWTIME(rowtime, <span class="hljs-type">INTERVAL</span> ‘<span class="hljs-number">12</span>’ <span class="hljs-keyword">HOUR</span>) <span class="hljs-keyword">AS</span> sEnd, <br><span class="hljs-built_in">SUM</span>(amount) <br><span class="hljs-keyword">FROM</span> Orders <br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> SESSION(rowtime, <span class="hljs-type">INTERVAL</span> ‘<span class="hljs-number">12</span>’ <span class="hljs-keyword">HOUR</span>), <span class="hljs-keyword">user</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<h3 id="官方文档案例记录"><a href="#官方文档案例记录" class="headerlink" title="官方文档案例记录"></a>官方文档案例记录</h3><h4 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h4><ul>
<li>Flink SQL 使得使用标准 SQL 开发流应用程序变的简单。如果你曾经在工作中使用过兼容 ANSI-SQL 2011 的数据库或类似的 SQL 系统，那么就很容易学习 Flink。</li>
</ul>
<h5 id="Source表"><a href="#Source表" class="headerlink" title="Source表"></a>Source表</h5><ul>
<li><p>与所有 SQL 引擎一样，Flink 查询操作是在表上进行。与传统数据库不同，Flink 不在本地管理静态数据；相反，它的查询在外部表上连续运行。</p>
</li>
<li><p>Flink 数据处理流水线开始于 source 表。source 表产生在查询执行期间可以被操作的行；它们是查询时 <code>FROM</code> 子句中引用的表。这些表可能是 Kafka 的 topics，数据库，文件系统，或者任何其它 Flink 知道如何消费的系统。</p>
</li>
<li><p>Flink 支持不同的<a href="https://nightlies.apache.org/flink/flink-docs-release-1.20/zh/docs/connectors/table/overview/">连接器</a>和<a href="https://nightlies.apache.org/flink/flink-docs-release-1.20/zh/docs/connectors/table/formats/overview/">格式</a>相结合以定义表。下面是一个示例，定义一个以 <a href="https://nightlies.apache.org/flink/flink-docs-release-1.20/zh/docs/connectors/table/formats/csv/">CSV 文件</a>作为存储格式的 source 表，其中 <code>emp_id</code>，<code>name</code>，<code>dept_id</code> 作为 <code>CREATE</code> 表语句中的列。</p>
</li>
<li><pre><code class="sqlite">CREATE TABLE employee_information (
    emp_id INT,
    name VARCHAR,
    dept_id INT
) WITH ( 
    &#39;connector&#39; = &#39;filesystem&#39;,
    &#39;path&#39; = &#39;/path/to/something.csv&#39;,
    &#39;format&#39; = &#39;csv&#39;
);
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><code class="hljs asciidoc"><br><span class="hljs-bullet">* </span><br><span class="hljs-bullet"></span><br><span class="hljs-bullet"></span>##### 连续查询（Transformation）<br><br><span class="hljs-bullet">* </span>一个[连续查询](<span class="hljs-link">https://nightlies.apache.org/flink/flink-docs-release-1.20/zh/docs/dev/table/concepts/dynamic_tables/#continuous-queries)永远不会终止，并会产生一个动态表作为结果。</span>[<span class="hljs-string">动态表</span>](https://nightlies.apache.org/flink/flink-docs-release-1.20/zh/docs/dev/table/concepts/dynamic_tables/#continuous-queries)是 Flink 中 Table API 和 SQL 对流数据支持的核心概念。<br><br><span class="hljs-bullet">* </span><span class="hljs-code">````</span>sqlite<br><span class="hljs-code">  SELECT </span><br><span class="hljs-code">     dept_id,</span><br><span class="hljs-code">     COUNT(*) as emp_count </span><br><span class="hljs-code">  FROM employee_information </span><br><span class="hljs-code">  GROUP BY dept_id;</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li></li>
</ul>
<h5 id="Sink表"><a href="#Sink表" class="headerlink" title="Sink表"></a>Sink表</h5><ul>
<li><p>当运行此查询时，SQL 客户端实时但是以只读方式提供输出。存储结果，作为报表或仪表板的数据来源，需要写到另一个表。这可以使用 <code>INSERT INTO</code> 语句来实现。本节中引用的表称为 sink 表。<code>INSERT INTO</code> 语句将作为一个独立查询被提交到 Flink 集群中。</p>
</li>
<li><pre><code class="sqlite">INSERT INTO department_counts
SELECT 
   dept_id,
   COUNT(*) as emp_count 
FROM employee_information
GROUP BY dept_id;
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><br>#### 查询相关内容(DQL)<br><br>##### 概览<br><br>* `TableEnvironment` 的 `sqlQuery()` 方法可以执行 `<span class="hljs-keyword">SELECT</span>` 和 `<span class="hljs-keyword">VALUES</span>` 语句。 这个方法把 `<span class="hljs-keyword">SELECT</span>` 语句(或 `<span class="hljs-keyword">VALUES</span>` 语句)的结果作为一个 `<span class="hljs-keyword">Table</span>` 返回。 `<span class="hljs-keyword">Table</span>`可以用在[后续 <span class="hljs-keyword">SQL</span> 和 <span class="hljs-keyword">Table</span> API 查询](https://nightlies.apache.org/flink/flink-docs-<span class="hljs-keyword">release</span><span class="hljs-number">-1.20</span>/zh/docs/dev/<span class="hljs-keyword">table</span>/common/#mixing-<span class="hljs-keyword">table</span>-api-<span class="hljs-keyword">and</span>-<span class="hljs-keyword">sql</span>)中，可以[转换为 DataStream](https://nightlies.apache.org/flink/flink-docs-<span class="hljs-keyword">release</span><span class="hljs-number">-1.20</span>/zh/docs/dev/<span class="hljs-keyword">table</span>/common/#integration-<span class="hljs-keyword">with</span>-datastream)， 或者 [写入到TableSink](https://nightlies.apache.org/flink/flink-docs-<span class="hljs-keyword">release</span><span class="hljs-number">-1.20</span>/zh/docs/dev/<span class="hljs-keyword">table</span>/common/#emit-a-<span class="hljs-keyword">table</span>)。 <span class="hljs-keyword">SQL</span> 和 <span class="hljs-keyword">Table</span> API 查询可以无缝混合，并进行整体优化并转换为单个程序。<br><br>##### Hints<br><br>* <span class="hljs-keyword">SQL</span> 提示（<span class="hljs-keyword">SQL</span> Hints）是和 <span class="hljs-keyword">SQL</span> 语句一起使用来改变执行计划的<br>* 主要作用就是通过一些定制化的hints，优化<span class="hljs-keyword">sql</span>的执行效率<br>* <span class="hljs-keyword">SQL</span> 提示一般可以用于以下：<br>  - 增强 planner：没有完美的 planner，所以实现 <span class="hljs-keyword">SQL</span> 提示让用户更好地控制执行是非常有意义的；<br>  - 增加元数据（或者统计信息）：如&quot;已扫描的表索引&quot;和&quot;一些混洗键（shuffle keys）的倾斜信息&quot;的一些统计数据对于查询来说是动态的，用提示来配置它们会非常方便，因为我们从 planner 获得的计划元数据通常不那么准确；<br>  - 算子（<span class="hljs-keyword">Operator</span>）资源约束：在许多情况下，我们会为执行算子提供默认的资源配置，即最小并行度或托管内存（UDF 资源消耗）或特殊资源需求（GPU 或 SSD 磁盘）等，可以使用 <span class="hljs-keyword">SQL</span> 提示非常灵活地为每个查询（非作业）配置资源。<br><br>##### <span class="hljs-keyword">with</span>语句<br><br>* <span class="hljs-keyword">sql</span>语句的一个语法糖<br><br>  ````sqlite<br>  <span class="hljs-keyword">WITH</span> orders_with_total <span class="hljs-keyword">AS</span> (<br>      <span class="hljs-keyword">SELECT</span> order_id, price + tax <span class="hljs-keyword">AS</span> total<br>      <span class="hljs-keyword">FROM</span> Orders<br>  )<br>  <span class="hljs-keyword">SELECT</span> order_id, SUM(total)<br>  <span class="hljs-keyword">FROM</span> orders_with_total<br>  <span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> order_id;<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li></li>
</ul>
<h5 id="select-where-distinct"><a href="#select-where-distinct" class="headerlink" title="select where distinct"></a>select where distinct</h5><ul>
<li><pre><code class="sqlite">SELECT order_id, price + tax FROM Orders;

SELECT order_id, price FROM (VALUES (1, 2.0), (2, 3.1))  AS t (order_id, price)
<figure class="highlight arcade"><table><tr><td class="code"><pre><code class="hljs arcade"><br>* <span class="hljs-string">``</span><span class="hljs-string">``</span>sqlite<br>  SELECT <span class="hljs-built_in">DISTINCT</span> id FROM Orders<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>对于流式查询, 计算查询结果所需要的状态可能会源源不断地增长,而状态大小又依赖不同行的数量.此时,可以通过配置文件为状态设置合适的存活时间(TTL),以防止过大的状态可能对查询结果的正确性的影响</p>
</li>
</ul>
<h5 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h5><ul>
<li>Apache Flink 提供了如下 <code>窗口表值函数</code>（table-valued function, 缩写TVF）把表的数据划分到窗口中</li>
</ul>
<h6 id="滚动窗口（tumble）"><a href="#滚动窗口（tumble）" class="headerlink" title="滚动窗口（tumble）"></a>滚动窗口（tumble）</h6><ul>
<li><p><code>TUMBLE</code> 函数指定每个元素到一个指定大小的窗口中。滚动窗口的大小固定且不重复。（窗口不断往前滚动）</p>
</li>
<li><p><code>TUMBLE(TABLE data, DESCRIPTOR(timecol), size [, offset ])</code></p>
<ul>
<li><code>data</code> ：拥有时间属性列的表。</li>
<li><code>timecol</code> ：列描述符，决定数据的哪个时间属性列应该映射到窗口。</li>
<li><code>size</code> ：窗口的大小（时长）。</li>
<li><code>offset</code> ：窗口的偏移量 [非必填]。</li>
</ul>
</li>
<li><pre><code class="sqlite">-- tables must have time attribute, e.g. `bidtime` in this table
Flink SQL&gt; desc Bid;
+-------------+------------------------+------+-----+--------+---------------------------------+
|        name |                   type | null | key | extras |                       watermark |
+-------------+------------------------+------+-----+--------+---------------------------------+
|     bidtime | TIMESTAMP(3) *ROWTIME* | true |     |        | `bidtime` - INTERVAL &#39;1&#39; SECOND |
|       price |         DECIMAL(10, 2) | true |     |        |                                 |
|        item |                 STRING | true |     |        |                                 |
+-------------+------------------------+------+-----+--------+---------------------------------+

Flink SQL&gt; SELECT * FROM Bid;
+------------------+-------+------+
|          bidtime | price | item |
+------------------+-------+------+
| 2020-04-15 08:05 |  4.00 | C    |
| 2020-04-15 08:07 |  2.00 | A    |
| 2020-04-15 08:09 |  5.00 | D    |
| 2020-04-15 08:11 |  3.00 | B    |
| 2020-04-15 08:13 |  1.00 | E    |
| 2020-04-15 08:17 |  6.00 | F    |
+------------------+-------+------+

Flink SQL&gt; SELECT * FROM TABLE(
   TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL &#39;10&#39; MINUTES));
-- or with the named params
-- note: the DATA param must be the first
Flink SQL&gt; SELECT * FROM TABLE(
   TUMBLE(
     DATA =&gt; TABLE Bid,
     TIMECOL =&gt; DESCRIPTOR(bidtime),
     SIZE =&gt; INTERVAL &#39;10&#39; MINUTES));
+------------------+-------+------+------------------+------------------+-------------------------+
|          bidtime | price | item |     window_start |       window_end |            window_time  |
+------------------+-------+------+------------------+------------------+-------------------------+
| 2020-04-15 08:05 |  4.00 | C    | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 |
| 2020-04-15 08:07 |  2.00 | A    | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 |
| 2020-04-15 08:09 |  5.00 | D    | 2020-04-15 08:00 | 2020-04-15 08:10 | 2020-04-15 08:09:59.999 |
| 2020-04-15 08:11 |  3.00 | B    | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 |
| 2020-04-15 08:13 |  1.00 | E    | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 |
| 2020-04-15 08:17 |  6.00 | F    | 2020-04-15 08:10 | 2020-04-15 08:20 | 2020-04-15 08:19:59.999 |
+------------------+-------+------+------------------+------------------+-------------------------+

-- apply aggregation on the tumbling windowed table
Flink SQL&gt; SELECT window_start, window_end, SUM(price) AS total_price
  FROM TABLE(
    TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL &#39;10&#39; MINUTES))
  GROUP BY window_start, window_end;
+------------------+------------------+-------------+
|     window_start |       window_end | total_price |
+------------------+------------------+-------------+
| 2020-04-15 08:00 | 2020-04-15 08:10 |       11.00 |
| 2020-04-15 08:10 | 2020-04-15 08:20 |       10.00 |
+------------------+------------------+-------------+
<figure class="highlight gherkin"><table><tr><td class="code"><pre><code class="hljs gherkin"><br><span class="hljs-symbol">*</span> <br><br><span class="hljs-comment">###### 滑动窗口 （hop）</span><br><br><span class="hljs-symbol">*</span> 滑动窗口函数指定元素到一个定长的窗口中。和滚动窗口很像，有窗口大小参数，另外增加了一个窗口滑动步长参数。如果滑动步长小于窗口大小，就能产生数据重叠的效果。在这个例子里，数据可以被分配在多个窗口。(不断的往前滑动相应的步长)<br><br><span class="hljs-symbol">*</span> `HOP(TABLE data, DESCRIPTOR(timecol), slide, size [, offset ])`<br><br>  - `data`：拥有时间属性列的表。<br>  - `timecol`：列描述符，决定数据的哪个时间属性列应该映射到窗口。<br>  - `slide`：窗口的滑动步长。<br>  - `size`：窗口的大小(时长)。<br>  - `offset`：窗口的偏移量 [非必填]。<br><br><span class="hljs-symbol">*</span> ````sqlite<br>  &gt; SELECT <span class="hljs-symbol">*</span> FROM TABLE(<br>      HOP(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL &#x27;5&#x27; MINUTES, INTERVAL &#x27;10&#x27; MINUTES));<br>  -- or with the named params<br>  -- note: the DATA param must be the first<br>  &gt; SELECT <span class="hljs-symbol">*</span> FROM TABLE(<br>      HOP(<br>        DATA =&gt; TABLE Bid,<br>        TIMECOL =&gt; DESCRIPTOR(bidtime),<br>        SLIDE =&gt; INTERVAL &#x27;5&#x27; MINUTES,<br>        SIZE =&gt; INTERVAL &#x27;10&#x27; MINUTES));<br>  +------------------+-------+------+------------------+------------------+-------------------------+<br>  |<span class="hljs-string">          bidtime </span>|<span class="hljs-string"> price </span>|<span class="hljs-string"> item </span>|<span class="hljs-string">     window_start </span>|<span class="hljs-string">       window_end </span>|<span class="hljs-string">           window_time   </span>|<br>  +------------------+-------+------+------------------+------------------+-------------------------+<br>  |<span class="hljs-string"> 2020-04-15 08:05 </span>|<span class="hljs-string">  4.00 </span>|<span class="hljs-string"> C    </span>|<span class="hljs-string"> 2020-04-15 08:00 </span>|<span class="hljs-string"> 2020-04-15 08:10 </span>|<span class="hljs-string"> 2020-04-15 08:09:59.999 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:05 </span>|<span class="hljs-string">  4.00 </span>|<span class="hljs-string"> C    </span>|<span class="hljs-string"> 2020-04-15 08:05 </span>|<span class="hljs-string"> 2020-04-15 08:15 </span>|<span class="hljs-string"> 2020-04-15 08:14:59.999 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:07 </span>|<span class="hljs-string">  2.00 </span>|<span class="hljs-string"> A    </span>|<span class="hljs-string"> 2020-04-15 08:00 </span>|<span class="hljs-string"> 2020-04-15 08:10 </span>|<span class="hljs-string"> 2020-04-15 08:09:59.999 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:07 </span>|<span class="hljs-string">  2.00 </span>|<span class="hljs-string"> A    </span>|<span class="hljs-string"> 2020-04-15 08:05 </span>|<span class="hljs-string"> 2020-04-15 08:15 </span>|<span class="hljs-string"> 2020-04-15 08:14:59.999 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:09 </span>|<span class="hljs-string">  5.00 </span>|<span class="hljs-string"> D    </span>|<span class="hljs-string"> 2020-04-15 08:00 </span>|<span class="hljs-string"> 2020-04-15 08:10 </span>|<span class="hljs-string"> 2020-04-15 08:09:59.999 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:09 </span>|<span class="hljs-string">  5.00 </span>|<span class="hljs-string"> D    </span>|<span class="hljs-string"> 2020-04-15 08:05 </span>|<span class="hljs-string"> 2020-04-15 08:15 </span>|<span class="hljs-string"> 2020-04-15 08:14:59.999 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:11 </span>|<span class="hljs-string">  3.00 </span>|<span class="hljs-string"> B    </span>|<span class="hljs-string"> 2020-04-15 08:05 </span>|<span class="hljs-string"> 2020-04-15 08:15 </span>|<span class="hljs-string"> 2020-04-15 08:14:59.999 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:11 </span>|<span class="hljs-string">  3.00 </span>|<span class="hljs-string"> B    </span>|<span class="hljs-string"> 2020-04-15 08:10 </span>|<span class="hljs-string"> 2020-04-15 08:20 </span>|<span class="hljs-string"> 2020-04-15 08:19:59.999 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:13 </span>|<span class="hljs-string">  1.00 </span>|<span class="hljs-string"> E    </span>|<span class="hljs-string"> 2020-04-15 08:05 </span>|<span class="hljs-string"> 2020-04-15 08:15 </span>|<span class="hljs-string"> 2020-04-15 08:14:59.999 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:13 </span>|<span class="hljs-string">  1.00 </span>|<span class="hljs-string"> E    </span>|<span class="hljs-string"> 2020-04-15 08:10 </span>|<span class="hljs-string"> 2020-04-15 08:20 </span>|<span class="hljs-string"> 2020-04-15 08:19:59.999 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:17 </span>|<span class="hljs-string">  6.00 </span>|<span class="hljs-string"> F    </span>|<span class="hljs-string"> 2020-04-15 08:10 </span>|<span class="hljs-string"> 2020-04-15 08:20 </span>|<span class="hljs-string"> 2020-04-15 08:19:59.999 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:17 </span>|<span class="hljs-string">  6.00 </span>|<span class="hljs-string"> F    </span>|<span class="hljs-string"> 2020-04-15 08:15 </span>|<span class="hljs-string"> 2020-04-15 08:25 </span>|<span class="hljs-string"> 2020-04-15 08:24:59.999 </span>|<br>  +------------------+-------+------+------------------+------------------+-------------------------+<br>  <br>  -- apply aggregation on the hopping windowed table<br>  &gt; SELECT window_start, window_end, SUM(price) AS total_price<br>    FROM TABLE(<br>      HOP(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL &#x27;5&#x27; MINUTES, INTERVAL &#x27;10&#x27; MINUTES))<br>    GROUP BY window_start, window_end;<br>  +------------------+------------------+-------------+<br>  |<span class="hljs-string">     window_start </span>|<span class="hljs-string">       window_end </span>|<span class="hljs-string"> total_price </span>|<br>  +------------------+------------------+-------------+<br>  |<span class="hljs-string"> 2020-04-15 08:00 </span>|<span class="hljs-string"> 2020-04-15 08:10 </span>|<span class="hljs-string">       11.00 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:05 </span>|<span class="hljs-string"> 2020-04-15 08:15 </span>|<span class="hljs-string">       15.00 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:10 </span>|<span class="hljs-string"> 2020-04-15 08:20 </span>|<span class="hljs-string">       10.00 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:15 </span>|<span class="hljs-string"> 2020-04-15 08:25 </span>|<span class="hljs-string">        6.00 </span>|<br>  +------------------+------------------+-------------+<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<h6 id="累积窗口（cumulate）"><a href="#累积窗口（cumulate）" class="headerlink" title="累积窗口（cumulate）"></a>累积窗口（cumulate）</h6><ul>
<li>累积窗口在某些场景中非常有用，比如说提前触发的滚动窗口。例如：每日仪表盘从 00:00 开始每分钟绘制累积 UV，10:00 时 UV 就是从 00:00 到 10:00 的UV 总数。累积窗口可以简单且有效地实现它。</li>
<li><code>CUMULATE(TABLE data, DESCRIPTOR(timecol), step, size)</code><ul>
<li><code>data</code>：拥有时间属性列的表。</li>
<li><code>timecol</code>：列描述符，决定数据的哪个时间属性列应该映射到窗口。</li>
<li><code>step</code>：指定连续的累积窗口之间增加的窗口大小。</li>
<li><code>size</code>：指定累积窗口的最大宽度的窗口时间。<code>size</code>必须是<code>step</code>的整数倍。</li>
<li><code>offset</code>：窗口的偏移量 [非必填]。</li>
</ul>
</li>
</ul>
<h6 id="会话窗口（session）（目前仅支持流模式，v1-20-0）"><a href="#会话窗口（session）（目前仅支持流模式，v1-20-0）" class="headerlink" title="会话窗口（session）（目前仅支持流模式，v1.20.0）"></a>会话窗口（session）（目前仅支持流模式，v1.20.0）</h6><h5 id="窗口聚合"><a href="#窗口聚合" class="headerlink" title="窗口聚合"></a>窗口聚合</h5><ul>
<li><p>会话窗口函数通过会话活动对元素进行分组。与滚动窗口和滑动窗口不同，会话窗口不重叠，也没有固定的开始和结束时间。 一个会话窗口会在一定时间内没有收到元素时关闭，比如超过一定时间不处于活跃状态。 会话窗口需要配置一个固定的会话间隙，以定义不活跃间隙的时长。 当超出不活跃间隙的时候，当前的会话窗口将会关闭，随后的元素将被分配到一个新的会话窗口内。</p>
</li>
<li><p><code>SESSION(TABLE data [PARTITION BY(keycols, ...)], DESCRIPTOR(timecol), gap)</code></p>
<ul>
<li><code>data</code>：拥有时间属性列的表。</li>
<li><code>keycols</code>：列描述符，决定会话窗口应该使用哪些列来分区数据。</li>
<li><code>timecol</code>：列描述符，决定数据的哪个时间属性列应该映射到窗口。</li>
<li><code>gap</code>：两个事件被认为属于同一个会话窗口的最大时间间隔。</li>
</ul>
</li>
<li><pre><code class="sqlite">/*注意:

会话窗口函数目前不支持批模式。
会话窗口函数目前不支持 性能调优 中的任何优化。
会话窗口 Join 、会话窗口 Top-N 、会话窗口聚合功能目前理论可用，但仍处于实验阶段。遇到问题可以在 JIRA 中报告。
*/
-- tables must have time attribute, e.g. `bidtime` in this table
Flink SQL&gt; desc Bid;
+-------------+------------------------+------+-----+--------+---------------------------------+
|        name |                   type | null | key | extras |                       watermark |
+-------------+------------------------+------+-----+--------+---------------------------------+
|     bidtime | TIMESTAMP(3) *ROWTIME* | true |     |        | `bidtime` - INTERVAL &#39;1&#39; SECOND |
|       price |         DECIMAL(10, 2) | true |     |        |                                 |
|        item |                 STRING | true |     |        |                                 |
+-------------+------------------------+------+-----+--------+---------------------------------+

Flink SQL&gt; SELECT * FROM Bid;
+------------------+-------+------+
|          bidtime | price | item |
+------------------+-------+------+
| 2020-04-15 08:07 |  4.00 | A    |
| 2020-04-15 08:06 |  2.00 | A    |
| 2020-04-15 08:09 |  5.00 | B    |
| 2020-04-15 08:08 |  3.00 | A    |
| 2020-04-15 08:17 |  1.00 | B    |
+------------------+-------+------+

-- session window with partition keys
&gt; SELECT * FROM TABLE(
    SESSION(TABLE Bid PARTITION BY item, DESCRIPTOR(bidtime), INTERVAL &#39;5&#39; MINUTES));
-- or with the named params
-- note: the DATA param must be the first
&gt; SELECT * FROM TABLE(
    SESSION(
      DATA =&gt; TABLE Bid PARTITION BY item,
      TIMECOL =&gt; DESCRIPTOR(bidtime),
      GAP =&gt; INTERVAL &#39;5&#39; MINUTES);
+------------------+-------+------+------------------+------------------+-------------------------+
|          bidtime | price | item |     window_start |       window_end |             window_time |
+------------------+-------+------+------------------+------------------+-------------------------+
| 2020-04-15 08:07 |  4.00 | A    | 2020-04-15 08:06 | 2020-04-15 08:13 | 2020-04-15 08:12:59.999 |
| 2020-04-15 08:06 |  2.00 | A    | 2020-04-15 08:06 | 2020-04-15 08:13 | 2020-04-15 08:12:59.999 |
| 2020-04-15 08:08 |  3.00 | A    | 2020-04-15 08:06 | 2020-04-15 08:13 | 2020-04-15 08:12:59.999 |
| 2020-04-15 08:09 |  5.00 | B    | 2020-04-15 08:09 | 2020-04-15 08:14 | 2020-04-15 08:13:59.999 |
| 2020-04-15 08:17 |  1.00 | B    | 2020-04-15 08:17 | 2020-04-15 08:22 | 2020-04-15 08:21:59.999 |
+------------------+-------+------+------------------+------------------+-------------------------+

-- apply aggregation on the session windowed table with partition keys
&gt; SELECT window_start, window_end, item, SUM(price) AS total_price
  FROM TABLE(
      SESSION(TABLE Bid PARTITION BY item, DESCRIPTOR(bidtime), INTERVAL &#39;5&#39; MINUTES))
  GROUP BY item, window_start, window_end;
+------------------+------------------+------+-------------+
|     window_start |       window_end | item | total_price |
+------------------+------------------+------+-------------+
| 2020-04-15 08:06 | 2020-04-15 08:13 | A    |        9.00 |
| 2020-04-15 08:09 | 2020-04-15 08:14 | B    |        5.00 |
| 2020-04-15 08:17 | 2020-04-15 08:22 | B    |        1.00 |
+------------------+------------------+------+-------------+

-- session window without partition keys
&gt; SELECT * FROM TABLE(
    SESSION(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL &#39;5&#39; MINUTES));
-- or with the named params
-- note: the DATA param must be the first
&gt; SELECT * FROM TABLE(
    SESSION(
      DATA =&gt; TABLE Bid,
      TIMECOL =&gt; DESCRIPTOR(bidtime),
      GAP =&gt; INTERVAL &#39;5&#39; MINUTES);
+------------------+-------+------+------------------+------------------+-------------------------+
|          bidtime | price | item |     window_start |       window_end |             window_time |
+------------------+-------+------+------------------+------------------+-------------------------+
| 2020-04-15 08:07 |  4.00 | A    | 2020-04-15 08:06 | 2020-04-15 08:14 | 2020-04-15 08:13:59.999 |
| 2020-04-15 08:06 |  2.00 | A    | 2020-04-15 08:06 | 2020-04-15 08:14 | 2020-04-15 08:13:59.999 |
| 2020-04-15 08:08 |  3.00 | A    | 2020-04-15 08:06 | 2020-04-15 08:14 | 2020-04-15 08:13:59.999 |
| 2020-04-15 08:09 |  5.00 | B    | 2020-04-15 08:06 | 2020-04-15 08:14 | 2020-04-15 08:13:59.999 |
| 2020-04-15 08:17 |  1.00 | B    | 2020-04-15 08:17 | 2020-04-15 08:22 | 2020-04-15 08:21:59.999 |
+------------------+-------+------+------------------+------------------+-------------------------+

-- apply aggregation on the session windowed table without partition keys
&gt; SELECT window_start, window_end, SUM(price) AS total_price
  FROM TABLE(
      SESSION(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL &#39;5&#39; MINUTES))
  GROUP BY window_start, window_end;
+------------------+------------------+-------------+
|     window_start |       window_end | total_price |
+------------------+------------------+-------------+
| 2020-04-15 08:06 | 2020-04-15 08:14 |       14.00 |
| 2020-04-15 08:17 | 2020-04-15 08:22 |        1.00 |
+------------------+------------------+-------------+
<figure class="highlight gherkin"><table><tr><td class="code"><pre><code class="hljs gherkin"><br><br><br><span class="hljs-comment">###### 窗口偏移</span><br><br><span class="hljs-symbol">*</span> `Offset` 可选参数，可以用来改变窗口的分配。可以是正或者负的区间。默认情况下窗口的偏移是 0。不同的偏移值可以决定记录分配的窗口<br><br><span class="hljs-comment">##### 窗口聚合</span><br><br><span class="hljs-symbol">*</span> `GROUPING SETS` 窗口聚合中 `GROUP BY` 子句必须包含 `window_start` 和 `window_end` 列，但 `GROUPING SETS` 子句中不能包含这两个字段。<br><br>  - 对于rollup和cube的支持也是类似的<br><br><span class="hljs-symbol">*</span> ````sqlite<br>  Flink SQL&gt; SELECT window_start, window_end, supplier_id, SUM(price) AS total_price<br>    FROM TABLE(<br>      TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL &#x27;10&#x27; MINUTES))<br>    GROUP BY window_start, window_end, GROUPING SETS ((supplier_id), ());<br>  +------------------+------------------+-------------+-------------+<br>  |<span class="hljs-string">     window_start </span>|<span class="hljs-string">       window_end </span>|<span class="hljs-string"> supplier_id </span>|<span class="hljs-string"> total_price </span>|<br>  +------------------+------------------+-------------+-------------+<br>  |<span class="hljs-string"> 2020-04-15 08:00 </span>|<span class="hljs-string"> 2020-04-15 08:10 </span>|<span class="hljs-string">      (NULL) </span>|<span class="hljs-string">       11.00 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:00 </span>|<span class="hljs-string"> 2020-04-15 08:10 </span>|<span class="hljs-string">   supplier2 </span>|<span class="hljs-string">        5.00 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:00 </span>|<span class="hljs-string"> 2020-04-15 08:10 </span>|<span class="hljs-string">   supplier1 </span>|<span class="hljs-string">        6.00 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:10 </span>|<span class="hljs-string"> 2020-04-15 08:20 </span>|<span class="hljs-string">      (NULL) </span>|<span class="hljs-string">       10.00 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:10 </span>|<span class="hljs-string"> 2020-04-15 08:20 </span>|<span class="hljs-string">   supplier2 </span>|<span class="hljs-string">        9.00 </span>|<br>  |<span class="hljs-string"> 2020-04-15 08:10 </span>|<span class="hljs-string"> 2020-04-15 08:20 </span>|<span class="hljs-string">   supplier1 </span>|<span class="hljs-string">        1.00 </span>|<br>  +------------------+------------------+-------------+-------------+<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>多级窗口聚合</p>
<ul>
<li><p><code>window_start</code> 和 <code>window_end</code> 列是普通的时间戳字段，并不是时间属性。因此它们不能在后续的操作中当做时间属性进行基于时间的操作。 为了传递时间属性，需要在 <code>GROUP BY</code> 子句中添加 <code>window_time</code> 列。<code>window_time</code> 是 <a href="https://nightlies.apache.org/flink/flink-docs-release-1.20/zh/docs/dev/table/sql/queries/window-tvf/#window-functions">Windowing TVFs</a> 产生的三列之一，它是窗口的时间属性。 <code>window_time</code> 添加到 <code>GROUP BY</code> 子句后就能被选定了。</p>
</li>
<li><pre><code class="sqlite">-- tumbling 5 minutes for each supplier_id
CREATE VIEW window1 AS
-- Note: The window start and window end fields of inner Window TVF are optional in the select clause. However, if they appear in the clause, they need to be aliased to prevent name conflicting with the window start and window end of the outer Window TVF.
SELECT window_start AS window_5mintumble_start, window_end AS window_5mintumble_end, window_time AS rowtime, SUM(price) AS partial_price
  FROM TABLE(
    TUMBLE(TABLE Bid, DESCRIPTOR(bidtime), INTERVAL &#39;5&#39; MINUTES))
  GROUP BY supplier_id, window_start, window_end, window_time;

-- tumbling 10 minutes on the first window
SELECT window_start, window_end, SUM(partial_price) AS total_price
  FROM TABLE(
      TUMBLE(TABLE window1, DESCRIPTOR(rowtime), INTERVAL &#39;10&#39; MINUTES))
  GROUP BY window_start, window_end;
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><br>  - <br><br>##### 分组聚合<br><br>* <span class="hljs-keyword">sql</span>常用的分组聚合操作都是支持的、<span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span>,count(<span class="hljs-keyword">distinct</span> ),<span class="hljs-keyword">grouping sets</span>维度组合聚合，<span class="hljs-keyword">having</span>过滤等<br><br>* ````sqlite<br>  <span class="hljs-keyword">SELECT</span> COUNT(*)<br>  <span class="hljs-keyword">FROM</span> Orders<br>  <span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> order_id<br>  <br>  <span class="hljs-keyword">SELECT</span> COUNT(<span class="hljs-keyword">DISTINCT</span> order_id) <span class="hljs-keyword">FROM</span> Orders<br>  <br>  <span class="hljs-keyword">SELECT</span> supplier_id, rating, COUNT(*) <span class="hljs-keyword">AS</span> total<br>  <span class="hljs-keyword">FROM</span> (<span class="hljs-keyword">VALUES</span><br>      (<span class="hljs-string">&#x27;supplier1&#x27;</span>, <span class="hljs-string">&#x27;product1&#x27;</span>, <span class="hljs-number">4</span>),<br>      (<span class="hljs-string">&#x27;supplier1&#x27;</span>, <span class="hljs-string">&#x27;product2&#x27;</span>, <span class="hljs-number">3</span>),<br>      (<span class="hljs-string">&#x27;supplier2&#x27;</span>, <span class="hljs-string">&#x27;product3&#x27;</span>, <span class="hljs-number">3</span>),<br>      (<span class="hljs-string">&#x27;supplier2&#x27;</span>, <span class="hljs-string">&#x27;product4&#x27;</span>, <span class="hljs-number">4</span>))<br>  <span class="hljs-keyword">AS</span> Products(supplier_id, product_id, rating)<br>  <span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> <span class="hljs-keyword">GROUPING SETS</span> ((supplier_id, rating), (supplier_id), ())<br>  <br>  <span class="hljs-keyword">SELECT</span> SUM(amount)<br>  <span class="hljs-keyword">FROM</span> Orders<br>  <span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> users<br>  <span class="hljs-keyword">HAVING</span> SUM(amount) &gt; <span class="hljs-number">50</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li></li>
</ul>
<h5 id="over聚合"><a href="#over聚合" class="headerlink" title="over聚合"></a>over聚合</h5><ul>
<li><p><code>OVER</code> 聚合通过排序后的范围数据为每行输入计算出聚合值。和 <code>GROUP BY</code> 聚合不同， <code>OVER</code> 聚合不会把结果通过分组减少到一行，它会为每行输入增加一个聚合值。</p>
</li>
<li><pre><code class="sqlite">-- 下面这个查询为每个订单计算前一个小时之内接收到的同一产品所有订单的总金额。
SELECT order_id, order_time, amount,
  SUM(amount) OVER (
    PARTITION BY product
    ORDER BY order_time
    RANGE BETWEEN INTERVAL &#39;1&#39; HOUR PRECEDING AND CURRENT ROW
  ) AS one_hour_prod_amount_sum
FROM Orders
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><br>* 特殊点：<br><br>  - 你可以在一个 `<span class="hljs-keyword">SELECT</span>` 子句中定义多个 `<span class="hljs-keyword">OVER</span>` 窗口聚合。然而，对于流式查询，由于目前的限制，所有聚合的 `<span class="hljs-keyword">OVER</span>` 窗口必须是相同的。<br><br>  - Flink 目前只支持 `<span class="hljs-keyword">OVER</span>` 窗口定义在升序（<span class="hljs-keyword">asc</span>）的 [时间属性](https://nightlies.apache.org/flink/flink-docs-<span class="hljs-keyword">release</span><span class="hljs-number">-1.20</span>/zh/docs/dev/<span class="hljs-keyword">table</span>/concepts/time_attributes/) 上。其他的排序不支持。<br><br>  - 范围（RANGE）定义指定了聚合中包含了多少行数据。范围通过 `<span class="hljs-keyword">BETWEEN</span>` 子句定义上下边界，其内的所有行都会聚合。Flink 只支持 `<span class="hljs-keyword">CURRENT</span> <span class="hljs-keyword">ROW</span>` 作为上边界。<br><br>    - RANGE间隔<br><br>      - `RANGE` 间隔是定义在排序列值上的，在 Flink 里，排序列总是一个时间属性。下面的 `RANG` 间隔定义了聚合会在比当前行的时间属性小 <span class="hljs-number">30</span> 分钟的所有行上进行。<br><br>      - ```<span class="hljs-keyword">sql</span><br>        <span class="hljs-keyword">RANGE</span> <span class="hljs-keyword">BETWEEN</span> <span class="hljs-type">INTERVAL</span> <span class="hljs-string">&#x27;30&#x27;</span> MINUTE <span class="hljs-keyword">PRECEDING</span> <span class="hljs-keyword">AND</span> <span class="hljs-keyword">CURRENT</span> <span class="hljs-keyword">ROW</span><br>        ```<br><br>    - <span class="hljs-keyword">ROW</span>间隔<br><br>      - `<span class="hljs-keyword">ROWS</span>` 间隔基于计数。它定义了聚合操作包含的精确行数。下面的 `<span class="hljs-keyword">ROWS</span>` 间隔定义了当前行 + 之前的 <span class="hljs-number">10</span> 行（也就是<span class="hljs-number">11</span>行）都会被聚合。<br><br>      - ```sqlite<br>        <span class="hljs-keyword">ROWS</span> <span class="hljs-keyword">BETWEEN</span> <span class="hljs-number">10</span> <span class="hljs-keyword">PRECEDING</span> <span class="hljs-keyword">AND</span> <span class="hljs-keyword">CURRENT</span> <span class="hljs-keyword">ROW</span><br>        <span class="hljs-keyword">WINDOW</span><br>        ```<br><br>    - <span class="hljs-keyword">WINDOW</span>子句的使用<br><br>    - ```sqlite<br>      <span class="hljs-keyword">SELECT</span> order_id, order_time, amount,<br>        SUM(amount) <span class="hljs-keyword">OVER</span> w <span class="hljs-keyword">AS</span> sum_amount,<br>        AVG(amount) <span class="hljs-keyword">OVER</span> w <span class="hljs-keyword">AS</span> avg_amount<br>      <span class="hljs-keyword">FROM</span> Orders<br>      <span class="hljs-keyword">WINDOW</span> w <span class="hljs-keyword">AS</span> (<br>        <span class="hljs-keyword">PARTITION</span> <span class="hljs-keyword">BY</span> product<br>        <span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> order_time<br>        <span class="hljs-keyword">RANGE</span> <span class="hljs-keyword">BETWEEN</span> <span class="hljs-type">INTERVAL</span> <span class="hljs-string">&#x27;1&#x27;</span> HOUR <span class="hljs-keyword">PRECEDING</span> <span class="hljs-keyword">AND</span> <span class="hljs-keyword">CURRENT</span> <span class="hljs-keyword">ROW</span>)<br>      ```<br><br>##### <span class="hljs-keyword">join</span><br><br>* Flink <span class="hljs-keyword">SQL</span>支持对动态表进行复杂而灵活的连接操作。 为了处理不同的场景，需要多种查询语义，因此有几种不同类型的 <span class="hljs-keyword">Join</span>。<br>* 默认情况下，joins 的顺序是没有优化的。表的 <span class="hljs-keyword">join</span> 顺序是在 `<span class="hljs-keyword">FROM</span>` 从句指定的。可以通过把更新频率最低的表放在第一个、频率最高的放在最后这种方式来微调 <span class="hljs-keyword">join</span> 查询的性能。需要确保表的顺序不会产生笛卡尔积，因为不支持这样的操作并且会导致查询失败。<br><br>###### Regular Joins<br><br>###### <span class="hljs-type">Interval</span> Joins<br><br>###### Temporal Joins<br><br>###### Lookup <span class="hljs-keyword">Join</span><br><br>###### 数组展开<br><br>###### <span class="hljs-keyword">Table</span> <span class="hljs-keyword">Function</span><br><br><br><br><br><br>##### 窗口关联<br><br>* 窗口关联就是增加时间维度到关联条件中。在此过程中，窗口关联将两个流中在同一窗口且符合 <span class="hljs-keyword">join</span> 条件的元素 <span class="hljs-keyword">join</span> 起来。窗口关联的语义和 [DataStream <span class="hljs-keyword">window</span> <span class="hljs-keyword">join</span>](https://nightlies.apache.org/flink/flink-docs-<span class="hljs-keyword">release</span><span class="hljs-number">-1.20</span>/zh/docs/dev/datastream/operators/joining/#<span class="hljs-keyword">window</span>-<span class="hljs-keyword">join</span>) 相同。<br><br>##### 集合操作<br><br>* <span class="hljs-keyword">UNION</span>,<span class="hljs-keyword">INTERSECT</span>,<span class="hljs-keyword">EXCEPT</span>,<span class="hljs-keyword">IN</span>,<span class="hljs-keyword">EXISTS</span><br><br>* ````sqlite<br>  Flink <span class="hljs-keyword">SQL</span>&gt; <span class="hljs-keyword">create</span> <span class="hljs-keyword">view</span> t1(s) <span class="hljs-keyword">as</span> <span class="hljs-keyword">values</span> (<span class="hljs-string">&#x27;c&#x27;</span>), (<span class="hljs-string">&#x27;a&#x27;</span>), (<span class="hljs-string">&#x27;b&#x27;</span>), (<span class="hljs-string">&#x27;b&#x27;</span>), (<span class="hljs-string">&#x27;c&#x27;</span>);<br>  Flink <span class="hljs-keyword">SQL</span>&gt; <span class="hljs-keyword">create</span> <span class="hljs-keyword">view</span> t2(s) <span class="hljs-keyword">as</span> <span class="hljs-keyword">values</span> (<span class="hljs-string">&#x27;d&#x27;</span>), (<span class="hljs-string">&#x27;e&#x27;</span>), (<span class="hljs-string">&#x27;a&#x27;</span>), (<span class="hljs-string">&#x27;b&#x27;</span>), (<span class="hljs-string">&#x27;b&#x27;</span>);<br>  <br>  <span class="hljs-comment">-- INTERSECT 和 INTERSECT ALL 返回两个表中共有的数据。 INTERSECT 会去重，INTERSECT ALL 不会去重。</span><br>  <br>  (<span class="hljs-keyword">SELECT</span> s <span class="hljs-keyword">FROM</span> t1) <span class="hljs-keyword">INTERSECT</span> (<span class="hljs-keyword">SELECT</span> s <span class="hljs-keyword">FROM</span> t2);<br>  <br>  <span class="hljs-comment">-- EXCEPT 和 EXCEPT ALL 返回在一个表中存在，但在另一个表中不存在数据。 EXCEPT 会去重，EXCEPT ALL不会去重。</span><br>  <br>  (<span class="hljs-keyword">SELECT</span> s <span class="hljs-keyword">FROM</span> t1) <span class="hljs-keyword">EXCEPT</span> (<span class="hljs-keyword">SELECT</span> s <span class="hljs-keyword">FROM</span> t2);<br>  <br>  <span class="hljs-keyword">SELECT</span> <span class="hljs-keyword">user</span>, amount<br>  <span class="hljs-keyword">FROM</span> Orders<br>  <span class="hljs-keyword">WHERE</span> product <span class="hljs-keyword">IN</span> (<br>      <span class="hljs-keyword">SELECT</span> product <span class="hljs-keyword">FROM</span> NewProducts<br>  )<br>  <br>  <span class="hljs-keyword">SELECT</span> <span class="hljs-keyword">user</span>, amount<br>  <span class="hljs-keyword">FROM</span> Orders<br>  <span class="hljs-keyword">WHERE</span> product <span class="hljs-keyword">EXISTS</span> (<br>      <span class="hljs-keyword">SELECT</span> product <span class="hljs-keyword">FROM</span> NewProducts<br>  )<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li></li>
</ul>
<h5 id="order-by语句"><a href="#order-by语句" class="headerlink" title="order by语句"></a>order by语句</h5><h5 id="limit语句"><a href="#limit语句" class="headerlink" title="limit语句"></a>limit语句</h5><h5 id="Top-N"><a href="#Top-N" class="headerlink" title="Top-N"></a>Top-N</h5><h5 id="窗口Top-N"><a href="#窗口Top-N" class="headerlink" title="窗口Top-N"></a>窗口Top-N</h5><h5 id="窗口去重"><a href="#窗口去重" class="headerlink" title="窗口去重"></a>窗口去重</h5><h5 id="去重"><a href="#去重" class="headerlink" title="去重"></a>去重</h5>]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>hbase</title>
    <url>/2021/06/27/hbase/</url>
    <content><![CDATA[<h1 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h1><h2 id="HBbse概述"><a href="#HBbse概述" class="headerlink" title="HBbse概述"></a>HBbse概述</h2><ul>
<li><p>HBase是一种分布式，可扩展，，支持海量数据存储的NoSQL数据库。</p>
</li>
<li><p>逻辑结构</p>
</li>
<li><p><a href="https://imgtu.com/i/RJBhlV"><img src="https://z3.ax1x.com/2021/06/27/RJBhlV.png" alt="RJBhlV.png"></a></p>
</li>
<li><p>row key类似于主键</p>
</li>
<li><p>region在行上的一个切分，几行分一个region</p>
</li>
<li><p>列族：Column Qualifier。</p>
</li>
<li><p>store：按照region和列族可以分为一个个store</p>
</li>
<li><p>物理结构</p>
<p><a href="https://imgtu.com/i/RJBomF"><img src="https://z3.ax1x.com/2021/06/27/RJBomF.png" alt="RJBomF.png"></a></p>
</li>
<li><p>数据模型</p>
<p><strong>1.</strong> <em><strong>Name Space</strong></em></p>
<p>命名空间，类似于关系型数据库的DatabBase概念，每个命名空间下有多个表。HBase有两个自带的命名空间，分别是hbase和default，hbase中存放的是HBase内置的表，default表是用户默认使用的命名空间。</p>
<p><strong>2.</strong> <em><strong>Region</strong></em></p>
<p>类似于关系型数据库的表概念。不同的是，HBase定义表时只需要声明列族即可，不需要声明具体的列。这意味着，往HBase写入数据时，字段可以动态、按需指定。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景。</p>
<p><strong>3.</strong> <em><strong>Row</strong></em></p>
<p>HBase表中的每行数据都由一个<em><strong>RowKey</strong></em>和多个<em><strong>Column</strong></em>（列）组成，数据是按照RowKey的字典顺序存储的，并且查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要。</p>
<p><strong>4.</strong> <em><strong>Column</strong></em></p>
<p>HBase中的每个列都由Column Family(列族)和Column Qualifier（列限定符）进行限定，例如info：name，info：age。建表时，只需指明列族，而列限定符无需预先定义。</p>
<p><strong>5.</strong> <em><strong>Time</strong></em> <em><strong>Stamp</strong></em></p>
<p>用于标识数据的不同版本（version），每条数据写入时，如果不指定时间戳，系统会自动为其加上该字段，其值为写入HBase的时间。</p>
<p><strong>6.</strong> <em><strong>Cell</strong></em></p>
<p>由{rowkey, column Family：column Qualifier, time Stamp} 唯一确定的单元。cell中的数据是没有类型的，全部是字节数组形式存贮。</p>
</li>
<li><p>架构</p>
<p><a href="https://imgtu.com/i/RJDyjK"><img src="https://z3.ax1x.com/2021/06/27/RJDyjK.png" alt="RJDyjK.png"></a></p>
</li>
<li><p>Region Server：region的管理者</p>
</li>
<li><p>Master：所有Region Server的管理者</p>
</li>
<li><p>Zookeeper：HBase通过Zookeeper来做Master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。</p>
</li>
<li><p>HDFS:HDFS为HBase提供最终的底层数据存储服务，同时为HBase提供高可用的支持。</p>
</li>
<li><p>一个RegionServer中有多个Region，每个Region中有多个store，每个Store中有一个Memstore和多个StoreFile</p>
</li>
</ul>
<h2 id="HBase架构"><a href="#HBase架构" class="headerlink" title="HBase架构"></a>HBase架构</h2><p><a href="https://imgtu.com/i/RDKkkR"><img src="https://z3.ax1x.com/2021/06/30/RDKkkR.png" alt="RDKkkR.png"></a></p>
<p>HBase分多个RegionServer，每个RegionServer内又有多个Region，每个Region中又有多个store，store中数据先写到内存，再写到SotreFile（分布式文件系统）上去。</p>
<ul>
<li>WAL:Write-Ahead logfile，防止写入内存后数据丢失，先写到这样一个日志文件中。类似于yarn中的edits文件。</li>
<li>保存实际数据的物理文件，StoreFile以HFile的形式存储在HDFS上。每个Store会有一个或多个StoreFile（HFile），数据在每个StoreFile中都是有序的。</li>
</ul>
<h3 id="写数据流程"><a href="#写数据流程" class="headerlink" title="写数据流程"></a>写数据流程</h3><p><a href="https://imgtu.com/i/RDKXHH"><img src="https://z3.ax1x.com/2021/06/30/RDKXHH.png" alt="RDKXHH.png"></a></p>
<p>meta表中存放了表的信息，而meta的元数据信息存放在ZK上。</p>
<ol>
<li><p>Client先访问zookeeper，获取hbase:meta（）表位于哪个Region Server，client如果在MetaCache中发现有该表的信息，就不会向ZK请求获取meta的位置了。</p>
</li>
<li><p>访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table&#x2F;rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</p>
</li>
<li><p>与目标Region Server进行通讯；</p>
</li>
<li><p>将数据顺序写入（追加）到WAL；</p>
</li>
<li><p>将数据写入对应的MemStore，数据会在MemStore进行排序；</p>
</li>
<li><p>向客户端发送ack；</p>
</li>
<li><p>等达到MemStore的刷写时机后，将数据刷写到HFile（分布式存储中）。</p>
</li>
</ol>
<h3 id="MemStore的刷写时机"><a href="#MemStore的刷写时机" class="headerlink" title="MemStore的刷写时机"></a>MemStore的刷写时机</h3><ul>
<li>当某个memstroe的大小达到了<strong>hbase.hregion.memstore.flush.size</strong>（默认值128M），其所在region的所有memstore都会刷写。但是数据持续写入到memstore也不会达到128就刷，当memstore的大小达到了128*4时，会阻止继续往该memstore写数据。</li>
<li>Java heap size 堆栈大小, 指Java 虚拟机的内存大小。在Java虚拟机中，分配多少内存用于调用对象,函数和数组。因为底层中，函数和数组的调用在计算机中是用堆栈实现。</li>
<li>当region server中memstore的总大小达到<br><strong>java_heapsize*hbase.regionserver.global.memstore.size.upper.limit</strong>（默认值0.95），<br>Region server会把它的所有region按照其所有memstore的大小顺序（由大到小）依次进行刷写。直到region server中所有region的memstore的总大小减小到*<strong>hbase.regionserver.global.memstore.size.lower.limit</strong>以下。</li>
<li>当region server中memstore的总大小达到<br><strong>java_heapsize*hbase.regionserver.global.memstore.size</strong>（默认值0.4）<br>时，会阻止继续往所有的memstore写数据。</li>
<li>达到自动刷写时间。默认1h。<strong>hbase.regionserver.optionalcacheflushinterval</strong></li>
</ul>
<h3 id="读数据流程"><a href="#读数据流程" class="headerlink" title="读数据流程"></a>读数据流程</h3><p><a href="https://imgtu.com/i/RrMr7Q"><img src="https://z3.ax1x.com/2021/07/01/RrMr7Q.png" alt="RrMr7Q.png"></a></p>
<ul>
<li>与写数据流程差不多。说一下区别，去目标RegionServer读取数据时，会向Block Cache(读缓存),MemStore,StoreFile中查询数据，并将所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put&#x2F;Delete）。</li>
<li>将从文件中查询到的数据块缓存到Block Cache中。客户端从读缓存中读取相应的数据。返回ACK。</li>
</ul>
<h3 id="StoreFile-Compaction"><a href="#StoreFile-Compaction" class="headerlink" title="StoreFile Compaction"></a>StoreFile Compaction</h3><ul>
<li><p>由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本（timestamp）和不同类型（Put&#x2F;Delete）有可能会分布在不同的HFile中，因此查询时需要遍历所有的HFile。为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile Compaction。</p>
</li>
<li><p>Compaction（合并）分为两种：分别是Minor Compaction和Major Compaction。</p>
</li>
<li><p>Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，但不会清理过期和删除的数据。默认是三个小文件会进行合并，达到一定的数量后也不会直接合并，会根据一个算法判断是否要进行合并。</p>
</li>
<li><p>Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，并且会清理掉过期和删除的数据。比较耗费资源，默认是一星期进行一次。</p>
</li>
</ul>
<h3 id="Region-Split"><a href="#Region-Split" class="headerlink" title="Region Split"></a>Region Split</h3><ul>
<li><p>默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个Region转移给其他的Region Server。</p>
</li>
<li><p>分裂时机：如果当前RegionServer上只有一个Region，当这个Region中的所有Store file的总和超过2*hbase.hregion.memstore.flush.size分裂，否则按照超过hbase.hregion.max.filesize 分裂。</p>
</li>
<li><p>这样切分会尽量使每个Table的在每个RegionServer中的Region数量尽量一致。</p>
</li>
</ul>
<h2 id="Hive与HBase集成"><a href="#Hive与HBase集成" class="headerlink" title="Hive与HBase集成"></a>Hive与HBase集成</h2><ul>
<li>hive默认是有jar包可与HBase集成的，可以用Hive对HBase中的数据进行分析，再将分析结果又返回到HBase中，方便其它的工作对分析好的数据的使用。</li>
</ul>
<h2 id="Phoenix"><a href="#Phoenix" class="headerlink" title="Phoenix"></a>Phoenix</h2><ul>
<li>Phoenix是HBase的开源SQL皮肤。可以使用标准JDBC API代替HBase客户端API来创建表，插入数据和查询HBase数据。</li>
<li>可以用类sql语言操作hbase</li>
<li>优势：支持索引。</li>
</ul>
]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive</title>
    <url>/2021/06/04/hive/</url>
    <content><![CDATA[<h1 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h1><h2 id="Hive基础知识"><a href="#Hive基础知识" class="headerlink" title="Hive基础知识"></a>Hive基础知识</h2><ul>
<li><p>Hive：由Facebook开源用于解决海量结构化日志的数据统计工具。</p>
<p>Hive是基于Hadoop的一个<em><strong>数据仓库工具</strong></em>，可以<em><strong>将结构化的数据文件映射为一张表</strong></em>，并<em><strong>提供类SQL查询功能</strong></em>。</p>
<p>本质是：将HQL转化成MapReduce程序</p>
</li>
<li><p>优点：1. 操作采用类SQL语法，提供快速开发的能力。</p>
</li>
<li><p>缺点： 1. 效率低</p>
</li>
<li><p>架构原理：<a href="https://imgtu.com/i/2YmPaT"><img src="https://z3.ax1x.com/2021/06/04/2YmPaT.png" alt="2YmPaT.png"></a></p>
</li>
<li><p>大致流程：Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。</p>
</li>
<li><p>Hive Beeline 是一个命令行工具，它是 Hive 的一部分，用于与 HiveServer2 进行交互。HiveServer2 是 Hive 的一个服务端组件，它提供了 JDBC&#x2F;ODBC 接口，允许外部客户端（如 Hive Beeline）连接并执行 HiveQL 查询。</p>
</li>
<li><p>hiveserver2，如果使用beeline，需要开启hiveserver2</p>
</li>
<li><p>架构解析：</p>
<ol>
<li><p>用户接口：Client.CLI（command-line interface）、JDBC&#x2F;ODBC(jdbc访问hive)、WEBUI（浏览器访问hive）</p>
</li>
<li><p>元数据：Meta store .元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列&#x2F;分区字段、表的类型（是否是外部表）、表的数据所在目录等；</p>
<p>默认存储在自带的derby数据库中（只支持但客户端访问），推荐使用MySQL存储Metastore（支持多客户端访问）</p>
<p>Metastore的作用是：客户端连接metastore服务，metastore再去连接MySQL数据库来存取元数据。有了metastore服务，就可以有多个客户端同时连接，而且这些客户端不需要知道MySQL数据库的用户名和密码，只需要连接metastore 服务即可。</p>
<ul>
<li>内嵌模式使用的是内嵌的Derby数据库来存储元数据，也不需要额外起Metastore服务。这个是默认的，配置简单，但是一次只能一个客户端连接，适用于用来实验，不适用于生产环境。</li>
<li>本地元存储和远程元存储都采用外部数据库来存储元数据，目前支持的数据库有：MySQL、Postgres、Oracle、MS SQL Server.在这里我们使用MySQL。</li>
<li>本地元存储和远程元存储的区别是：本地元存储不需要单独起metastore服务，用的是跟hive在同一个进程里的metastore服务。远程元存储需要单独起metastore服务，然后每个客户端都在配置文件里配置连接到该metastore服务。远程元存储的metastore服务和hive运行在不同的进程里。</li>
</ul>
</li>
<li><p>Hadooop.使用HDFS进行存储，使用MapReduce进行计算。</p>
</li>
<li><p>驱动器：Driver.</p>
<ol>
<li>包括解析器（SQL Parser）:将SQL字符串转换成抽象语法树AST</li>
<li>编译器（Physical Plan）：将AST编译生成逻辑执行计划。</li>
<li>优化器（Query Optimizer）：对逻辑执行计划进行优化。</li>
<li>执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR&#x2F;Spark&#x2F;Tez。</li>
</ol>
</li>
</ol>
</li>
<li><p>Tez引擎</p>
<ul>
<li>可以理解为一个加强版的MapReduce。比MapReduce更快，但是消耗更多的内存</li>
</ul>
</li>
<li><p>hive的数据类型</p>
<table>
<thead>
<tr>
<th>Hive数据类型</th>
<th>Java数据类型</th>
<th>长度</th>
<th>例子</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>SMALINT</td>
<td>short</td>
<td>2byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>INT</td>
<td>int</td>
<td>4byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BIGINT</td>
<td>long</td>
<td>8byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型，true或者false</td>
<td>TRUE  FALSE</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>double</td>
<td>双精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>STRING</td>
<td>string</td>
<td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td>
<td>‘now is the time’ “for all good men”</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td></td>
<td>时间类型</td>
<td></td>
</tr>
<tr>
<td>BINARY</td>
<td></td>
<td>字节数组</td>
<td></td>
</tr>
</tbody></table>
</li>
</ul>
<p>集合数据类型：</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td>STRUCT</td>
<td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td>
<td>struct()例如struct&lt;street:string, city:string&gt;</td>
</tr>
<tr>
<td>MAP</td>
<td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td>
<td>map()例如map&lt;string, int&gt;</td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td>
<td>Array()例如array<string></td>
</tr>
</tbody></table>
<ul>
<li>数据类型转化<ul>
<li>隐式转化   与java类似。1.所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE</li>
<li>强制转化 CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</li>
</ul>
</li>
</ul>
<h3 id="Hive-执行引擎"><a href="#Hive-执行引擎" class="headerlink" title="Hive 执行引擎"></a>Hive 执行引擎</h3><ul>
<li>hive前期版本默认是mapreduce。后面有tez引擎（基于内存较多，生成dag执行图，会做一些优化，比mr快很多，但对内存的要求也更高）</li>
<li><strong>Spark on Hive</strong><ul>
<li>就是同步Sparksql,加载hive的配置文件，获取到hive的元数据信息</li>
<li>Spark sql获取到hive的元数据信息之后就可以拿到hive的所有表的数据</li>
<li>接下来就可以通过spark sql来操作hive表中的数据</li>
</ul>
</li>
<li><strong>Hive on Spark</strong><ul>
<li>就是把hive查询从MR换成spark,参考官方的版本配置，不然很容易遇到依赖冲突。</li>
<li>之后还是在hive上写hivesql，但是执行引擎是spark，任务会转化成spark rdd算子去执行。</li>
<li>优化器还是hive的优化器，而没有spark sql自带的优化器的效果好</li>
</ul>
</li>
<li>二者的核心区别在于，客户端的 SQL 是否提交给了服务角色 HiveServer2 (org.apache.hive.service.server.HiveServer2)，且该hs2配置了 hive.execution.engine&#x3D;spark;</li>
</ul>
<h2 id="SQL语言的分类"><a href="#SQL语言的分类" class="headerlink" title="SQL语言的分类"></a>SQL语言的分类</h2><p><strong>SQL语言的分类</strong></p>
<p>SQL语言共分为四大类：数据查询语言DQL，数据操纵语言DML，数据定义语言DDL，数据控制语言DCL。</p>
<p><strong>1. 数据查询语言DQL</strong><br>数据查询语言DQL基本结构是由SELECT子句，FROM子句，WHERE<br>子句组成的查询块：<br>SELECT &lt;字段名表&gt;<br>FROM &lt;表或视图名&gt;<br>WHERE &lt;查询条件&gt;</p>
<p><strong>2 .数据操纵语言DML</strong><br>数据操纵语言DML主要有三种形式：</p>
<ol>
<li>插入：INSERT</li>
<li>更新：UPDATE</li>
<li>删除：DELETE</li>
</ol>
<p><strong>3. 数据定义语言DDL</strong><br>数据定义语言DDL用来创建数据库中的各种对象—–表、视图、<br>索引、同义词、聚簇等如：<br>CREATE TABLE&#x2F;VIEW&#x2F;INDEX&#x2F;SYN&#x2F;CLUSTER<br>| | | | |<br>表 视图 索引 同义词 簇</p>
<p>DDL操作是隐性提交的！不能rollback </p>
<p><strong>4. 数据控制语言DCL</strong><br>数据控制语言DCL用来授予或回收访问数据库的某种特权，并控制<br>数据库操纵事务发生的时间及效果，对数据库实行监视等。如：</p>
<ol>
<li><p>GRANT：授权。</p>
</li>
<li><p>ROLLBACK [WORK] TO [SAVEPOINT]：回退到某一点。<br>回滚—ROLLBACK<br>回滚命令使数据库状态回到上次最后提交的状态。其格式为：<br>SQL&gt;ROLLBACK;</p>
</li>
<li><p>COMMIT [WORK]：提交。</p>
</li>
</ol>
<p>  在数据库的插入、删除和修改操作时，只有当事务在提交到数据<br>库时才算完成。在事务提交前，只有操作数据库的这个人才能有权看<br>到所做的事情，别人只有在最后提交完成后才可以看到。<br>提交数据有三种类型：显式提交、隐式提交及自动提交。下面分<br>别说明这三种类型。</p>
<p>(1) 显式提交<br>用COMMIT命令直接完成的提交为显式提交。其格式为：<br>SQL&gt;COMMIT；</p>
<p>(2) 隐式提交<br>用SQL命令间接完成的提交为隐式提交。这些命令是：<br>ALTER，AUDIT，COMMENT，CONNECT，CREATE，DISCONNECT，DROP，<br>EXIT，GRANT，NOAUDIT，QUIT，REVOKE，RENAME。</p>
<p>(3) 自动提交<br>若把AUTOCOMMIT设置为ON，则在插入、修改、删除语句执行后，<br>系统将自动进行提交，这就是自动提交。其格式为：<br>SQL&gt;SET AUTOCOMMIT ON；</p>
<h2 id="DDL语句"><a href="#DDL语句" class="headerlink" title="DDL语句"></a>DDL语句</h2><ul>
<li><p>库的DDL</p>
</li>
<li><p>创建语句，location就相当于数据库，他们之间是有映射关系的</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">CREATE DATABASE [IF NOT EXISTS] database_name<br>[COMMENT database_comment]<br>[LOCATION hdfs_path]<br>[WITH DBPROPERTIES (property_name=property_value, ...)];<br><br>--显示数据库<br>show databases;<br>hive&gt; show databases like &#x27;db_hive*&#x27;;<br>OK<br>db_hive<br>db_hive_1<br><br>--显示数据库详细信息<br>desc database extended db_hive;<br><br>--删除数据库<br>drop database db_hive2;<br><br>--如果数据库不为空，可以采用cascade命令，强制删除<br>drop database db_hive cascade;<br></code></pre></td></tr></table></figure>


</li>
<li><p>表的DDL</p>
</li>
<li><p>创建表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name <br>[(col_name data_type [COMMENT col_comment], ...)] <br>[COMMENT table_comment] <br>[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] <br>[CLUSTERED BY (col_name, col_name, ...) <br>[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] <br>[ROW FORMAT row_format] <br>[STORED AS file_format] <br>[LOCATION hdfs_path]<br>[TBLPROPERTIES (property_name=property_value, ...)]<br>[AS select_statement]<br><br>--（7）ROW FORMAT <br>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]<br>        [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] <br>   | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]<br>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。<br><br></code></pre></td></tr></table></figure>
</li>
<li><p>内部表</p>
<ul>
<li>默认创建的是内部表，也叫管理表。当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。</li>
</ul>
</li>
<li><p>外部表</p>
<ul>
<li>创建时加上 external</li>
<li>以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</li>
<li><strong>结论: 外部表的数据不由hive自身负责管理，虽然数据会被加载到&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;，但是不由hive管理。</strong></li>
<li><strong>指定location：指定加载数据的位置，不再是默认加载到&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;目录下了。</strong></li>
</ul>
</li>
<li><p>内部表外部表转化</p>
<ul>
<li><p>修改内部表student2为外部表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">修改内部表student2为外部表<br>alter table student2 set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;TRUE&#x27;);<br><br>--查询表的类型<br>desc formatted student2;<br>--转换为内部表<br>alter table student2 set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;FALSE&#x27;);<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>分区表</p>
</li>
<li><p><font color=red>Hive中的分区就是分目录</font>，把一个大的数据集根据业务需要分割成小的数据集。</p>
<ul>
<li><pre><code class="hive">create table dept_partition(deptno int, dname string, loc string
)
partitioned by (month string)
row format delimited fields terminated by &#39;\t&#39;;
--分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。
-- 查询分区表的分区
show partitions dept_partition
--如果提前准备数据，但是没有元数据
--把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式
--1.添加分区
alter table dept_partition add partition(class=&quot;03&quot;)
--2.直接修复
msck repair table stu_par;
--3.上传带分区

--同时创建分区
alter table dept_partition add partition(month=&#39;201705&#39;), partition(month=&#39;201704&#39;);
--删除多个分区
 alter table dept_partition drop partition (month=&#39;201705&#39;), partition (month=&#39;201706&#39;);
<figure class="highlight lasso"><table><tr><td class="code"><pre><code class="hljs lasso">  <br>- 分区表不能转换，只能在建表时就建好<br><br>- 支持二级分区<br><br>  <span class="hljs-string">``</span><span class="hljs-string">``</span>hive<br>  create table dept_partition2(<br>                 deptno int, dname <span class="hljs-built_in">string</span>, loc <span class="hljs-built_in">string</span><br>                 )<br>                 partitioned <span class="hljs-keyword">by</span> (month <span class="hljs-built_in">string</span>, day <span class="hljs-built_in">string</span>)<br>                 row format delimited fields terminated <span class="hljs-keyword">by</span> <span class="hljs-string">&#x27;\t&#x27;</span>;<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p>修改表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">-- 修改表注释<br>ALTER TABLE 表名 SET TBLPROPERTIES(&#x27;comment&#x27; = &#x27;表注释内容&#x27;);<br>-- 修改表名<br>ALTER TABLE table_name RENAME TO new_table_name<br>-- 更新列<br>ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]<br>-- 增加和替换列<br>ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) <br>-- REPLACE 则是表示替换表中所有字段（修改不修改的列都需要写上）。<br>-- 删除表<br>drop table dept_partition;<br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="DML"><a href="#DML" class="headerlink" title="DML"></a>DML</h2><ul>
<li><p>数据导入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--创建语句<br>create table student(id string, name string) row format delimited fields terminated by &#x27;\t&#x27;;<br>--1.load导入数据<br>load data [local] inpath &#x27;/opt/module/datas/student.txt&#x27; [overwrite] into table student [partition (partcol1=val1,…)];<br>--本地数据导入<br>load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table default.student;<br>--hdfs数据导入<br>load data inpath &#x27;/user/zt/hive/student.txt&#x27; into table default.student;<br>--加载数据覆盖表中已有的数据<br>load data inpath &#x27;/user/zt/hive/student.txt&#x27; overwrite into table default.student;<br>--hdfs的导入是移动，本地导入是复制<br></code></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--2.通过查询语句向表中插入数据（Insert）<br> create table student_par(id int, name string) partitioned by (month string) row format delimited fields terminated by &#x27;\t&#x27;;<br> <br>insert overwrite table student partition(month=&#x27;201708&#x27;)<br>             select id, name from student where month=&#x27;201709&#x27;;<br>             <br>--insert into：以追加数据的方式插入到表或分区，原有数据不会删除<br>--insert overwrite：会覆盖表或分区中已存在的数据<br><br>--3.建表时用as select<br>create table if not exists student3<br>as select id, name from student;<br><br>--4.创建表时通过Location指定加载数据路径<br>create external table if not exists student5(<br>              id int, name string<br>              )<br>              row format delimited fields terminated by &#x27;\t&#x27;<br>              location &#x27;/student;&#x27;<br><br><br>-- 5.Import数据到指定Hive表中<br>import table student2 partition(month=&#x27;201709&#x27;) from<br> &#x27;/user/hive/warehouse/export/student&#x27;;<br></code></pre></td></tr></table></figure>


</li>
<li><p>数据导出(不重要)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--1. Insert导出<br>--将查询结果导出到本地<br>insert overwrite local directory &#x27;/opt/module/datas/export/student&#x27;<br>            select * from student;<br>            <br>--将查询的结果格式化导出到本地<br>insert overwrite local directory &#x27;/opt/module/datas/export/student1&#x27;<br>           ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;             select * from student;<br>           <br>--将查询的结果导出到HDFS上(没有local)<br>insert overwrite directory &#x27;/user/zt/student2&#x27;<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; <br>select * from student;<br>--整张表export 到处到HDFS<br>export table default.student to<br> &#x27;/user/hive/warehouse/export/student&#x27;;<br></code></pre></td></tr></table></figure>


</li>
<li><p>清除数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--Truncate只能删除管理表（内部表），不能删除外部表中数据<br>--只删除数据，不删除本身<br>truncate table student;<br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="DQL"><a href="#DQL" class="headerlink" title="DQL"></a>DQL</h2><h3 id="数据查询语言"><a href="#数据查询语言" class="headerlink" title="数据查询语言"></a>数据查询语言</h3><h4 id="1-基本查询（select…-from…-）"><a href="#1-基本查询（select…-from…-）" class="headerlink" title="1. 基本查询（select…..from…..）"></a>1. 基本查询（select…..from…..）</h4><p>（1）SQL 语言大小写不敏感。 </p>
<p>（2）SQL 可以写在一行或者多行</p>
<p>（3）关键字不能被缩写也不能分行</p>
<p>（4）各子句一般要分行写。</p>
<p>（5）使用缩进提高语句的可读性。</p>
<ul>
<li><p>别名</p>
<ul>
<li><p>紧跟列名，也可以在列名和别名之间加入关键字‘AS’</p>
</li>
<li><p>select ename AS name, deptno dn from emp;</p>
</li>
</ul>
</li>
<li><p>算数运算符</p>
<ul>
<li>select sal +1 from emp;</li>
</ul>
<table>
<thead>
<tr>
<th>运算符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>A+B</td>
<td>A和B 相加</td>
</tr>
<tr>
<td>A-B</td>
<td>A减去B</td>
</tr>
<tr>
<td>A*B</td>
<td>A和B 相乘</td>
</tr>
<tr>
<td>A&#x2F;B</td>
<td>A除以B</td>
</tr>
<tr>
<td>A%B</td>
<td>A对B取余</td>
</tr>
<tr>
<td>A&amp;B</td>
<td>A和B按位取与</td>
</tr>
<tr>
<td>A|B</td>
<td>A和B按位取或</td>
</tr>
<tr>
<td>A^B</td>
<td>A和B按位取异或</td>
</tr>
<tr>
<td>~A</td>
<td>A按位取反</td>
</tr>
</tbody></table>
</li>
<li><p>常用函数</p>
<ul>
<li>UDF函数：一个输入一个输出 select substring(ename,1,1) from emp;（从ｅname的1开始，取一个字符）用户定义（普通）函数，只对单行数值产生作用。实现：继承UDF，实现evaluate()方法</li>
<li>UDAF函数：多个输入，一个输出 select count(*) cnt from emp;用户定义聚合函数，可对多行数据产生作用；等同与SQL中常用的SUM()，AVG()，也是聚合函数；</li>
<li>UDTF函数：一个输入，多个输出。用户定义表生成函数。用来解决输入一行输出多行；实现：继承GenericUDTF，实现initialize(),process(),close()方法</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--1．求总行数（count）<br>select count(*) cnt from emp;<br>--2．求工资的最大值（max）<br>select max(sal) max_sal from emp;<br>--3．求工资的最小值（min）<br>select min(sal) min_sal from emp;<br>--4．求工资的总和（sum）<br>select sum(sal) sum_sal from emp; <br>--5．求工资的平均值（avg）<br>select avg(sal) avg_sal from emp;<br></code></pre></td></tr></table></figure>
</li>
<li><p>LIMIT子句用于限制返回的行数</p>
<ul>
<li>select * from emp limit 5;</li>
</ul>
</li>
</ul>
<h4 id="2-条件过滤"><a href="#2-条件过滤" class="headerlink" title="2.条件过滤"></a>2.条件过滤</h4><ol>
<li><p>使用where子句，将不满足条件的行过滤掉</p>
<p>select * from emp where sal &#x3D;5000;</p>
</li>
<li><p>比较运算符：下面表中描述了谓词操作符，这些操作符同样可以用于JOIN…ON和HAVING语句中</p>
<table>
<thead>
<tr>
<th>操作符</th>
<th>支持的数据类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>A&#x3D;B</td>
<td>基本数据类型</td>
<td>如果A等于B则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;&#x3D;&gt;B</td>
<td>基本数据类型</td>
<td>如果A和B都为NULL，则返回TRUE，其他的和等号（&#x3D;）操作符的结果一致，如果任一为NULL则结果为NULL</td>
</tr>
<tr>
<td>A&lt;&gt;B, A!&#x3D;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;&#x3D;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&gt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&gt;&#x3D;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A [NOT] BETWEEN B AND C</td>
<td>基本数据类型</td>
<td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td>A IS NULL</td>
<td>所有数据类型</td>
<td>如果A等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A IS NOT NULL</td>
<td>所有数据类型</td>
<td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>IN(数值1, 数值2)</td>
<td>所有数据类型</td>
<td>使用 IN运算显示列表中的值</td>
</tr>
<tr>
<td>A [NOT] LIKE B</td>
<td>STRING 类型</td>
<td>B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td>A RLIKE B, A REGEXP B</td>
<td>STRING 类型</td>
<td>B是基于java的正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td>
</tr>
</tbody></table>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--通配符字符串匹配　% _<br>--%匹配任意串，_匹配任意字符<br>--查询以A开头的员工<br>select * from emp where ename like &quot;A%&quot;;<br><br>--正则匹配<br>--查询以A开头的员工<br>select * from emp where ename rlike &quot;^A&quot;; <br></code></pre></td></tr></table></figure>

<ul>
<li><p>rlike匹配正则表达式</p>
</li>
<li><p>正则表达式</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><code class="hljs powershell">一般字符匹配自己<br>^ 匹配一行开头 ^R 以<span class="hljs-built_in">R</span>开头<br><span class="hljs-variable">$</span> 匹配一行结束 <span class="hljs-built_in">R</span><span class="hljs-variable">$</span> 以<span class="hljs-built_in">R</span>结尾<br>. 匹配任意字符 ^.<span class="hljs-variable">$</span> 一行只有一个字符<br>* 前一个子式匹配零次或多次<br>.*匹配任意字符<br>[] 匹配一个范围内的任意字符<br>\ 转义<br></code></pre></td></tr></table></figure></li>
</ul>
<ol start="3">
<li><p>逻辑运算符（AND，OR，NOT）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">select * from emp where sal&gt;1000 and deptno=30;<br>select * from emp where sal&gt;1000 or deptno=30;<br>select * from emp where deptno not IN(30, 20);<br></code></pre></td></tr></table></figure></li>
</ol>
<h4 id="3-分组"><a href="#3-分组" class="headerlink" title="3. 分组"></a>3. 分组</h4><ul>
<li><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。</p>
</li>
<li><p>（1）where后面不能写分组函数，而having后面可以使用分组函数。</p>
<p>（2）having只用于group by分组统计语句。</p>
</li>
<li><pre><code class="hive">--计算emp表每个部门的平均工资
select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno
--求每个部门的平均薪水大于2000的部门
select deptno, avg(sal) avg_sal from emp group by deptno having
 avg_sal &gt; 2000;
<figure class="highlight n1ql"><table><tr><td class="code"><pre><code class="hljs n1ql"><br>#### 4.连接<br><br>* Hive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。<br><br>  - 内连接<br>  - 左外连接<br>  - 右外连接<br>  - 满外连接<br><br>  ````hive<br>  <span class="hljs-keyword">select</span><br>      <span class="hljs-built_in">e</span>.empno,<br>      <span class="hljs-built_in">e</span>.ename,<br>      d.deptno, <br>      d.dname <br>  <span class="hljs-keyword">from</span> <br>      emp <span class="hljs-built_in">e</span> <br>  <span class="hljs-keyword">join</span><br>      dept d <br>  <span class="hljs-keyword">on</span><br>      <span class="hljs-built_in">e</span>.deptno = d.deptno;<br>      <br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>表的别名：好处：1.简化查询，使用表名前缀可以提高执行效率</p>
</li>
<li><p>多表连接</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">SELECT <br>    e.ename,<br>    d.dname, <br>    l.loc_name<br>FROM<br>	emp e <br>JOIN<br>	dept d<br>ON<br>	d.deptno = e.deptno <br>JOIN<br>	location l<br>ON<br>	d.loc = l.loc;<br></code></pre></td></tr></table></figure>


</li>
<li><p><strong>hive join目前不支持在on子句中使用谓词or(hive1 不支持)</strong></p>
<p>select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno</p>
<p>&#x3D; d.deptno or e.ename&#x3D;d.deptno;  在hive1错误的,hive3支持</p>
</li>
</ul>
<h4 id="5-排序"><a href="#5-排序" class="headerlink" title="5.排序"></a>5.排序</h4><ul>
<li>order by:全局排序，只有一个Reducer(极易造成数据倾斜)</li>
<li>ASC（ascend）: 升序（默认）</li>
<li>DESC（descend）: 降序</li>
<li>sort by:局部排序（sort by 为每个reduce产生一个排序文件。每个Reduce内部进行排序，对全局结果来说不是排序）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--一般需求不会要求给所有的数据排序，而要求知道前几<br>--求工资前10的人，Map会先求局部前10<br>select *<br>from emp<br>order by sal desc<br>limit 10;<br><br>--还有一种可能，我们只需要看大概的数据趋势，不需要全排序<br>--Hive的局部排序 sort by<br>select *<br>from emp<br>sort by empno desc;<br><br>--多条件排序，先按部门排序，再按工资排序<br>select *<br>from emp<br>order by<br>deptno asc,<br>sal desc;<br><br>--limit,offset<br>limit X,Y 跳过X条数据，取Y条数据<br>offset X 跳过X条数据<br></code></pre></td></tr></table></figure>

<ul>
<li><p>分区排序 （Distribute By）</p>
</li>
<li><p><em><strong>distribute by</strong></em>类似MR中partition（自定义分区），进行分区，结合sort by使用。 </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--指定局部排序的分区字段<br>select * from emp<br>distribute by empno<br>sort by sal desc;<br><br>--如果分区和排序的字段一样，我们可以用cluster by代替<br>select * from emp distribute by empno sort by empno;<br>select * from emp cluster by empno;<br></code></pre></td></tr></table></figure>
</li>
<li><p>当distribute by和sorts by字段相同时，可以使用cluster by方式。</p>
</li>
</ul>
<h4 id="6-分桶"><a href="#6-分桶" class="headerlink" title="6.分桶"></a>6.分桶</h4><ul>
<li><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</p>
</li>
<li><p>分区针对的是数据的存储路径；分桶针对的是数据文件。</p>
</li>
<li><p>分桶：针对某一个区的数据，把它的数据进一步组织成多个文件</p>
</li>
<li><p>分区：把多个数据，分成文件夹管理</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">create table stu_buck(id int, name string)<br>clustered by(id) <br>into 4 buckets<br>row format delimited fields terminated by &#x27;\t&#x27;;<br><br><br></code></pre></td></tr></table></figure>
</li>
<li><p>分桶抽样查询</p>
</li>
<li><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">select * from stu_buck tablesample(bucket 1 out of 4 on id);<br>--tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) <br>--y必须是table总bucket数的倍数或者因子(因子就是所有可以整除这个数的数,不包括这个数自身)。hive根据y的大小，决定抽样的比例<br>把数据按照bucket分成y份，取其中的第x份<br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="常用查询函数"><a href="#常用查询函数" class="headerlink" title="常用查询函数"></a>常用查询函数</h3><h4 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">hive中查询函数<br>show functions<br>show functions like &quot;collect*&quot;<br>查看函数的描述<br>desc function 函数名<br>--nvl空字段赋值<br>select comm, nvl(comm, -1) from emp;<br></code></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--case when<br>--统计不同部门男女各有多少人<br>select<br>    dept_id,<br>    count(*) total,<br>    sum(case sex when &#x27;男&#x27; then 1 else 0 end) male,<br>    sum(case sex when &#x27;女&#x27; then 1 else 0 end) female<br>from<br>    emp_sex<br>group by<br>    dept_id;<br></code></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">在 Group by 子句中，Select 查询的列，要么需要是 Group by 中的列，要么得是用聚合函数（比如 sum、count 等）加工过的列。不支持直接引用非 Group by 的列。这一点和 MySQL 有所区别。Hive 错误 Expression not in GROUP BY key的原因。<br>--行转列<br>collect_list(x),聚合成一个数组，聚合函数<br>concat_ws(&quot;分隔符&quot;，数组)，把数组按分割符拼成一个字符串<br>contact(str1,str2......,strn)拼接几列在一起<br><br>select<br>    concat(constellation,&quot;,&quot;,blood_type) xzxx,<br>    concat_ws(&quot;|&quot;, collect_list(name)) rentou<br>from<br>    person_info<br>group by<br>    constellation,blood_type;<br></code></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--列转行<br>--explode(a)函数<br>--如果传入的是一个数组，则将其分成多行<br>--如果传入一个map,按照key,value分成两列<br>--split(str,regrex)函数<br>--将一个字符串按照正则表达式规则划分成一个数组<br>lateral view 后面接一个表名，起一个列名，列名取决于explode()炸开后的效果。<br>select<br>    m.movie,<br>    tbl.cate<br>from<br>    movie_info m<br>lateral view<br>    explode(split(category, &quot;,&quot;)) tbl as cate;<br></code></pre></td></tr></table></figure>

<h4 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h4><ul>
<li><p>相关函数说明</p>
<p>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。</p>
<p>CURRENT ROW：当前行</p>
<p>n PRECEDING：往前n行数据</p>
<p>n FOLLOWING：往后n行数据</p>
<p>UNBOUNDED：UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点</p>
<p>LAG(col,n,default_val)：往前第n行数据</p>
<p>LEAD(col,n, default_val)：往后第n行数据</p>
<p>NTILE(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。</p>
<p>percent_rank()将数据按百分比分</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--聚合<br>select name,count(*) over () <br>from business <br>where substring(orderdate,1,7) = &#x27;2017-04&#x27; <br>group by name;<br></code></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--各种聚合<br>select name,orderdate,cost, <br>sum(cost) over() as sample1,--所有行相加 <br>sum(cost) over(partition by name) as sample2,--按name分组，组内数据相加 <br>sum(cost) over(partition by name order by orderdate) as sample3,--按name分组，组内数据累加 <br>sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,--和sample3一样,由起点到当前行的聚合 <br>sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, --当前行和前面一行做聚合 <br>sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行 <br>sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行 <br>from business;<br></code></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--结合其他函数使用<br>select<br>    name, orderdate, cost, <br>    lag(orderdate, 1) <br>    over(partition by name order by orderdate) last_order,<br>    lead(orderdate, 1) <br>    over(partition by name order by orderdate) next_order<br>from<br>    business;<br></code></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--ntile<br>Ntile(group_num) 将所有记录分成group_num个组，每组序号一样<br>SELECT<br>	*<br>FROM<br>	(<br>		select name,<br>		orderdate,<br>		cost,<br>		ntile(5) over(<br>		order by orderdate) n<br>	from<br>		business) t1<br>WHERE<br>	n = 1;<br></code></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">-- percent_rank<br>-- percent_rank() 含义就是 当前行-1 / 当前组总行数-1<br>select<br>	name,<br>	orderdate,<br>	cost,<br>	PERCENT_RANK() over(<br>	order by orderdate) pr<br>from<br>	business;<br></code></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--rank<br>rank()排序，相同的一样排名，数字按照实际的来，类似于高考排名<br>dense_rank() 相同的一样排名，数字按照排名的数字来<br>row_number() 直接排名，相同的排名也不一样<br>SELECT<br>	*,<br>	rank() OVER(partition by subject<br>order by<br>	score desc) r,<br>	DENSE_RANK() OVER(partition by subject<br>order by<br>	score desc) dr,<br>	ROW_NUMBER() OVER(partition by subject<br>order by<br>	score desc) rn<br>from<br>	score;<br></code></pre></td></tr></table></figure>

<h4 id="日期函数"><a href="#日期函数" class="headerlink" title="日期函数"></a>日期函数</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--current_date 返回当前日期<br>select current_date();<br></code></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">-- 日期的加减<br>-- 今天开始90天以后的日期<br>select date_add(current_date(), 90);<br>-- 今天开始90天以前的日期<br>select date_sub(current_date(), 90);<br></code></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--日期差<br>SELECT datediff(CURRENT_DATE(), &quot;1990-06-04&quot;);<br></code></pre></td></tr></table></figure>

<p>习题：有哪些顾客连续两天来过我的店，数据是business表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">--习题:连续登录<br>-- 有哪些顾客连续两天来过我的店，数据是business表<br><br>--time1下一次购买商品的时间<br>select<br>name,cost,orderdate,<br>lead(orderdate,1,&quot;2021-09-01&quot;) over(partition by name order by orderdate) time1<br>from<br>business<br><br>--时间差<br>select<br>name,cost,orderdate,time1,<br>datediff(time1,orderdate) difftime<br>from<br>    (select<br>name,cost,orderdate,<br>lead(orderdate,1,&quot;2021-09-01&quot;) over(partition by name order by orderdate) time1<br>from<br>business) tab;<br><br>--找到时间差为1的人<br>select<br>name,corderdate,time1,difftime<br>from<br>(select<br>name,cost,orderdate,time1,<br>datediff(time1,orderdate) difftime<br>from<br>    (select<br>name,cost,orderdate,<br>lead(orderdate,1,&quot;2021-09-01&quot;) over(partition by name order by orderdate) time1<br>from<br>business) tab) tab1<br>where<br>difftime=1;<br><br></code></pre></td></tr></table></figure>

<ul>
<li>hive重点：写sql,熟练使用函数，尤其是开窗函数</li>
</ul>
<h3 id="SQL一般执行顺序"><a href="#SQL一般执行顺序" class="headerlink" title="SQL一般执行顺序"></a>SQL一般执行顺序</h3><ol>
<li>from 确定基表</li>
<li>join on 如果一张基表不够, 再联接其他表,或者lateral view explode(需炸裂的列) table_name as 炸裂后的列名 </li>
<li>where 过滤总基表中的行</li>
<li>group by 分组, 分组依据的列.（可以开始使用select中的别名，从group 开始往后都可用）</li>
<li>select 把分组依据的列放在select后, 再考虑要选择哪些列, 及进行哪些函数调用 sum(),count(1)等</li>
<li>having 进一步把分组后的虚表行过滤</li>
<li><em><strong>窗口函数</strong></em>，select中若包含over()开窗函数，<strong>执行完非开窗函数后，select等待执行完开窗函数，随后select执行结束</strong>，开窗函数通过表数据进行分区和排序，跟select查询中的字段是平行关系，不依赖查询字段。</li>
<li>distinct</li>
<li>order by 最终表的一个排序显示.</li>
<li>limit</li>
</ol>
<h2 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h2><h3 id="小文件优化"><a href="#小文件优化" class="headerlink" title="小文件优化"></a>小文件优化</h3><ul>
<li><p>小文件的产生：</p>
<ul>
<li>动态分区插入数据，产生大量小文件，导致map数量剧增</li>
<li>reduce个数等于输出的文件个数，如果reduce个数多同时文件大小小的话就会产业很多小文件</li>
<li><strong>用datax同步数据时，设则多线程，如果数据量不大的情况下，生成了小文件</strong></li>
</ul>
</li>
<li><p>带来的问题：</p>
<ul>
<li>小文件会导致开启很多很多map，一个map开一个JVM执行，所以这些任务的初始化，启动，执行会浪费大量的资源，严重影响性能。</li>
<li>小文件也会占用NameNode元数据的内存，如果太多小文件的话，会占用很多的NameNode的内存</li>
</ul>
</li>
<li><p>常用的一些hive参数设置，可以做为一些通用的优化手段。但是对于没有效果的的sql代码还是需要具体情况具体分析</p>
</li>
<li><pre><code class="hive">-- 执行Map前进行小文件合并
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
-- 填的数都为byte数
-- 每个Map最大输入大小(这个值决定了合并后文件的数量)
set mapred.max.split.size=128000000;
-- 每个map最小输入大小
set mapred.min.split.size=10000000;

-- 上面的参数配置同时也要考虑到：1.文件的格式是否可以切分；2.是否开启了压缩等因素

-- 一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)
set mapred.min.split.size.per.node=100000000;

-- 一个机架下split的至少的大小(这个值决定了多个机架上的文件是否需要合并)
set mapred.min.split.size.per.rack=100000000;

-- 这四个参数是有优先级的，一般来说优先级如下：
-- max.split.size &lt;= min.split.size &lt;= min.size.per.node &lt;= min.size.per.rack

-- 要想达到控制map任务个数的效果，要按照优先级合理配置参数的值
-- 一般来说按照上面配置参数后可以到达map数约等于文件大小/128 个（但是要注意文件是否压缩，是否可切分等情况）
/**
-- map数量由三个方面决定
1.文件个数
2.文件大小
3.blocksize
-- 因为map端一般都会默认开启聚合小文件的参数，所以文件个数我们先不考虑
-- 一般情况下，粗略计算任务map个数=文件大小/blocksize
*/


-- 设置map端输出进行合并，默认为true
set hive.merge.mapfiles = true;

-- 设置reduce端输出进行合并，默认为false
set hive.merge.mapredfiles = true;

-- 设置合并文件的大小
set hive.merge.size.per.task = 128000000;

-- 当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。
set hive.merge.smallfiles.avgsize=16000000;

-- 每个reducer任务处理的数据量
set hive.exec.reducers.bytes.per.reducer=128000000;
-- 4999 每个任务的最大reducer数量
hive.exec.reducers.max=4999;

-- reduce任务个数一般等于map输出的文件大小/hive.exec.reducers.bytes.per.reduce 的值。（也要考虑文件是否压缩的情况）

-- 或者指定设置reduce个数。-1表示不指定
set mapreduce.job.reduces=-1;
/**
一些情况指定设置reduce个数，是无效的
1.order by 最终只会生成一个reduce
2.笛卡尔积，笛卡尔积也是全局聚合，只能一个reduce处理
2.map端输出的数据量很小
*/
<figure class="highlight routeros"><table><tr><td class="code"><pre><code class="hljs routeros">  <br>* 预防小文件的一个基础通用配置，可以默认放在sql前面<br><br>  ````hive<br>  -- 设置Hive输入,执行map前进行小文件合并<br>  <span class="hljs-built_in">set</span> hive.input.<span class="hljs-attribute">format</span>=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;<br>  -- 每个map最大输入大小<br>  <span class="hljs-built_in">set</span> mapred.max.split.<span class="hljs-attribute">size</span>=256000000;<br>  -- 每个map最小输入大小<br>  <span class="hljs-built_in">set</span> mapred.min.split.<span class="hljs-attribute">size</span>=128000000;<br>  --节点上最小分片大小,每个节点处理的最小split<br>  <span class="hljs-built_in">set</span> mapred.min.split.size.per.<span class="hljs-attribute">node</span>=128000000;<br>  -- 机架上最小分片大小,决定不同机架的DataNode上文件是都进行合并。每个机架处理的最小split<br>  <span class="hljs-built_in">set</span> mapred.min.split.size.per.<span class="hljs-attribute">rack</span>=128000000;<br>  <br>  --输出合并<br>  -- 设置map端输出进行合并，默认为<span class="hljs-literal">true</span><br>  <span class="hljs-built_in">set</span> hive.merge.mapfiles = <span class="hljs-literal">true</span>;<br>  -- 设置reduce端输出进行合并，默认为<span class="hljs-literal">false</span><br>  <span class="hljs-built_in">set</span> hive.merge.mapredfiles = <span class="hljs-literal">true</span>;<br>  -- 小于这个值会开启一个独立的mapreduce任务进行小文件合并,默认16m<br>  <span class="hljs-built_in">set</span> hive.merge.smallfiles.<span class="hljs-attribute">avgsize</span>=16000000;<br>  -- 合并后的文件大小,默认256m,推荐128m,一个hdfs分块的大小<br>  <span class="hljs-built_in">set</span> hive.merge.size.per.<span class="hljs-attribute">task</span>=128000000;<br></code></pre></td></tr></table></figure>

</code></pre>
</li>
<li><p>distribute by处理已有的小文件</p>
</li>
<li><pre><code class="hive">insert overwrite table 目标表 [partition(hour=...)] select * from 目标表 
distribute by floor( rand() * 具体最后落地生成多少个文件数);
distribute by `floor`(rand() * 5)
/**
rand()函数生成一个介于0（包含）和1（不包含）之间的随机浮点数。
这个随机数乘以5，得到一个介于0（包含）和5（不包含）之间的随机浮点数。
floor()函数取这个浮点数的整数部分，结果将是0、1、2、3或4中的一个。
因此，floor(rand() * 5)将生成一个随机整数，这个整数用作分发的键，将数据随机分配到5个reducer中的一个。
*/

/**
作用
数据随机化：如果你想要随机地将数据分发到reducer上，这可能是有用的。例如，如果你正在进行随机采样或者想要确保数据在reducer之间均匀分布。
测试：在测试Hive查询或MapReduce作业时，随机分配数据可以帮助你检查代码是否能够正确处理不同的reducer分配情况。
负载均衡：如果某个键的值非常倾斜（即大部分数据都属于同一个键），使用随机分配可以帮助平衡reducer之间的负载。

注意：
需要注意的是，使用rand()可能会导致性能问题，因为每个输入行都会调用rand()函数，这可能会增加计算的开销。此外，这种方法可能会导致数据分布不均匀，因为随机性意味着无法保证每个reducer获得相同数量的数据。在使用这种方法时，应该考虑到这些潜在的问题。
*/
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><br><span class="hljs-operator">*</span> concatenate 命令<br><br>  <span class="hljs-operator">-</span> ````hive<br>    <span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> test [<span class="hljs-keyword">partition</span>(...)] concatenate<br>    <br>    <br>    <span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> ods.ods_qxb_t_report_details_di <span class="hljs-keyword">partition</span>(ds<span class="hljs-operator">=</span><span class="hljs-string">&#x27;20240328&#x27;</span>) concatenate;<br>    <span class="hljs-comment">-- 这种方法仅仅适用于orc格式存储的表</span><br>    <span class="hljs-comment">-- 只能内部表使用</span><br>    <span class="hljs-comment">-- Concatenate/Merge can only be performed on managed tables</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<h3 id="常见参数调优"><a href="#常见参数调优" class="headerlink" title="常见参数调优"></a>常见参数调优</h3><ul>
<li><pre><code class="hive">-- 修改引擎
set hive.execution.engine=tez;
-- 用于避免小文件的场景或者task特别多的场景，这类场景大多数执行时间都很短，因为hive调起mapreduce任务，JVM的启动过程会造成很大的开销，尤其是job有成千上万个task任务时，JVM重用可以使得JVM实例在同一个job中重新使用N次
set mapred.job.reuse.jvm.num.tasks=10; --10为重用个数
<figure class="highlight arcade"><table><tr><td class="code"><pre><code class="hljs arcade"><br>* 数据倾斜<br><br>  - 表现：任务进度长时间维持在<span class="hljs-number">99</span>%（或<span class="hljs-number">100</span>%），查看任务监控页面，发现只有少量（<span class="hljs-number">1</span>个或几个）<span class="hljs-built_in">reduce</span>子任务未完成。因为其处理的数据量和其他<span class="hljs-built_in">reduce</span>差异过大。单一<span class="hljs-built_in">reduce</span>的记录数与平均记录数差异过大，通常可能达到<span class="hljs-number">3</span>倍甚至更多。最长时长远大于平均时长。<br><br>  - 原因<br><br>    - key分布不均匀<br>    - 业务数据本身的特性<br>    - 建表时考虑不周<br>    - 某些sql语句本身就有数据倾斜<br>    - [![pAkiuTA.png](https:<span class="hljs-comment">//s21.ax1x.com/2024/08/26/pAkiuTA.png)](https://imgse.com/i/pAkiuTA)</span><br><br>  - 参数优化<br><br>    <span class="hljs-string">``</span><span class="hljs-string">``</span>hive<br>    -- <span class="hljs-built_in">map</span>端开启聚合<br>    set hive.<span class="hljs-built_in">map</span>.aggr=<span class="hljs-literal">true</span>;<br>    -- 数据倾斜时生成两个MRJOB，第一个MR，先随机打散key，减少数据倾斜。第二个MR，再根据预处理的数据结果按照Group By Key分布到<span class="hljs-built_in">reduce</span>中，最终完成<br>    set hive.<span class="hljs-built_in">groupby</span>.skewindata=<span class="hljs-literal">true</span>;<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<h3 id="Map-Join"><a href="#Map-Join" class="headerlink" title="Map Join"></a>Map Join</h3><ul>
<li><p>适用于大表join小表。将小表放入内存。由于表的JOIN操作是在Map端且在内存进行的，所以其并不需要启动Reduce任务也就不需要经过shuffle阶段，从而能在一定程度上节省资源提高JOIN效率。</p>
</li>
<li><p>在Hive0.11前，需要指定mapjoin才能操作</p>
</li>
<li><pre><code class="hive">SELECT /*+ MAPJOIN(smalltable)*/  .key,value
FROM smalltable JOIN bigtable ON smalltable.key = bigtable.key

-- 在高版本中：如果想用上面这种语法
set hive.auto.convert.join=false;-- (关闭自动MAPJOIN转换操作)
set hive.ignore.mapjoin.hint=false;-- (不忽略MAPJOIN标记)
-- 开启无条件转Map Join
set hive.auto.convert.join.noconditionaltask=false;


-- 建议不使用显示指定，调整自动mapjoin的一些参数就行
<figure class="highlight mathematica"><table><tr><td class="code"><pre><code class="hljs mathematica"><br><span class="hljs-operator">*</span> 后面的版本，不用显示的指定<span class="hljs-variable">mapjoin</span>。<span class="hljs-variable">hive</span>优化器根据参与<span class="hljs-variable">join</span>的表的数据量大小，<span class="hljs-operator">**</span>自动触发<span class="hljs-operator">**</span><br><br><span class="hljs-operator">*</span> <span class="hljs-variable">Hive</span>在编译<span class="hljs-variable">SQL</span>语句阶段，起初所有的<span class="hljs-variable">join</span>操作均采用<span class="hljs-variable">Common</span> <span class="hljs-built_in">Join</span>算法实现。<br><br>  之后在物理优化阶段：<br><br>  根据每个<span class="hljs-variable">Common</span> <span class="hljs-built_in">Join</span>任务所需表的大小判断该<span class="hljs-variable">Common</span> <span class="hljs-built_in">Join</span>任务是否能够转换为<span class="hljs-built_in">Map</span> <span class="hljs-built_in">Join</span>任务，若满足要求，便将<span class="hljs-variable">Common</span> <span class="hljs-built_in">Join</span>任务自动转换为<span class="hljs-built_in">Map</span> <span class="hljs-built_in">Join</span>任务。<br><br>  如果在<span class="hljs-variable">SQL</span>的编译阶段不能确定是否能够转换的，（例如对子查询进行<span class="hljs-variable">join</span>操作）。<br><br>  针对这种情况，<span class="hljs-variable">Hive</span>会在编译阶段生成一个条件任务（<span class="hljs-variable">Conditional</span> <span class="hljs-variable">Task</span>）<br><br><span class="hljs-operator">*</span> 自动触发的一些参数条件<br><br>  <span class="hljs-operator">````</span><span class="hljs-variable">hive</span><br>  <br>  <span class="hljs-operator">--</span>启动<span class="hljs-built_in">Map</span> <span class="hljs-built_in">Join</span>自动转换<br>  <span class="hljs-variable">set</span> <span class="hljs-variable">hive</span><span class="hljs-operator">.</span><span class="hljs-variable">auto</span><span class="hljs-operator">.</span><span class="hljs-variable">convert</span><span class="hljs-operator">.</span><span class="hljs-variable">join</span><span class="hljs-operator">=</span><span class="hljs-variable">true</span><span class="hljs-operator">;</span><br>   <br>  <span class="hljs-operator">--</span> 一个<span class="hljs-variable">Common</span> <span class="hljs-built_in">Join</span> <span class="hljs-variable">operator</span>转为<span class="hljs-built_in">Map</span> <span class="hljs-built_in">Join</span> <span class="hljs-variable">operator</span>的判断条件<span class="hljs-operator">,</span><br>  <span class="hljs-operator">--</span> 若该<span class="hljs-variable">Common</span> <span class="hljs-built_in">Join</span>相关的表中<span class="hljs-operator">,</span>存在<span class="hljs-variable">n</span><span class="hljs-operator">-</span><span class="hljs-number">1</span>张表的已知大小总和<span class="hljs-operator">&lt;=</span>该值<span class="hljs-operator">,</span>则生成一个<span class="hljs-built_in">Map</span> <span class="hljs-built_in">Join</span>计划<span class="hljs-operator">,</span><br>  <span class="hljs-operator">--</span> 此时可能存在多种<span class="hljs-variable">n</span><span class="hljs-operator">-</span><span class="hljs-number">1</span>张表的组合均满足该条件<span class="hljs-operator">,</span>则<span class="hljs-variable">hive</span>会为每种满足条件的组合均生成一个<span class="hljs-built_in">Map</span> <span class="hljs-built_in">Join</span>计划<span class="hljs-operator">,</span><br>  <span class="hljs-operator">--</span> 同时还会保留原有的<span class="hljs-variable">Common</span> <span class="hljs-built_in">Join</span>计划作为后备<span class="hljs-punctuation">(</span><span class="hljs-variable">back</span> <span class="hljs-variable">up</span><span class="hljs-punctuation">)</span>计划<br>  <span class="hljs-operator">--</span> 实际运行时<span class="hljs-operator">,</span>优先执行<span class="hljs-built_in">Map</span> <span class="hljs-built_in">Join</span>计划，若不能执行成功，则启动<span class="hljs-variable">Common</span> <span class="hljs-built_in">Join</span>后备计划。<br>  <span class="hljs-variable">set</span> <span class="hljs-variable">hive</span><span class="hljs-operator">.</span><span class="hljs-variable">mapjoin</span><span class="hljs-operator">.</span><span class="hljs-variable">smalltable</span><span class="hljs-operator">.</span><span class="hljs-variable">filesize</span><span class="hljs-operator">=</span><span class="hljs-number">250000</span><span class="hljs-operator">;</span><br>   <br>  <span class="hljs-operator">--</span> 开启无条件转<span class="hljs-built_in">Map</span> <span class="hljs-built_in">Join</span><br>  <span class="hljs-variable">set</span> <span class="hljs-variable">hive</span><span class="hljs-operator">.</span><span class="hljs-variable">auto</span><span class="hljs-operator">.</span><span class="hljs-variable">convert</span><span class="hljs-operator">.</span><span class="hljs-variable">join</span><span class="hljs-operator">.</span><span class="hljs-variable">noconditionaltask</span><span class="hljs-operator">=</span><span class="hljs-variable">true</span><span class="hljs-operator">;</span><br>   <br>  <span class="hljs-operator">--</span> 无条件转<span class="hljs-built_in">Map</span> <span class="hljs-built_in">Join</span>时的小表之和阈值<span class="hljs-operator">,</span>若一个<span class="hljs-variable">Common</span> <span class="hljs-built_in">Join</span> <span class="hljs-variable">operator</span>相关的表中，<br>  <span class="hljs-operator">--</span> 存在<span class="hljs-variable">n</span><span class="hljs-operator">-</span><span class="hljs-number">1</span>张表的大小总和<span class="hljs-operator">&lt;=</span>该值<span class="hljs-operator">,</span>此时<span class="hljs-variable">hive</span>便不会再为每种<span class="hljs-variable">n</span><span class="hljs-operator">-</span><span class="hljs-number">1</span>张表的组合均生成<span class="hljs-built_in">Map</span> <span class="hljs-built_in">Join</span>计划<br>  <span class="hljs-operator">--</span> 同时也不会保留<span class="hljs-variable">Common</span> <span class="hljs-built_in">Join</span>作为后备计划。而是只生成一个最优的<span class="hljs-built_in">Map</span> <span class="hljs-built_in">Join</span>计划。<br>  <span class="hljs-variable">set</span> <span class="hljs-variable">hive</span><span class="hljs-operator">.</span><span class="hljs-variable">auto</span><span class="hljs-operator">.</span><span class="hljs-variable">convert</span><span class="hljs-operator">.</span><span class="hljs-variable">join</span><span class="hljs-operator">.</span><span class="hljs-variable">noconditionaltask</span><span class="hljs-operator">.</span><span class="hljs-variable">size</span><span class="hljs-operator">=</span><span class="hljs-number">10000000</span><span class="hljs-operator">;</span><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<h3 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h3><ul>
<li><p>concat()函数和concat_ws()区别</p>
<ul>
<li><p>能否拼接INT类型</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">select concat(&#x27;&#x27;,123,&#x27;xxx&#x27;); --可以拼接<br>select concat_ws(&#x27;&#x27;,123,&#x27;xxx&#x27;);-- 不可以拼接。must be &quot;string or array&lt;string&gt;&quot;<br><br></code></pre></td></tr></table></figure>
</li>
<li><p>拼接null值得结果</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs hive">select concat(&#x27;&#x27;,null,&#x27;xxx&#x27;);--结果为null<br>select concat_ws(&#x27;&#x27;,null,&#x27;xxx&#x27;); -- 会忽略null值<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/09/01/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>java8新特性</title>
    <url>/2021/04/23/java8-new-features/</url>
    <content><![CDATA[<h1 id="Java8新特性"><a href="#Java8新特性" class="headerlink" title="Java8新特性"></a>Java8新特性</h1><h2 id="Lambda表达式与函数式接口"><a href="#Lambda表达式与函数式接口" class="headerlink" title="Lambda表达式与函数式接口"></a>Lambda表达式与函数式接口</h2><h3 id="函数式接口"><a href="#函数式接口" class="headerlink" title="函数式接口"></a>函数式接口</h3><blockquote>
<ol>
<li>Consumer<T> 消费器, 作用是消费一个T类型的对象, 并没有返回.</li>
</ol>
<ul>
<li>void accept(T t) : 有输入无输出</li>
</ul>
</blockquote>
<hr>
<blockquote>
<ol start="2">
<li>Supplier<T> 供给器, 作用是供给一个T类型的对象, 不需要参数.</li>
</ol>
<ul>
<li>T get() : 无输入有输出</li>
</ul>
</blockquote>
<hr>
<blockquote>
<ol start="3">
<li>Function&lt;T, R&gt; 转换器, 作用是输入一个T类型对象, 经过处理, 返回的是R类型对象.</li>
</ol>
<ul>
<li>R apply(T t) : 有输入有输出</li>
</ul>
</blockquote>
<hr>
<blockquote>
<ol start="4">
<li>Predicate<T> 判定器, 作用是输入一个T类型对象, 经过某种判断, 返回true或false</li>
</ol>
<ul>
<li>boolean test(T t) : 有输入有固定输出布尔</li>
</ul>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">package</span> java8;<br><br><span class="hljs-keyword">import</span> java.util.function.Consumer;<br><span class="hljs-keyword">import</span> java.util.function.Function;<br><span class="hljs-keyword">import</span> java.util.function.Predicate;<br><span class="hljs-keyword">import</span> java.util.function.Supplier;<br><br><span class="hljs-keyword">import</span> org.junit.Test;<br><br><span class="hljs-keyword">import</span> javabean.Student;<br><br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 函数式接口</span><br><span class="hljs-comment"> * 		只有一个抽象方法的接口, 可以用<span class="hljs-doctag">@FunctionalInterface</span>修饰</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * Consumer&lt;T&gt; 消费器, 作用是消费一个T类型的对象, 并没有返回.</span><br><span class="hljs-comment"> * 		void accept(T t) : 有输入无输出</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * Supplier&lt;T&gt; 供给器, 作用是供给一个T类型的对象, 不需要参数.</span><br><span class="hljs-comment"> * 		T get() : 无输入有输出</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * Function&lt;T, R&gt; 转换器, 作用是输入一个T类型对象, 经过处理, 返回的是R类型对象.</span><br><span class="hljs-comment"> * 		R apply(T t) : 有输入有输出</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * Predicate&lt;T&gt; 判定器, 作用是输入一个T类型对象, 经过某种判断, 返回true或false</span><br><span class="hljs-comment"> * 		boolean test(T t) : 有输入有固定输出布尔</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * 方法引用 : 接口中的抽象方法的模式(输入和输出) 和 Lambda体中的方法调用是一致时, 就可以简化写法.</span><br><span class="hljs-comment"> * 类或对象 :: 方法名</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> */</span><br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">FunctionTest</span> &#123;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">exer22</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-comment">//Supplier&lt;Student&gt; supplier2 = () -&gt; new Student();</span><br>		Supplier&lt;Student&gt; supplier2 = Student::<span class="hljs-keyword">new</span>;<br>		System.out.println(supplier2.get());<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test42</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-comment">//Function&lt;Integer, String&gt; function2 = t -&gt; String.valueOf(t);</span><br>		Function&lt;Integer, String&gt; function2 = String::valueOf;<br>		System.out.println(function2.apply(<span class="hljs-number">1112</span>));<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test32</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-comment">//Supplier&lt;Double&gt; supplier2 = () -&gt; Math.random();</span><br>		Supplier&lt;Double&gt; supplier2 = Math::random;<br>		System.out.println(supplier2.get());<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test12</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-comment">//Consumer&lt;String&gt; consumer2 = t -&gt; System.out.println(t);</span><br>		Consumer&lt;String&gt; consumer2 = System.out::println;<br>		consumer2.accept(<span class="hljs-string">&quot;lkjxlkcjccc&quot;</span>);<br>	&#125;<br>	<br>	<span class="hljs-comment">// 写一个判定器, 判断一个学生是否及格</span><br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">exer4</span><span class="hljs-params">()</span> &#123;<br>		Predicate&lt;Integer&gt; predicate1 = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Predicate</span>&lt;Integer&gt;() &#123;<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> <span class="hljs-type">boolean</span> <span class="hljs-title function_">test</span><span class="hljs-params">(Integer t)</span> &#123;<br>				<span class="hljs-keyword">return</span> t&gt;= <span class="hljs-number">60</span>;<br>			&#125;<br>		&#125;;<br>		<span class="hljs-type">boolean</span> <span class="hljs-variable">test</span> <span class="hljs-operator">=</span> predicate1.test(<span class="hljs-number">100</span>);<br>		System.out.println(test);<br>		<br>		Predicate&lt;Integer&gt; predicate2 = t -&gt; t &gt;= <span class="hljs-number">60</span>;<br>		System.out.println(predicate2.test(<span class="hljs-number">100</span>));<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test5</span><span class="hljs-params">()</span> &#123;<br>		Predicate&lt;Integer&gt; predicate1 = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Predicate</span>&lt;Integer&gt;() &#123;<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> <span class="hljs-type">boolean</span> <span class="hljs-title function_">test</span><span class="hljs-params">(Integer t)</span> &#123;<br>				<span class="hljs-keyword">return</span> t % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>;<br>			&#125;<br>		&#125;;<br>		<span class="hljs-type">boolean</span> <span class="hljs-variable">test</span> <span class="hljs-operator">=</span> predicate1.test(<span class="hljs-number">83</span>);<br>		System.out.println(test);<br>		<br>		Predicate&lt;Integer&gt; predicate2 = t -&gt; t % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>;<br>		System.out.println(predicate2.test(<span class="hljs-number">20</span>));<br>	&#125;<br>	<br>	<span class="hljs-comment">// 写一个转换器, 把学生对象转换成字符串, 内容是姓名+分数</span><br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">exer3</span><span class="hljs-params">()</span> &#123;<br>		Function&lt;Student, String&gt; function1 = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Function</span>&lt;Student, String&gt;() &#123;<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> String <span class="hljs-title function_">apply</span><span class="hljs-params">(Student t)</span> &#123;<br>				<span class="hljs-keyword">return</span> t.getName() + <span class="hljs-string">&quot;:&quot;</span> + t.getScore();<br>			&#125;<br>		&#125;;<br>		<span class="hljs-type">String</span> <span class="hljs-variable">apply</span> <span class="hljs-operator">=</span> function1.apply(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Student</span>(<span class="hljs-number">2</span>, <span class="hljs-string">&quot;小刚&quot;</span>, <span class="hljs-number">5</span>, <span class="hljs-number">80</span>));<br>		System.out.println(apply);<br>		<br>		Function&lt;Student, String&gt; function2 = t -&gt; t.getName() + <span class="hljs-string">&quot;:&quot;</span> + t.getScore();<br>		System.out.println(function2.apply(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Student</span>(<span class="hljs-number">1</span>, <span class="hljs-string">&quot;小花&quot;</span>, <span class="hljs-number">2</span>, <span class="hljs-number">100</span>)));<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test4</span><span class="hljs-params">()</span> &#123;<br>		Function&lt;Integer, String&gt; function1 = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Function</span>&lt;Integer, String&gt;() &#123;<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> String <span class="hljs-title function_">apply</span><span class="hljs-params">(Integer t)</span> &#123;<br>				<span class="hljs-keyword">return</span> String.valueOf(t);<br>			&#125;<br>		&#125;;<br>		<br>		<span class="hljs-type">String</span> <span class="hljs-variable">apply</span> <span class="hljs-operator">=</span> function1.apply(<span class="hljs-number">9238</span>);<br>		System.out.println(apply);<br>		<br>		Function&lt;Integer, String&gt; function2 = t -&gt; String.valueOf(t);<br>		System.out.println(function2.apply(<span class="hljs-number">1112</span>));<br>	&#125;<br>	<br>	<span class="hljs-comment">// 写一个供给器, 每调用一次供给一个学生对象</span><br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">exer2</span><span class="hljs-params">()</span> &#123;<br>		Supplier&lt;Student&gt; supplier1 = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Supplier</span>&lt;Student&gt;() &#123;<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> Student <span class="hljs-title function_">get</span><span class="hljs-params">()</span> &#123;<br>				<span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Student</span>();<br>			&#125;<br>		&#125;;<br>		<span class="hljs-type">Student</span> <span class="hljs-variable">student</span> <span class="hljs-operator">=</span> supplier1.get();<br>		System.out.println(student);<br>		<br>		Supplier&lt;Student&gt; supplier2 = () -&gt; <span class="hljs-keyword">new</span> <span class="hljs-title class_">Student</span>();<br>		System.out.println(supplier2.get());<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test3</span><span class="hljs-params">()</span> &#123;<br>		Supplier&lt;Double&gt; supplier = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Supplier</span>&lt;Double&gt;() &#123;<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> Double <span class="hljs-title function_">get</span><span class="hljs-params">()</span> &#123;<br>				<span class="hljs-keyword">return</span> Math.random();<br>			&#125;<br>		&#125;;<br>		System.out.println(supplier.get());<br>		<br>		Supplier&lt;Double&gt; supplier2 = () -&gt; Math.random();<br>		System.out.println(supplier2.get());<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test2</span><span class="hljs-params">()</span> &#123;<br>		Supplier&lt;Integer&gt; supplier1 = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Supplier</span>&lt;Integer&gt;() &#123;<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> Integer <span class="hljs-title function_">get</span><span class="hljs-params">()</span> &#123;<br>				<span class="hljs-keyword">return</span> <span class="hljs-number">100</span>;<br>			&#125;<br>		&#125;;<br>		<span class="hljs-type">Integer</span> <span class="hljs-variable">integer</span> <span class="hljs-operator">=</span> supplier1.get();<br>		System.out.println(integer);<br>		<br>		Supplier&lt;Integer&gt; supplier2 = () -&gt; <span class="hljs-number">100</span>;<br>		<span class="hljs-type">Integer</span> <span class="hljs-variable">integer2</span> <span class="hljs-operator">=</span> supplier2.get();<br>		System.out.println(integer2);<br>	&#125;<br>	<br>	<span class="hljs-comment">// 写一个消费器, 消费一个Student对象.</span><br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">exer1</span><span class="hljs-params">()</span> &#123;<br>		Consumer&lt;Student&gt; consumer1 = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Consumer</span>&lt;Student&gt;() &#123;<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">accept</span><span class="hljs-params">(Student t)</span> &#123;<br>				System.out.println(t);<br>			&#125;<br>		&#125;;<br>		consumer1.accept(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Student</span>());<br>		<br>		Consumer&lt;Student&gt; consumer2 = t -&gt; System.out.println(t);<br>		consumer2.accept(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Student</span>(<span class="hljs-number">1</span>, <span class="hljs-string">&quot;小花&quot;</span>, <span class="hljs-number">2</span>, <span class="hljs-number">50</span>));<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test1</span><span class="hljs-params">()</span> &#123;<br>		Consumer&lt;String&gt; consumer1 = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Consumer</span>&lt;String&gt;() &#123;<br>			<span class="hljs-meta">@Override</span><br>			<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">accept</span><span class="hljs-params">(String t)</span> &#123;<br>				System.out.println(t);<br>			&#125;<br>		&#125;;<br>		consumer1.accept(<span class="hljs-string">&quot;alsdkjfalksdjf&quot;</span>);<br>		<br>		Consumer&lt;String&gt; consumer2 = t -&gt; System.out.println(t);<br>		consumer2.accept(<span class="hljs-string">&quot;lkjxlkcjccc&quot;</span>);<br>	&#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h2 id="Stream流"><a href="#Stream流" class="headerlink" title="Stream流"></a>Stream流</h2><figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">package</span> java8;<br><span class="hljs-keyword">import</span> java.util.*;<br><span class="hljs-keyword">import</span> java.util.stream.Collectors;<br><span class="hljs-keyword">import</span> java.util.stream.Stream;<br><br><span class="hljs-keyword">import</span> org.junit.Test;<br><br><span class="hljs-keyword">import</span> javabean.Student;<br><span class="hljs-keyword">import</span> javabean.StudentTest;<br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Stream : </span><br><span class="hljs-comment"> * 	1) 不保存数据, 只负责处理数据</span><br><span class="hljs-comment"> * 	2) 处理数据不会造成原始数据的变化 , 每次处理都会产生新的流</span><br><span class="hljs-comment"> * 	3) 所有操作都是延迟执行的, 只有终止操作执行时才执行中间操作</span><br><span class="hljs-comment"> * 	4) 每个流只能&quot;消费&quot;一次, 消费过后就作废.</span><br><span class="hljs-comment"> * 	5) 单向, 一次性使用, 可以支持高并发...</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * 典型的操作 :</span><br><span class="hljs-comment"> * 	1) 创建流(获取流) </span><br><span class="hljs-comment"> * 		1) 从集合获取流, 集合.stream();</span><br><span class="hljs-comment"> * 		2) 从数组获取流, Arrays.stream(Xxx[] arr);</span><br><span class="hljs-comment"> * 		3) 基于散数据, Stream.of(T... objs)</span><br><span class="hljs-comment"> * 		4) 使用供给器, 无限流</span><br><span class="hljs-comment"> * 	2) 中间操作, 多个中间操作就形成流水线, 是延迟执行的, 中间操作可以省略</span><br><span class="hljs-comment"> * 		***filter(Predicate p) : 让流中的每个对象都经过判定器, 如果结果为true留下, 如果是false丢弃. 产生新流</span><br><span class="hljs-comment"> * 		distinct(); 把流中的数据去重并产生新流, 依据对象的hashCode和equals</span><br><span class="hljs-comment"> * 		limit(long maxSize) 限制流中的最大数据量</span><br><span class="hljs-comment"> * 		skip(long n) 跳过前n个元素</span><br><span class="hljs-comment"> * 		***map(Function f) 让流中的每个对象都转换为新对象, 所以它的结果的流全变了.</span><br><span class="hljs-comment"> * 		sorted() 把流中的对象排序 , 自然排序</span><br><span class="hljs-comment"> * 		*sorted(Comparator c) 定制排序</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * 	3) 终止操作, 一旦中止, 所有的中间操作就开始执行, 终止操作是必须的.</span><br><span class="hljs-comment"> * 		***forEach(Consumer c) : 让流中的每个对象都经过消费器消费一下.</span><br><span class="hljs-comment"> * 		findFirst() 返回流中的第一个对象</span><br><span class="hljs-comment"> * 		**count() 计数</span><br><span class="hljs-comment"> * 		**collect(采集器) 可以把结果集采集到一个新的容器中.</span><br><span class="hljs-comment"> * 		***reduce(BinaryOperator op) 把流中的对象两两处理最后产生一个结果</span><br><span class="hljs-comment"> *	</span><br><span class="hljs-comment"> *	Optional是一容器, 里面放一个引用, 如果引用为空, 获取时直接抛异常.</span><br><span class="hljs-comment"> *	防止空指针.</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">StreamTest</span> &#123;<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">exer6</span><span class="hljs-params">()</span> &#123;<br>		List&lt;Student&gt; collect = StudentTest.getList()<br>					.stream()<br>					.distinct()<br>					.filter(t -&gt; t.getGrade() == <span class="hljs-number">3</span>)<br>					.filter(t -&gt; t.getScore() &lt; <span class="hljs-number">60</span>).sorted((o1, o2) -&gt; -(<span class="hljs-type">int</span>)(o1.getScore() - o2.getScore()))<br>					.collect(Collectors.toList());<br>		<span class="hljs-keyword">for</span> (Student student : collect) &#123;<br>			System.out.println(student);<br>		&#125;<br>	&#125;<br>	<br>	<span class="hljs-comment">// 找出全校最高分</span><br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">exer5</span><span class="hljs-params">()</span> &#123;<br>		Optional&lt;Double&gt; reduce = StudentTest.getList().stream().distinct().map(t -&gt; t.getScore()).reduce((d1, d2) -&gt; d1 &gt; d2 ? d1 : d2);<br>		<span class="hljs-type">Double</span> <span class="hljs-variable">orElse</span> <span class="hljs-operator">=</span> reduce.orElse((<span class="hljs-type">double</span>) <span class="hljs-number">999</span>);<br>		System.out.println(orElse);<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">exer4</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-type">long</span> <span class="hljs-variable">count</span> <span class="hljs-operator">=</span> StudentTest.getList().stream().distinct().count();<br>		System.out.println(count);<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">exer3</span><span class="hljs-params">()</span> &#123;<br>		Optional&lt;Student&gt; findFirst = StudentTest.getList()<br>												.stream()<br>												.distinct()<br>												.filter(t -&gt; t.getGrade() == <span class="hljs-number">4</span>)<br>												.filter(t -&gt; t.getScore() &lt; <span class="hljs-number">60</span>).sorted((o1, o2) -&gt; -(<span class="hljs-type">int</span>)(o1.getScore() - o2.getScore()))<br>												.limit(<span class="hljs-number">2</span>).findFirst();<br>		<span class="hljs-comment">//Student student = findFirst.get();</span><br>		<span class="hljs-type">Student</span> <span class="hljs-variable">student</span> <span class="hljs-operator">=</span> findFirst.orElse(<span class="hljs-keyword">new</span> <span class="hljs-title class_">Student</span>()); <span class="hljs-comment">// 最大化减少空指针</span><br>		System.out.println(student);<br>	&#125;<br>		<br>	<span class="hljs-comment">// 3年级没有及格的同学倒序, 取出前2个.</span><br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">exer2</span><span class="hljs-params">()</span> &#123;<br>		StudentTest.getList()<br>					.stream()<br>					.distinct()<br>					.filter(t -&gt; t.getGrade() == <span class="hljs-number">3</span>)<br>					.filter(t -&gt; t.getScore() &lt; <span class="hljs-number">60</span>).sorted((o1, o2) -&gt; -(<span class="hljs-type">int</span>)(o1.getScore() - o2.getScore()))<br>					.limit(<span class="hljs-number">2</span>).forEach(System.out::println);<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test9</span><span class="hljs-params">()</span> &#123;<br>		StudentTest.getList().stream().distinct().sorted((t1, t2) -&gt; (<span class="hljs-type">int</span>)(t1.getScore() - t2.getScore())).forEach(System.out::println);<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test8</span><span class="hljs-params">()</span> &#123;<br>		StudentTest.getList().stream().distinct().map(t -&gt; t.getScore()).forEach(System.out::println);<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">Test7</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-comment">// 第6个到第10个</span><br>		StudentTest.getList().stream().distinct().skip(<span class="hljs-number">10</span>).limit(<span class="hljs-number">5</span>).forEach(System.out::println);<br>	&#125;<br>	<br>	<span class="hljs-comment">// 找出5年级姓张的同学</span><br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">exer1</span><span class="hljs-params">()</span> &#123;<br>		List&lt;Student&gt; list = StudentTest.getList();<br>		list.stream().filter(t -&gt; t.getGrade() == <span class="hljs-number">5</span>).filter(t -&gt; t.getName().startsWith(<span class="hljs-string">&quot;张&quot;</span>)).forEach(System.out::println);<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test62</span><span class="hljs-params">()</span> &#123;<br>		List&lt;Student&gt; list = StudentTest.getList();<br>		list.stream().filter(t -&gt; t.getGrade() == <span class="hljs-number">3</span>).filter(t -&gt; t.getScore() &gt;= <span class="hljs-number">60</span>).forEach(System.out::println);<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test6</span><span class="hljs-params">()</span> &#123;<br>		List&lt;Student&gt; list = StudentTest.getList();<br>		Stream&lt;Student&gt; stream = list.stream();<br>		Stream&lt;Student&gt; stream2 = stream.filter(t -&gt; t.getGrade() == <span class="hljs-number">3</span>);<br>		Stream&lt;Student&gt; stream3 = stream2.filter(t -&gt; t.getScore() &gt;= <span class="hljs-number">60</span>);<br>		stream3.forEach(System.out::println);<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test5</span><span class="hljs-params">()</span> &#123;<br>		Stream.generate(Math::random).limit(<span class="hljs-number">10</span>).forEach(System.out::println);<span class="hljs-comment">// 无限流</span><br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test4</span><span class="hljs-params">()</span> &#123;<br>		Stream&lt;Integer&gt; generate = Stream.generate(() -&gt; <span class="hljs-number">200</span>); <span class="hljs-comment">// 无限流</span><br>		generate.forEach(System.out::println);<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test3</span><span class="hljs-params">()</span> &#123;<br>		Stream&lt;Number&gt; of = Stream.of(<span class="hljs-number">3.22</span>, <span class="hljs-number">9.33</span>, <span class="hljs-number">4.88</span>, <span class="hljs-number">4.2</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>);<br>		of.forEach(System.out::println);<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test2</span><span class="hljs-params">()</span> &#123;<br>		String[] arr = &#123;<span class="hljs-string">&quot;kjsf&quot;</span>, <span class="hljs-string">&quot;qqa&quot;</span>, <span class="hljs-string">&quot;cv&quot;</span>, <span class="hljs-string">&quot;XXX&quot;</span>&#125;;<br>		Stream&lt;String&gt; stream = Arrays.stream(arr);<br>		stream.forEach(System.out::println);<br>	&#125;<br>	<br>	<span class="hljs-meta">@Test</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test1</span><span class="hljs-params">()</span> &#123;<br>		List&lt;Integer&gt; list = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;Integer&gt;();<br>		<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">10</span>; i++) &#123;<br>			list.add((<span class="hljs-type">int</span>)(Math.random() * <span class="hljs-number">20</span>));<br>		&#125;<br>		System.out.println(list);<br>		<br>		Stream&lt;Integer&gt; stream = list.stream();<br>		stream.forEach(System.out::println);<br>		<br>	&#125;<br>&#125;<br><br></code></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka</title>
    <url>/2021/06/24/kafka/</url>
    <content><![CDATA[<h1 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h1><h2 id="Kafka概述"><a href="#Kafka概述" class="headerlink" title="Kafka概述"></a>Kafka概述</h2><ul>
<li><p>kafaka是一个分布式的基于发布&#x2F;订阅模式的消息队列，主要应用于大数据实时处理领域。</p>
</li>
<li><p>消息队列一般分为两种模式：</p>
<ol>
<li>点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）</li>
<li>发布&#x2F;订阅模式（一对多，消费者消费数据之后不会清除消息）</li>
</ol>
</li>
<li><p>基础架构</p>
<p><a href="https://imgtu.com/i/RgX9C4"><img src="https://z3.ax1x.com/2021/07/03/RgX9C4.png" alt="RgX9C4.png"></a></p>
<ul>
<li>producer:消息生产者，向Kafka broker发消息的客户端</li>
<li>consumer：消息消费者，向Kafka broker取消息的客户端</li>
<li>consumer group：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</li>
<li>broker：一台kafka服务器就是一个broker。一个Kafak集群由多个broker组成。一个broker可以容纳多个topic。</li>
<li>topic：可以理解为一个队列，生产者和消费者面向的都是一个topic</li>
<li>partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；</li>
<li>replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower</li>
<li>leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。</li>
<li>follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。</li>
</ul>
</li>
</ul>
<h2 id="架构深入"><a href="#架构深入" class="headerlink" title="架构深入"></a>架构深入</h2><h3 id="文件存储机制"><a href="#文件存储机制" class="headerlink" title="文件存储机制"></a>文件存储机制</h3><ul>
<li><p>Kafka中的消息使以topic进行分类的，生产者生产消息，消费者消费消息都是面向topic的。</p>
</li>
<li><p>topic是逻辑的一个概念，partition是物理上的一个概念，每一个partition对应一个log文件，log文件中存放生产者生产的数据。每条数据也有自己的offset，消费者组中的每一个消费者，都会实时记录自己消费到了哪个offset，以便于出错时恢复，从上次的位置继续消费。</p>
</li>
<li><p><a href="https://imgtu.com/i/RgXeUO"><img src="https://z3.ax1x.com/2021/07/03/RgXeUO.png" alt="RgXeUO.png"></a></p>
</li>
<li><p>当log文件过大时就会分成多个segment,主要是为了加快索引速度，而且一个log文件分成多个segment都是在磁盘上连续存储的，Kafka这样的设计可以大幅度提高数据读写速度。</p>
</li>
<li><p>一个segment对应三个文件，一个log文件，两个index文件，一个.index文件和一个.timeindex文件。</p>
</li>
<li><p>“.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元数据指向对应数据文件中message的物理偏移地址。</p>
</li>
<li><p>index和log文件以当前segment的第一条消息的offset命名</p>
<p><a href="https://imgtu.com/i/RgXQxA"><img src="https://z3.ax1x.com/2021/07/03/RgXQxA.png" alt="RgXQxA.png"></a></p>
</li>
</ul>
<h3 id="Kafka生产者"><a href="#Kafka生产者" class="headerlink" title="Kafka生产者"></a>Kafka生产者</h3><ul>
<li>Kafka的数据也是类似于键值对形式的数据。例如&lt;String,String&gt;</li>
</ul>
<h4 id="分区策略"><a href="#分区策略" class="headerlink" title="分区策略"></a>分区策略</h4><ul>
<li><p>分区的原因：1.方便在集群中扩展。2.可以提高并发，以partition为单位进行读写。</p>
</li>
<li><p>分区写入策略：</p>
<ul>
<li><p>所谓轮询策略，即按顺序轮流将每条数据分配到每个分区中。</p>
<ul>
<li><p>举个例子，假设主题test有三个分区，分别是分区A，分区B和分区C。那么主题对接收到的第一条消息写入A分区，第二条消息写入B分区，第三条消息写入C分区，第四条消息则又写入A分区，依此类推。</p>
<p>轮询策略是默认的策略，故而也是使用最频繁的策略，它能最大限度保证所有消息都平均分配到每一个分区。除非有特殊的业务需求，否则使用这种方式即可。</p>
</li>
</ul>
</li>
<li><p>随机策略，也就是每次都随机地将消息分配到每个分区。其实大概就是先得出分区的数量，然后每次获取一个随机数，用该随机数确定消息发送到哪个分区。</p>
<ul>
<li>在比较早的版本，默认的分区策略就是随机策略，但其实使用随机策略也是为了更好得将消息均衡写入每个分区。但后来发现对这一需求而言，轮询策略的表现更优，所以社区后来的默认策略就是轮询策略了。</li>
</ul>
</li>
<li><p>按键保存策略，就是当生产者发送数据的时候，可以指定一个key，计算这个key的hashCode值，按照hashCode的值对不同消息进行存储。</p>
<ul>
<li>只要让生产者发送的时候指定key就行。刚刚不是说默认的是轮询策略吗？其实啊，kafka默认是实现了两个策略，没指定key的时候就是轮询策略，有的话那激素按键保存策略了。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="数据可靠性的保证"><a href="#数据可靠性的保证" class="headerlink" title="数据可靠性的保证"></a>数据可靠性的保证</h4><ul>
<li><p>producer向server发信息时是异步通信，为确保数据的可靠性，需要向producer发送ACK（acknowledgement确认收到）。如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。这时会出现三种返回ack的时机：</p>
<p>1.收到后还没写立刻发送。最不安全</p>
<p>2.leader写完数据后发送ack。</p>
<p>3.ISR中所有副本都写完数据后发送ack。kafka默认的选择。优点：容忍n台节点故障，只需要n+1个副本。</p>
</li>
<li><p>根据这三种情况，将ack的值分别分为0，1，all(-1)。</p>
</li>
<li><p>在第三种情况中，会出现一种意外情况：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？</p>
</li>
<li><p>Kafka引入一个Leader维护了一个动态的in-sync replica set (ISR)，解决上述问题。和leader保持同步的follower在ISR中，如果Follower长时间未向leader同步数据，该follower会被踢出ISR，实践阈值由<em><strong>replica.lag.time.max.ms</strong></em>参数设定。leader故障则会重新选举leader。</p>
<p><a href="https://imgtu.com/i/RgXWRJ"><img src="https://z3.ax1x.com/2021/07/03/RgXWRJ.png" alt="RgXWRJ.png"></a></p>
<ul>
<li>当follower故障时会被临时踢出ISR，待待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该<em><strong>follower的LEO大于等于该Partition的HW</strong></em>，即follower追上leader之后，就可以重新加入ISR了。</li>
<li>leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。</li>
<li>这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复</li>
</ul>
</li>
<li><p>exactly once语义：At Least Once + 幂等性 &#x3D; Exactly Once。</p>
</li>
<li><p>幂等性就是指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条。</p>
</li>
<li><p>将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据，即At Least Once语义。</p>
</li>
<li><p>要启用幂等性，只需要将Producer的参数中enable.idompotence设置为true即可。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带Sequence Number。而Broker端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。</p>
</li>
<li><p>局限性：但是PID重启就会变化，同时不同的Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once。</p>
</li>
</ul>
<h3 id="Kafka消费者"><a href="#Kafka消费者" class="headerlink" title="Kafka消费者"></a>Kafka消费者</h3><h4 id="消费方式"><a href="#消费方式" class="headerlink" title="消费方式"></a>消费方式</h4><ul>
<li>消费方式：consumer采用pull(拉)模式从broker中读取数据。</li>
<li>pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。</li>
</ul>
<h4 id="分区分配策略"><a href="#分区分配策略" class="headerlink" title="分区分配策略"></a>分区分配策略</h4><ul>
<li><p>理想状况是一个consumergroup中每一个consumer负责拉取一个分区的数据。</p>
</li>
<li><p>roundrobin：当分区数大于consumer数量时，轮询分配，类似于斗地主一张一张的发牌。</p>
</li>
<li><p>range:类似于斗地主一次性发几张牌</p>
</li>
</ul>
<h4 id="offset维护"><a href="#offset维护" class="headerlink" title="offset维护"></a>offset维护</h4><ul>
<li>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</li>
<li>将offset放在Kafka内置的一个topic中，__consumer_offsets。由50个分区。</li>
</ul>
<h4 id="Zookeeper在Kafka中作用"><a href="#Zookeeper在Kafka中作用" class="headerlink" title="Zookeeper在Kafka中作用"></a>Zookeeper在Kafka中作用</h4><ul>
<li><p>Kafka集群中有一个broker会被选举为Controller，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。Controller的选举就是比谁快。</p>
</li>
<li><p>Controller的管理工作都是依赖于Zookeeper的。</p>
<p><a href="https://imgtu.com/i/RgXXzd"><img src="https://z3.ax1x.com/2021/07/03/RgXXzd.png" alt="RgXXzd.png"></a></p>
</li>
<li><p>leader挂掉之后，Controller会选举新的leader，更新永久节点state中的信息。state存放某个topic,某个分区的以些必要信息。</p>
</li>
</ul>
<h4 id="Kafka事务"><a href="#Kafka事务" class="headerlink" title="Kafka事务"></a>Kafka事务</h4><ul>
<li>为了实现跨分区跨会话的事务，需要引入一个全局唯一的Transaction ID，并将Producer获得的PID和Transaction ID绑定。这样当Producer重启后就可以通过正在进行的Transaction ID获得原来的PID。</li>
</ul>
<h3 id="KafkaAPI"><a href="#KafkaAPI" class="headerlink" title="KafkaAPI"></a>KafkaAPI</h3><ul>
<li><p>produceAPI</p>
</li>
<li><p>Kafka的Producer发送消息采用的是<em><strong>异步发送</strong></em>的方式。在消息发送的过程中，涉及到了<strong>两个线程——main线程和Sender线程</strong>，以及<em><strong>一个线程共享变量——RecordAccumulator</strong></em>。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。</p>
<p><a href="https://imgtu.com/i/RGBi1f"><img src="https://z3.ax1x.com/2021/06/26/RGBi1f.png" alt="RGBi1f.png"></a></p>
<ul>
<li><p>相关参数：</p>
<p><em><strong>batch.size</strong></em><em><strong>：</strong></em>只有数据积累到batch.size之后，sender才会发送数据。</p>
<p><em><strong>linger.ms</strong></em><em><strong>：</strong></em>如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。</p>
</li>
</ul>
</li>
<li><p>ConsumerAPI</p>
</li>
<li><p>Consumer消费数据时的可靠性是很容易保证的，因为数据在Kafka中是持久化的，故不用担心数据丢失问题。</p>
<p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</p>
<p>所以offset的维护是Consumer消费数据是必须考虑的问题。</p>
</li>
</ul>
<h3 id="Exactly-Once语义"><a href="#Exactly-Once语义" class="headerlink" title="Exactly Once语义"></a>Exactly Once语义</h3><ul>
<li><p>At Least Once 是Kafka默认提供的语义，它保证每条消息都能至少接收并处理一次，缺点是可能有重复数据。</p>
</li>
<li><p>At Most Once 最多一次就是保证一条消息只发送一次，这个其实最简单，异步发送一次然后不管就可以，缺点是容易丢数据，所以一般不采用。</p>
</li>
<li><p>要实现Exactly Once语义首先要弄清楚哪里会丢数据，哪里会重复数据</p>
<ul>
<li>首先是生产者丢失信息：<ul>
<li>Producer客户端有一个ack的配置，异步通信，通过ack来确定数据接收情况。要达到最严格的无消息丢失配置，应该是要将ack的参数设置为-1（all）。<strong>同时还需要使用带有回调的producer api，来发送数据</strong></li>
<li>Kafka 有一个配置参数min.insync.replicas，默认是1（也就是只有leader，实际生产应该调高），该属性规定了最小的ISR数。这意味着当acks为-1（即all）的时候，这个参数规定了必须写入的ISR集中的副本数，如果没达到，那么producer会产生异常</li>
</ul>
</li>
<li>Broker丢失数据<ul>
<li>首先是replication.factor配置参数，这个配置决定了副本的数量，默认是1。注意这个参数不能超过broker的数量</li>
<li>unclean.leader.election.enable参数设置为false，这个参数表示，当ISR集合中没有副本可以成为leader时，要不要从ISR之外的比较慢的副本选出leader，这样会导致丢失数据，虽然可以提高一些可用性，但是我们这里考虑的时精确一次消费，所以我们需要将这个参数置为false</li>
</ul>
</li>
<li>消费者丢失数据<ul>
<li>消费者丢失的情况，其实跟消费者offset处理不当有关。消费者消费的offset提交有一个参数，enable.auto.commit，默认是true，决定是否要让消费者自动提交位移。如果开启，那么consumer每次都是先提交位移，再进行消费。这样也会导致丢失数据（当消费者提交offset后，数据未消费完就挂了，但是offset的记录已经不对了，会导致重启后数据也找不到就丢失了）</li>
<li>可以将enable.auto.commit设置为false，改为手动提交offset，消费完之后再提交offset信息。但是这样又有可能导致重复消费。毕竟exactly once处理一直是一个问题呀（&#x2F;摊手）。遗憾的是kafka目前没有保证consumer幂等消费的措施，如果确实需要保证consumer的幂等，可以对每条消息维持一个全局的id，每次消费进行去重，当然耗费这么多的资源来实现exactly once的消费到底值不值，那就得看具体业务了。</li>
</ul>
</li>
</ul>
</li>
<li><p>这是无消息丢失的几个主要配置</p>
<ul>
<li>producer的acks设置位-1，同时min.insync.replicas设置大于1。并且使用带有回调的producer api发生消息。</li>
<li>默认副本数replication.factor设置为大于1，或者创建topic的时候指定大于1的副本数。</li>
<li>unclean.leader.election.enable 设置为false，防止定期副本leader重选举</li>
<li>消费者端，自动提交位移enable.auto.commit设置为false。在消费完后手动提交位移。</li>
</ul>
</li>
<li><p>实现Exactly需要的几个配置</p>
<ul>
<li><p>首先是开启幂等性</p>
<ul>
<li>在kafka中，幂等性意味着一个消息无论重复多少次，都会被当作一个消息来持久化处理。</li>
<li>开启幂等性后，ack默认已经设置为-1了</li>
<li>创建producer客户端的时候，添加这一行配置 ：props.put(“enable.idempotence”, ture)</li>
<li>底层实现也很简单，就是对每条消息生成一个id值，broker会根据这个id值进行去重，从而实现幂等</li>
<li>幂等性的缺陷：幂等性只能保证单个Producer对于同一个分区的Exactly Once语义<ul>
<li>幂等性的producer只能做到单分区上的幂等性，即单分区消息不重复，多分区无法保证幂等性</li>
<li>只能保持单会话的幂等性，无法实现跨会话的幂等性，也就是说如果producer挂掉再重启，无法保证两个会话间的幂等（新会话可能会重发）。因为broker端无法获取之前的状态信息，所以无法实现跨会话的幂等。</li>
</ul>
</li>
</ul>
</li>
<li><p>在幂等性缺陷无法解决的时候就要考虑使用事务了</p>
<ul>
<li><p>事务可以支持多分区的数据完整性，原子性。并且支持跨会话的exactly once处理语义，也就是说如果producer宕机重启，依旧能保证数据只处理一次。</p>
</li>
<li><p>开启事务也很简单，首先需要开启幂等性，即设置enable.idempotence为true。然后对producer发送代码做一些小小的修改。</p>
</li>
<li><pre><code class="java">//初始化事务
producer.initTransactions();
try &#123;
    //开启一个事务
    producer.beginTransaction();
    producer.send(record1);
    producer.send(record2);
    //提交
    producer.commitTransaction();
&#125; catch (KafkaException e) &#123;
    //出现异常的时候，终止事务
    producer.abortTransaction();
&#125;
</code></pre>
</li>
<li><p>但无论开启幂等还是事务的特性，都会对性能有一定影响，这是必然的。所以kafka默认也并没有开启这两个特性，而是交由开发者根据自身业务特点进行处理。</p>
</li>
<li><p>为了实现这种效果，应用程序必须提供一个稳定的（重启后不变）唯一的ID，也即Transaction ID。Transactin ID与PID可能一一对应。区别在于Transaction ID由用户提供，而PID是内部的实现对用户透明。另外，为了保证新的Producer启动后，旧的具有相同Transaction ID的Producer即失效，每次Producer通过Transaction ID拿到PID的同时，还会获取一个单调递增的epoch。由于旧的Producer的epoch比新Producer的epoch小，Kafka可以很容易识别出该Producer是老的Producer并拒绝其请求。</p>
</li>
<li><p>跨Session的数据幂等发送。当具有相同Transaction ID的新的Producer实例被创建且工作时，旧的且拥有相同Transaction ID的Producer将不再工作。</p>
<p>跨Session的事务恢复。如果某个应用实例宕机，新的实例可以保证任何未完成的旧的事务要么Commit要么死亡，使得新实例从一个正常状态开始工作。</p>
</li>
</ul>
</li>
<li><p>需要注意的是，上述的事务保证是从Producer的角度去考虑的。从Consumer的角度来看，该保证会相对弱一些。尤其是不能保证所有被某事务Commit过的所有消息都被一起消费</p>
</li>
<li><p>原因</p>
<ul>
<li><p>对于压缩的Topic而言，同一事务的某些消息可能被其它版本覆盖</p>
<p>事务包含的消息可能分布在多个Segment中（即使在同一个Partition内），当老的Segment被删除时，该事务的部分数据可能会丢失</p>
<p>Consumer在一个事务内可能通过seek方法访问任意Offset的消息，从而可能丢失部分消息</p>
<p>Consumer可能并不需要消费某一事务内的所有Partition，因此它将永远不会读取组成该事务的所有消息</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Java多线程基础</title>
    <url>/2021/04/20/multithreading/</url>
    <content><![CDATA[<h1 id="Java多线程"><a href="#Java多线程" class="headerlink" title="Java多线程"></a>Java多线程</h1><h2 id="1-程序，进程，线程"><a href="#1-程序，进程，线程" class="headerlink" title="1.程序，进程，线程"></a>1.程序，进程，线程</h2><blockquote>
<ul>
<li>程序：（可以执行的静态代码，是保存在硬盘上的一个文件）是为完成特定任务，用某种语言编写的一组指令的集合。即<em><strong>一段静态的代码</strong></em>，静态对象.</li>
<li>进程：（正在执行中的一个程序，在内存中处于激活状态，有生命周期）是程序的一次执行过程，或是<em><strong>正在运行的一个程序</strong></em>。动态过程：有它自身的产生、存在和消亡的过程</li>
<li>线程：（进程中的子任务）进程可以进一步细化为线程，是一个程序内部的一条执行路径</li>
</ul>
</blockquote>
<h2 id="2-Java中多线程的创建和使用"><a href="#2-Java中多线程的创建和使用" class="headerlink" title="2.Java中多线程的创建和使用"></a>2.Java中多线程的创建和使用</h2><blockquote>
<ul>
<li>实现Runnable接口与继承Thread类</li>
<li><h3 id="Thread类的主要方法"><a href="#Thread类的主要方法" class="headerlink" title="Thread类的主要方法"></a>Thread类的主要方法</h3></li>
</ul>
<blockquote>
<ul>
<li>每个线程都是通过某个特定Thread对象的run()方法来完成操作的，经常把run()方法的主题称为线程体</li>
<li>通过该Thread对象的start()方法来调用这个线程</li>
<li>static Thread currentThread(),返回当前方法正在执行此方法所压入的栈的线程对象</li>
<li>void join()它的作用是调用此方法的另一个线程阻塞，当前线程执行完再执行另一个线程</li>
<li>static void sleep(long millis)作用是让当前线程（正在执行此方法的栈的线程）进入睡眠状态<blockquote>
<ul>
<li>两种方式结束sleep状态：1.时间到了。2.被其他进程打断 interrupt() 方法</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
</blockquote>
<h3 id="创建并启动线程的方式"><a href="#创建并启动线程的方式" class="headerlink" title="创建并启动线程的方式"></a>创建并启动线程的方式</h3><h4 id="实现Runnable的方式"><a href="#实现Runnable的方式" class="headerlink" title="实现Runnable的方式"></a>实现Runnable的方式</h4><blockquote>
<blockquote>
<ol>
<li>写一个具体类，实现Runnable接口，并实现接口中的抽象方法run(),这个run方法就是线程体</li>
<li>创建这个具体类对象，并把这个对象作为实参，创建Thread线程对象</li>
<li>调用Thread线程对象的start方法</li>
</ol>
<ul>
<li>代码示例</li>
</ul>
</blockquote>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">package</span> multi_thread;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Counter</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">Runnable</span> &#123;<br>	<span class="hljs-keyword">private</span> <span class="hljs-type">int</span> <span class="hljs-variable">cnt</span> <span class="hljs-operator">=</span> <span class="hljs-number">200</span>;<br><br>	<span class="hljs-meta">@Override</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">run</span><span class="hljs-params">()</span> &#123;<br>		System.out.println(cnt);<br>		<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">50</span>; i++) &#123;<br>			<span class="hljs-keyword">synchronized</span> (<span class="hljs-string">&quot;&quot;</span>) &#123;<span class="hljs-comment">// ()中是一个锁对象，任意对象都可以做锁，称为互斥锁，作用是只允许一个线程进入执行，其他线程等待</span><br><br>				cnt -= <span class="hljs-number">2</span>;<br>				<span class="hljs-keyword">try</span> &#123;<br>					Thread.sleep(<span class="hljs-number">10</span>);<br>				&#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;<br>					<span class="hljs-comment">// TODO Auto-generated catch block</span><br>					e.printStackTrace();<br>				&#125;<br>				System.out.println(Thread.currentThread().getName() + <span class="hljs-string">&quot;: &quot;</span> + cnt);<br>			&#125;<br>		&#125;<br>	&#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure>
<hr>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">package</span> multi_thread;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">CounterTest</span> &#123;<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>		<span class="hljs-type">Runnable</span> <span class="hljs-variable">counter</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Counter</span>();<br>		<span class="hljs-type">Thread</span> <span class="hljs-variable">thread1</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(counter);<br>		<span class="hljs-type">Thread</span> <span class="hljs-variable">thread2</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(counter);<br>		thread1.start();<br>		thread2.start();<br>	&#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure>
<h4 id="继承Thread的方式"><a href="#继承Thread的方式" class="headerlink" title="继承Thread的方式"></a>继承Thread的方式</h4><blockquote>
<blockquote>
<ol>
<li>写一个类，继承Thread,并重写run方法，此方法就是线程体</li>
<li>创建这个类的对象，相当于创建了线程对象</li>
<li>调用这个线程对象的start方法</li>
</ol>
<ul>
<li>代码示例</li>
</ul>
</blockquote>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs Java"><span class="hljs-keyword">package</span> multi_thread;<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyThread</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Thread</span>&#123;<br>	<span class="hljs-meta">@Override</span><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">run</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;<span class="hljs-number">100</span>;i++) &#123;<br>			System.out.println(currentThread().getName()+<span class="hljs-string">&quot; &quot;</span>+i);<br>		&#125;<br>	&#125;<br>&#125;<br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ThreadTest2</span> &#123;<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>		<span class="hljs-type">Thread</span> <span class="hljs-variable">myThread1</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">MyThread</span>();<br>		myThread1.start();<br>		<span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;<span class="hljs-number">100</span>;i++) &#123;<br>			System.out.println(<span class="hljs-string">&quot;-----main &quot;</span>+i);<br>		&#125;<br>	&#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure>
<h4 id="使用callable接口创建多线程"><a href="#使用callable接口创建多线程" class="headerlink" title="使用callable接口创建多线程"></a>使用callable接口创建多线程</h4><blockquote>
<ul>
<li>落地方法call()</li>
<li>Callable接口作为JDK1.5新增的接口，与使用Runnable相比其功能更强大些。</li>
<li>相比run()方法，可以有返回值</li>
<li>方法可以抛出异常</li>
<li>支持泛型的返回值</li>
<li>需要借助FutureTask类，比如获取返回结果。</li>
<li>Callable接口一般用于配合ExecutorService使用</li>
</ul>
</blockquote>
<h5 id="Feture接口"><a href="#Feture接口" class="headerlink" title="Feture接口"></a>Feture接口</h5><blockquote>
<ul>
<li>可以对具体Runnable、Callable任务的执行结果进行取消、查询是否完成、获取结果等。</li>
<li>FutrueTask是Futrue接口的实现类</li>
<li>FutureTask 同时实现了Runnable, Future接口。它既可以作为Runnable被线程执行，又可以作为Future得到Callable的返回值。</li>
<li>多个线程同时执行一个FutureTask，只要一个线程执行完毕，其他线程不会再执行其call()方法。</li>
<li>get()方法会阻塞当前线程！<br>实例代码:</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">class</span> MyThread implements Callable&lt;<span class="hljs-type">Integer</span>&gt; &#123;<br>	@Override<br>	<span class="hljs-built_in">public</span> <span class="hljs-type">Integer</span> <span class="hljs-keyword">call</span>() throws <span class="hljs-keyword">Exception</span> &#123;<br>		<span class="hljs-keyword">System</span>.<span class="hljs-keyword">out</span>.println(Thread.currentThread().getName()+&quot; Come in call&quot;);<br>		//睡<span class="hljs-number">5</span>秒<br>		TimeUnit.SECONDS.sleep(<span class="hljs-number">5</span>);<br>		//返回<span class="hljs-number">200</span>的状态码<br>		<span class="hljs-keyword">return</span> <span class="hljs-number">200</span>;<br>		<br>	&#125;<br>&#125;<br><br><span class="hljs-built_in">public</span> <span class="hljs-keyword">class</span> CallableTest &#123;<br>	<span class="hljs-built_in">public</span> static <span class="hljs-type">void</span> main(String[] args) throws InterruptedException, ExecutionException &#123;<br>		MyThread myThread = <span class="hljs-built_in">new</span> MyThread();<br>		FutureTask&lt;<span class="hljs-type">Integer</span>&gt; futureTask = <span class="hljs-built_in">new</span> FutureTask&lt;&gt;(myThread);<br>		<span class="hljs-built_in">new</span> Thread(futureTask, &quot;未来任务&quot;).<span class="hljs-keyword">start</span>();<br>        <span class="hljs-keyword">System</span>.<span class="hljs-keyword">out</span>.println(&quot;主线程结束！&quot;);<br>		<span class="hljs-type">Integer</span> <span class="hljs-type">integer</span> = futureTask.<span class="hljs-keyword">get</span>();<br>		<span class="hljs-keyword">System</span>.<span class="hljs-keyword">out</span>.println(<span class="hljs-type">integer</span>);<br>		<br>	&#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h4 id="线程池创建多线程"><a href="#线程池创建多线程" class="headerlink" title="线程池创建多线程"></a>线程池创建多线程</h4><blockquote>
<ul>
<li>线程池：经常创建和销毁、使用量特别大的资源，比如并发情况下的线程，对性能影响很大。因此提前创建好多个线程，放入线程池中，使用时直接获取，使用完放回池中。可以避免频繁创建销毁、实现重复利用。</li>
<li>优点<blockquote>
<ol>
<li>提高响应速度（减少了创建新线程的时间）</li>
<li>降低资源消耗（重复利用线程池中线程，不需要每次都创建）</li>
<li>便于线程管理</li>
</ol>
<p>	</p>
</blockquote>
</li>
<li>ExecutorService：真正的线程池接口。常见子类ThreadPoolExecutor。</li>
<li>void execute(Runnable command) ：执行任务&#x2F;命令，没有返回值，一般用来执行Runnable</li>
<li><T> Future<T> submit(Callable<T> task)：执行任务，有返回值，一般用来执行Callable</li>
<li>void shutdown() ：关闭连接池</li>
<li>创建线程池的方式：<blockquote>
<ol>
<li>直接通过ThreadPoolExecutor实现类new</li>
<li>通过工厂类Executors的静态方法创建，本质上也是通过1)创建的线程池</li>
</ol>
</blockquote>
</li>
</ul>
</blockquote>
<figure class="highlight livescript"><table><tr><td class="code"><pre><code class="hljs livescript">public <span class="hljs-keyword">static</span> <span class="hljs-literal">void</span> main(<span class="hljs-built_in">String</span>[] args) &#123;<br>		<span class="hljs-regexp">//创建一个包含10个线程的线程池</span><br><span class="hljs-regexp">		ExecutorService executorService = Executors.newFixedThreadPool(10);</span><br><span class="hljs-regexp">//</span>		ExecutorService executorService = Executors.newSingleThreadExecutor();<br>		<span class="hljs-keyword">for</span> (int i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">12</span>; i++) &#123;<br>			executorService.execute<span class="hljs-function"><span class="hljs-params">(()-&gt;&#123;</span></span><br><span class="hljs-params"><span class="hljs-function">				System.out.println(Thread.currentThread().getName());</span></span><br><span class="hljs-params"><span class="hljs-function">			&#125;)</span>;</span><br><span class="hljs-function">		&#125;</span><br><span class="hljs-function">		<span class="hljs-title">executorService</span>.<span class="hljs-title">shutdown</span><span class="hljs-params">()</span>;</span><br><span class="hljs-function">	&#125;</span><br></code></pre></td></tr></table></figure>
<h2 id="3-线程的同步"><a href="#3-线程的同步" class="headerlink" title="3.线程的同步"></a>3.线程的同步</h2><blockquote>
<ul>
<li>synchronized (lock){}</li>
<li>()中是一个锁对象，任意对象都可以做锁，称为互斥锁，作用是只允许一个线程进入执行，其他线程等待</li>
<li>具有原子性，不可分割</li>
<li>synchronized()可重入锁（同一个线程可以无限次获取同一个锁）</li>
<li>避免死锁：不要嵌套synchronized，即使有嵌套，锁对象尽量少</li>
</ul>
</blockquote>
<h3 id="synchronized和Lock的区别"><a href="#synchronized和Lock的区别" class="headerlink" title="synchronized和Lock的区别"></a>synchronized和Lock的区别</h3><blockquote>
<ul>
<li>synchronized不需要手动上锁和解锁，lock需要手动上锁和解锁</li>
<li>synchronized能实现的功能lock都可以实现，且lock更加强大</li>
</ul>
</blockquote>
<h2 id="JUC工具类"><a href="#JUC工具类" class="headerlink" title="JUC工具类"></a>JUC工具类</h2><h3 id="ReentrantReadWriteLock"><a href="#ReentrantReadWriteLock" class="headerlink" title="ReentrantReadWriteLock"></a>ReentrantReadWriteLock</h3><figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.atguigu.juc;<br><br><span class="hljs-keyword">import</span> java.util.concurrent.ExecutorService;<br><span class="hljs-keyword">import</span> java.util.concurrent.Executors;<br><span class="hljs-keyword">import</span> java.util.concurrent.locks.ReadWriteLock;<br><span class="hljs-keyword">import</span> java.util.concurrent.locks.ReentrantReadWriteLock;<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyQueue</span> &#123;<br>	<span class="hljs-comment">// 创建读写锁</span><br>	<span class="hljs-type">ReadWriteLock</span> <span class="hljs-variable">rwl</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ReentrantReadWriteLock</span>();<br><br>	<span class="hljs-keyword">private</span> Object obj;<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">readObj</span><span class="hljs-params">()</span> &#123;<br>		<span class="hljs-comment">// 上读锁</span><br>		rwl.readLock().lock();<br>		<span class="hljs-keyword">try</span> &#123;<br>			System.out.println(Thread.currentThread().getName() + <span class="hljs-string">&quot;当前线程读到的内容是：&quot;</span> + obj);<br><br>		&#125; <span class="hljs-keyword">finally</span> &#123;<br>			<span class="hljs-comment">// 解读锁</span><br>			rwl.readLock().unlock();<br>		&#125;<br>	&#125;<br><br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">writeObj</span><span class="hljs-params">(Object obj)</span> &#123;<br>		<span class="hljs-comment">// 上写锁</span><br>		rwl.writeLock().lock();<br>		<span class="hljs-keyword">try</span> &#123;<br>			<span class="hljs-built_in">this</span>.obj = obj;<br>			System.out.println(Thread.currentThread().getName() + <span class="hljs-string">&quot;当前线程写入的内容是：&quot;</span> + obj);<br>		&#125; <span class="hljs-keyword">finally</span> &#123;<br>			<span class="hljs-comment">// 解写锁</span><br>			rwl.writeLock().unlock();<br>		&#125;<br>	&#125;<br><br>&#125;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Description</span>: 一个线程写入,100个线程读取</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ReadWriteLockDemo</span> &#123;<br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> InterruptedException &#123;<br>		<span class="hljs-comment">// 创建资源对象</span><br>		<span class="hljs-type">MyQueue</span> <span class="hljs-variable">mq</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">MyQueue</span>();<br>		<span class="hljs-comment">// 一个线程写入</span><br>		<span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(() -&gt; &#123;<br>			mq.writeObj(<span class="hljs-string">&quot;写入的内容&quot;</span>);<br>		&#125;, <span class="hljs-string">&quot;AA&quot;</span>).start();<br>		<span class="hljs-comment">// 100个线程读取</span><br>		<span class="hljs-type">ExecutorService</span> <span class="hljs-variable">executorService</span> <span class="hljs-operator">=</span> Executors.newFixedThreadPool(<span class="hljs-number">100</span>);<br>		<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">100</span>; i++) &#123;<br>			executorService.execute(() -&gt; &#123;<br>				mq.readObj();<br>			&#125;);<br>		&#125;<br>		executorService.shutdown();<br>	&#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h3 id="CountDownLatch"><a href="#CountDownLatch" class="headerlink" title="CountDownLatch"></a>CountDownLatch</h3><figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.atguigu.juc;<br><br><span class="hljs-keyword">import</span> java.util.concurrent.CountDownLatch;<br><br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Description</span>:</span><br><span class="hljs-comment"> *  *让一些线程阻塞直到另一些线程完成一系列操作后才被唤醒。</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * 解释：6个同学陆续离开教室后值班同学才可以关门。</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * main主线程必须要等前面6个线程完成全部工作后，自己才能开干 </span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">CountDownLatchDemo</span><br>&#123;<br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> InterruptedException<br>	&#123;<br>		<span class="hljs-type">CountDownLatch</span> <span class="hljs-variable">cd</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">CountDownLatch</span>(<span class="hljs-number">6</span>);<br>		<span class="hljs-comment">//6个同学离开教室</span><br>		<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">1</span>; i &lt;= <span class="hljs-number">6</span>; i++) &#123;<br>			<span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(()-&gt;&#123;System.out.println(Thread.currentThread().getName()+<span class="hljs-string">&quot;号同学离开教室&quot;</span>);&#125;, String.valueOf(i)).start();<br>		<span class="hljs-comment">//减少计数</span><br>			cd.countDown();<br>		&#125;<br>		<span class="hljs-comment">//等待		</span><br>		cd.await();<br>		<span class="hljs-comment">//班长锁门</span><br>		System.out.println(Thread.currentThread().getName()+<span class="hljs-string">&quot;班长锁门&quot;</span>);<br>	&#125;<br><br><br>&#125;<br></code></pre></td></tr></table></figure>
<h3 id="CyclicBarrier"><a href="#CyclicBarrier" class="headerlink" title="CyclicBarrier"></a>CyclicBarrier</h3><figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.atguigu.juc;<br><br><span class="hljs-keyword">import</span> java.util.concurrent.BrokenBarrierException;<br><span class="hljs-keyword">import</span> java.util.concurrent.CyclicBarrier;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Description</span>: TODO(这里用一句话描述这个类的作用)  </span><br><span class="hljs-comment"> *</span><br><span class="hljs-comment"> * CyclicBarrier</span><br><span class="hljs-comment"> * 的字面意思是可循环（Cyclic）使用的屏障（Barrier）。它要做的事情是，</span><br><span class="hljs-comment"> * 让一组线程到达一个屏障（也可以叫同步点）时被阻塞，</span><br><span class="hljs-comment"> * 直到最后一个线程到达屏障时，屏障才会开门，所有</span><br><span class="hljs-comment"> * 被屏障拦截的线程才会继续干活。</span><br><span class="hljs-comment"> * 线程进入屏障通过CyclicBarrier的await()方法。</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * 集齐7颗龙珠就可以召唤神龙</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">CyclicBarrierDemo</span><br>&#123;<br>	<span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> <span class="hljs-variable">NUMBER</span> <span class="hljs-operator">=</span> <span class="hljs-number">7</span>;<br>	<br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span><br>	&#123;<br>		<span class="hljs-comment">//CyclicBarrier(int parties, Runnable barrierAction) </span><br>		<span class="hljs-type">CyclicBarrier</span> <span class="hljs-variable">cb</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">CyclicBarrier</span>(NUMBER, ()-&gt; &#123;System.out.println(<span class="hljs-string">&quot;可以召唤神龙了&quot;</span>);&#125;);<br>		<span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>;i&lt;=NUMBER;i++) &#123;<br>			<span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(()-&gt;&#123;System.out.println(Thread.currentThread().getName()+<span class="hljs-string">&quot;号被收集&quot;</span>);<br>			<span class="hljs-keyword">try</span> &#123;<br>				cb.await();<br>			&#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;<br>				e.printStackTrace();<br>			&#125; <br>			&#125;,String.valueOf(i) ).start();<br>		&#125;<br>	&#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h3 id="Semaphore"><a href="#Semaphore" class="headerlink" title="Semaphore"></a>Semaphore</h3><figure class="highlight java"><table><tr><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.atguigu.juc;<br><br><span class="hljs-keyword">import</span> java.util.Random;<br><span class="hljs-keyword">import</span> java.util.concurrent.Semaphore;<br><span class="hljs-keyword">import</span> java.util.concurrent.TimeUnit;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> * <span class="hljs-doctag">@Description</span>: TODO(这里用一句话描述这个类的作用)</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> *               在信号量上我们定义两种操作： acquire（获取）</span><br><span class="hljs-comment"> *               当一个线程调用acquire操作时，它要么通过成功获取信号量（信号量减1）， 要么一直等下去，直到有线程释放信号量，或超时。</span><br><span class="hljs-comment"> *               release（释放）实际上会将信号量的值加1，然后唤醒等待的线程。</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> *               信号量主要用于两个目的，一个是用于多个共享资源的互斥使用，另一个用于并发线程数的控制。</span><br><span class="hljs-comment"> * </span><br><span class="hljs-comment"> *               情景：3个停车位，6部汽车争抢车位</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">SemaphoreDemo</span> &#123;<br>	<span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>		<span class="hljs-comment">// 3个停车位</span><br>		<span class="hljs-type">Semaphore</span> <span class="hljs-variable">semaphore</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Semaphore</span>(<span class="hljs-number">3</span>);<br>		<span class="hljs-comment">// 6部汽车抢车位</span><br>		<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">1</span>; i &lt;= <span class="hljs-number">6</span>; i++) &#123;<br>			<span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(() -&gt; &#123;<br>				<span class="hljs-comment">// 获取资源</span><br>				<span class="hljs-keyword">try</span> &#123;<br>					semaphore.acquire();<br>					System.out.println(Thread.currentThread().getName() + <span class="hljs-string">&quot;号驶入停车位&quot;</span>);<br>					<span class="hljs-comment">// 停车3秒</span><br>					TimeUnit.SECONDS.sleep(<span class="hljs-number">3</span>);<br>					System.out.println(Thread.currentThread().getName() + <span class="hljs-string">&quot;号驶出停车位&quot;</span>);<br>					<span class="hljs-comment">// 释放资源</span><br>					semaphore.release();<br>				&#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;<br>					e.printStackTrace();<br>				&#125;<br>			&#125;, String.valueOf(i)).start();<br>		&#125;<br>	&#125;<br>&#125;<br><br></code></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL</title>
    <url>/2021/04/25/mysql/</url>
    <content><![CDATA[<h1 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h1><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src="https://pic2.zhimg.com/9bb3acbba65f349299df27a93286ffb1_r.jpg" alt="Mysql架构"></p>
<ul>
<li><p>连接层：最上层是一些客户端和连接服务。主要完成一些类似于连接处理，授权认证以及相关的安全方案。</p>
</li>
<li><p>服务层：</p>
<table>
<thead>
<tr>
<th>Management Serveices &amp; Utilities</th>
<th>系统管理和控制工具</th>
</tr>
</thead>
<tbody><tr>
<td>SQL Interface:</td>
<td>SQL接口。接受用户的SQL命令，并且返回用户需要查询的结果。比如select from就是调用SQL Interface</td>
</tr>
<tr>
<td>Parser</td>
<td>解析器。 SQL命令传递到解析器的时候会被解析器验证和解析</td>
</tr>
<tr>
<td>Optimizer</td>
<td>查询优化器。  SQL语句在查询之前会使用查询优化器对查询进行优化，比如有where条件时，优化器来决定先投影还是先过滤。</td>
</tr>
<tr>
<td>Cache和Buffer</td>
<td>查询缓存。如果查询缓存有命中的查询结果，查询语句就可以直接去查询缓存中取数据。这个缓存机制是由一系列小缓存组成的。比如表缓存，记录缓存，key缓存，权限缓存等</td>
</tr>
</tbody></table>
</li>
<li><p>引擎层：存储引擎层，存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API与存储引擎进行通信。不同的存储引擎具有不同的功能，这样我们可以根据自己的实际需求进行选取。</p>
</li>
<li><p>存储层：数据存储层，主要是将数据存储在运行与裸设备的文件系统之上，并完成与存储引擎的交互。</p>
</li>
</ul>
<h2 id="大致的查询流程"><a href="#大致的查询流程" class="headerlink" title="大致的查询流程"></a>大致的查询流程</h2><ul>
<li>MySQL客户端通过协议与MySQL服务器建立连接，发送查询语句，先检查缓存，如果命中直接返回结果。</li>
<li>语法解析器和预处理：首先MySQL通过关键字将SQL语句进行解析，并生成一颗对应的“解析树”。MySQL解析器使用MySQL语法规则验证和解析查询，预处理器则根据一些MySQL规则进一步检查解析树是否合法。</li>
<li>查询优化器当解析树被认为是合法的了，并且由优化器将其转化成执行计划。一条查询可以有很多种执行方式，最后都返回相同的结果。查询优化器会找出其中最好的执行计划。</li>
<li>MySQL使用默认的B+树索引</li>
</ul>
<h2 id="存储引擎"><a href="#存储引擎" class="headerlink" title="存储引擎"></a>存储引擎</h2><ul>
<li><p>InnoDB:默认使用</p>
<ul>
<li>InnoDB自增主键：InnoDB表如果没有指定主键，InnoDB会自动从表中选择合适的字段作为主键，如果没有合适的字段，InnoDB会创建一个不可见的，长度为6个字节的row_id。InnoDB维护了一个全局的dict_sys.row_id值，所有无主键的InnoDB表，每插入一行数据，都将当前的dict_sys.row_id值作为要插入数据的row_id，然后把dict_sys.row_id的值加1。</li>
</ul>
</li>
<li><p>MyISAM：自带系统表使用</p>
</li>
<li><p>InnoDB的数据文件自己就是索引文件。MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。</p>
</li>
<li><p>由于InnoDB的数据文件自己要按主键汇集，因此InnoDB要求必须有主键（MyISAM能够没有），若是没有显示指定，则MySQL系统会自动选择一个能够唯一标识数据记录的列作为主键，若不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整型。</p>
<table>
<thead>
<tr>
<th>对比项</th>
<th>MyISAM</th>
<th>InnoDB</th>
</tr>
</thead>
<tbody><tr>
<td>外键</td>
<td>不支持</td>
<td>支持</td>
</tr>
<tr>
<td>事务</td>
<td>不支持</td>
<td>支持</td>
</tr>
<tr>
<td>行表锁</td>
<td>表锁，即使操作一条记录也会锁住整个表，不适合高并发的操作</td>
<td>行锁,操作时只锁某一行，不对其它行有影响，,<font color=red>适合高并发的操作</font></td>
</tr>
<tr>
<td>缓存</td>
<td>只缓存索引，不缓存真实数据</td>
<td>不仅缓存索引还要缓存真实数据，对内存要求较高，而且内存大小对性能有决定性的影响</td>
</tr>
<tr>
<td>关注点</td>
<td>节省资源、消耗少、简单业务</td>
<td>并发写、事务、更大资源</td>
</tr>
<tr>
<td>默认安装</td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>默认使用</td>
<td>N</td>
<td>Y</td>
</tr>
<tr>
<td>自带系统表使用</td>
<td>Y</td>
<td>N</td>
</tr>
</tbody></table>
</li>
</ul>
<h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><ul>
<li><p>创建索引的方式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><code class="hljs sqlite">--直接创建索引<br>create index index_name on table (column(length))<br>--修改表结构的方式添加索引<br>alter table table_name add index index_name on (column(length))<br>--创建表的时候同时创建索引<br>CREATE TABLE `table` (<br>	`id` int(11) NOT NULL AUTO_INCREMENT ,<br>	`title` char(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL ,<br>	`content` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL ,<br>	`time` int(10) NULL DEFAULT NULL ,<br>	PRIMARY KEY (`id`),<br>	INDEX index_name (title(length))<br>)<br>-- 删除索引<br>ALTER TABLE table_name DROP INDEX index_name<br>DROP INDEX index_name ON table_name<br></code></pre></td></tr></table></figure>


</li>
<li><p>Btree索引：</p>
<p><a href="https://imgtu.com/i/4qdvuQ"><img src="https://z3.ax1x.com/2021/10/03/4qdvuQ.png" alt="4qdvuQ.png"></a></p>
<ul>
<li>查找过程：<ul>
<li>如果要查找数据项29，那么首先会把磁盘块1由磁盘加载到内存，此时发生一次IO，在内存中用二分查找确定29在17和35之间，锁定磁盘块1的P2指针，内存时间因为非常短（相比磁盘的IO）可以忽略不计，通过磁盘块1的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，29在26和30之间，锁定磁盘块3的P2指针，通过指针加载磁盘块8到内存，发生第三次IO，同时内存中做二分查找找到29，结束查询，总计三次IO。</li>
</ul>
</li>
</ul>
</li>
<li><p>B+tree索引：MySQL默认索引</p>
<p><a href="https://imgtu.com/i/4qwF3T"><img src="https://z3.ax1x.com/2021/10/03/4qwF3T.png" alt="4qwF3T.png"></a></p>
</li>
<li><p>b树和b+树的区别：</p>
<ul>
<li>b树索引，关键字，记录是放在一起的。b+树的非叶子节点只有指向下一个节点的索引和关键字，记录只存放在叶子节点中。</li>
<li>b树中越靠近根节点的记录查的越快。而b+树每个记录的查找时间都是一样的，都需要从根节点走到叶子节点，并在叶子节点中查找关键字。</li>
<li>b+树的性能更好。因为b+树的非叶子节点不存放实际的数据，因此可以容纳的元素的更多，同样的数据下，树高比b树小，这样带来的好处是减少磁盘访问次数（例如，同样的数据量，b树三层，b+树两层，这样的话b+树就少一次IO操作，会快很多）。尽管B+树找到一个记录所需的比较次数要比b树多，但是一次磁盘访问的时间远远比内存比较的时间长。实际中，b+树的性能更好一些，而且b+树的叶子节点使用指针连接在一起，方便顺序遍历（例如查看一个目录下的所有文件，一个表中的所有记录等），这也是很多数据库和文件系统使用b+树的缘故。</li>
</ul>
</li>
<li><p>为什么B+树更适合在实际应用中做操作系统的文件索引和数据库索引？</p>
<ul>
<li>b+树的磁盘读写代价更低</li>
<li>b+树的查询效率更稳定。每个数据的查询效率相当。</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>C&#x2F;S 架构</li>
<li>服务器程序是 mysqld.exe</li>
<li>客户端程序是 mysql.exe</li>
<li>cmd的打开方式：</li>
</ul>
</blockquote>
<figure class="highlight routeros"><table><tr><td class="code"><pre><code class="hljs routeros">mysql -uroot -p123456 -h127.0.0.1 -P3306<br>长选项：<br>mysql <span class="hljs-attribute">--host</span>=主机地址 <span class="hljs-attribute">--port</span>=端口 <span class="hljs-attribute">--user</span>=用户名 <span class="hljs-attribute">--password</span>=密码 默认数据库<br></code></pre></td></tr></table></figure>
<h2 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h2><blockquote>
<ul>
<li>查看服务器中的所有数据库</li>
</ul>
</blockquote>
<figure class="highlight abnf"><table><tr><td class="code"><pre><code class="hljs abnf">show databases<span class="hljs-comment">;</span><br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>创建新的数据库</li>
<li>数据库以目录的形式保存在服务器<br>安装目录&#x2F;data目录下</li>
</ul>
</blockquote>
<figure class="highlight abnf"><table><tr><td class="code"><pre><code class="hljs abnf">show databases<span class="hljs-comment">;</span><br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>切换当前工作数据库<br>mysql&gt; use company;</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>查看当前数据库中的所有表<br>show tables;</li>
<li>跨库查看表<br>show tables from 其他库</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>把.sql文件中数据导入数据库<br>source e:sql&#x2F;company.sql;</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>查看表中的所有数据<br>select * from employees;</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>创建world数据库, 并导入 world.sql , 查看表中的数据</li>
<li>create database world;</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>use world;</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>source d:&#x2F;mywork&#x2F;mysql&#x2F;world.sql;</li>
<li>查看当前数据库<br>select database();</li>
<li>创建表</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> customer(<br>	id <span class="hljs-type">int</span>,<br>	<span class="hljs-type">name</span> <span class="hljs-type">varchar</span>(<span class="hljs-number">20</span>),<br>	age <span class="hljs-type">int</span>,<br>	email <span class="hljs-type">varchar</span>(<span class="hljs-number">50</span>),<br>	gender enum(<span class="hljs-string">&#x27;男&#x27;</span>, <span class="hljs-string">&#x27;女&#x27;</span>)<br>);<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>丢弃表<br>drop table customer;</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>查看表结构<br>describe customer;</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>desc 表名</li>
<li>查看表的建表语句<br>show create table 表名;</li>
<li>插入数据</li>
<li>存储引擎 : InnoDB(支持事务, 外键等高级特性), MyISAM(不支持事务, 不支持外键)</li>
<li>show engines;显示支持的引擎</li>
<li>&#96;&#96;号专门用于包围数据库对象的名称(数据库, 表, 列, 主键, 外键, 索引, 函数, 存储过程, 触发器)<br>查询表中的所有数据</li>
<li>select * from 表名; from关键字可以省略</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql">插入数据<br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> customer(<br>	id,<br>	<span class="hljs-type">name</span>,<br>	age,<br>	email,<br>	gender <br>) <span class="hljs-keyword">values</span> (<br>	<span class="hljs-number">1</span>,<br>	<span class="hljs-string">&#x27;张三&#x27;</span>,<br>	<span class="hljs-number">30</span>,<br>	<span class="hljs-string">&#x27;zhang3@qq.com&#x27;</span>,<br>	<span class="hljs-string">&#x27;男&#x27;</span><br>);<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>修改数据, 如果没有where 语句会导致修改所有记录</li>
</ul>
</blockquote>
<figure class="highlight gams"><table><tr><td class="code"><pre><code class="hljs gams">update customer <span class="hljs-keyword">set</span> <br>	age <span class="hljs-comment">= 3,</span><br>	email <span class="hljs-comment">=</span> <span class="hljs-comment">&#x27;QQQ&#x27;</span><br>where <br>	id <span class="hljs-comment">= 1</span>;<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>删除数据, 如果没有where语句会导致删除所有记录</li>
</ul>
</blockquote>
<figure class="highlight applescript"><table><tr><td class="code"><pre><code class="hljs applescript">delete <span class="hljs-keyword">from</span> customer<br><span class="hljs-keyword">where</span> <span class="hljs-built_in">id</span> = <span class="hljs-number">3</span>;<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>针对数据的操作.</li>
</ul>
</blockquote>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><code class="hljs n1ql">C <span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span>  //<span class="hljs-keyword">Create</span> <br>R <span class="hljs-keyword">select</span> 		//Retrieve<br>U <span class="hljs-keyword">update</span> 		//<span class="hljs-keyword">Update</span><br>D <span class="hljs-keyword">delete</span> 		//<span class="hljs-keyword">Delete</span><br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>查看表结构 :<br>desc 表名;</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>SQL注意 :<blockquote>
<ul>
<li>SQL 语言大小写不敏感。 </li>
<li>SQL 可以写在一行或者多行</li>
<li>关键字不能被缩写也不能分行</li>
<li>各子句一般要分行写。</li>
<li>使用缩进提高语句的可读性。</li>
</ul>
</blockquote>
</li>
<li>给列起别名, 可以省略as关键字, 别名中如果有特殊符号, 可以使用””包围.</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">select</span> <br>	population <span class="hljs-keyword">as</span> pop,<br>	<span class="hljs-type">name</span> &quot;国家 名称&quot;,<br>	code<br><span class="hljs-keyword">from</span> <br>	country;<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>where 条件布尔(一个表达式，返回结果永远是一个布尔值)</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>执行顺序SQL：先from, 再where 最后select</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">SELECT</span> <br>	employee_id empId, <br>	last_name <span class="hljs-type">name</span>, <br>	job_id job, <br>	department_id deptId<br><span class="hljs-keyword">FROM</span>   <br>	employees<br><span class="hljs-keyword">WHERE</span>  <br>	department_id = <span class="hljs-number">90</span> ;<br>	<br><span class="hljs-comment">-- 错误!! where中不可以使用列的别名, 因为此时虚表的列还没有生成好.</span><br></code></pre></td></tr></table></figure>

<blockquote>
<ul>
<li>Between a and b 都包含</li>
<li>like <blockquote>
<ul>
<li>% 表示任意个任意字符</li>
<li>_ 表示一个任意字符</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><code class="hljs n1ql">查询姓名中第2个字母是o其他无所谓<br><span class="hljs-keyword">SELECT</span> last_name<br><span class="hljs-keyword">FROM</span>   employees<br><span class="hljs-keyword">WHERE</span>  last_name <span class="hljs-keyword">LIKE</span> <span class="hljs-string">&#x27;_o%&#x27;</span>;<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>只要有null参与比较运算, 结果一定是false</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql">查询哪些国家没有首都<br>只要有<span class="hljs-keyword">null</span>参与比较运算, 结果一定是<span class="hljs-keyword">false</span><br><span class="hljs-comment">--错误!!</span><br><span class="hljs-keyword">select</span> <br>	<span class="hljs-type">name</span>,<br>	continent,<br>	capital<br><span class="hljs-keyword">from</span> <br>	country <br><span class="hljs-keyword">where</span> <br>	capital = <span class="hljs-keyword">null</span>;<br>	<br>查询哪些国家没有首都<br><span class="hljs-keyword">select</span> <br>	<span class="hljs-type">name</span>,<br>	continent,<br>	capital<br><span class="hljs-keyword">from</span> <br>	country <br><span class="hljs-keyword">where</span> <br>	capital <span class="hljs-keyword">is</span> <span class="hljs-keyword">null</span>;<br><br>查询哪些国家有首都<br><span class="hljs-keyword">select</span> <br>	<span class="hljs-type">name</span>,<br>	continent,<br>	capital<br><span class="hljs-keyword">from</span> <br>	country <br><span class="hljs-keyword">where</span> <br>	capital <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span>;<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>where 也支持算术运算, 结果为0表示假非0表示真</li>
<li>distinct 去重, 要求列真的有重复的</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">select</span><br>	<span class="hljs-keyword">distinct</span><br>		continent,<br>		<span class="hljs-type">name</span><br><span class="hljs-keyword">from</span><br>	country;<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>order by  可以排序, 只是给结果集虚表排序</li>
<li>默认是升充(asc)	</li>
<li>降序必须指定(desc)</li>
<li>order by 可以使用列的别名.</li>
<li>order by  列1, 列2 先以列1排序, 再在相同的列1数据中, 再依据列2再微排.</li>
</ul>
</blockquote>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">SELECT</span> <br>	last_name, <br>	department_id, <br>	salary<br><span class="hljs-keyword">FROM</span>   <br>	employees<br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> <br>	department_id, <br>	salary <span class="hljs-keyword">DESC</span>;<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>写SQL的步骤 : <blockquote>
<ul>
<li>(1) from 基表</li>
<li>(2) where 过滤哪些行</li>
<li>(3) select 选择哪些列</li>
<li>(4) order by 以哪些列为排序依据.</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h2 id="多表查询"><a href="#多表查询" class="headerlink" title="多表查询"></a>多表查询</h2><h3 id="表联接"><a href="#表联接" class="headerlink" title="表联接"></a>表联接</h3><figure class="highlight axapta"><table><tr><td class="code"><pre><code class="hljs axapta">表联接<br><span class="hljs-keyword">select</span> <br>	* <br><span class="hljs-keyword">from</span> <br>	city2,<br>	country2;<br>笛卡尔集中的数据绝大多数都是垃圾, 必须使用行过滤.<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>笛卡尔集中的数据绝大多数都是垃圾, 必须使用行过滤.</li>
<li>解决列名冲突可以使用表名限定</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><span class="hljs-comment">--表名也可以起别名, 而且是如果多表联接, 最好起别名.</span><br><span class="hljs-keyword">select</span> <br>	ci.name cityName,<br>	ci.population cityPop,<br>	co.name countryName,<br>	co.population countryPop,<br>	co.continent<br><span class="hljs-keyword">from</span> <br>	city2 ci,<br>	country2 co <br><span class="hljs-keyword">where</span> <br>	ci.countrycode = co.code;<br>	<br><span class="hljs-comment">--一旦给表起了别名, 原名不可以使用, 必须使用别名. 原因是from最先执行, 它把原始表名变了.</span><br><span class="hljs-keyword">select</span> <br>	city2.name cityName,<br>	city2.population cityPop,<br>	country2.name countryName,<br>	country2.population countryPop,<br>	country2.continent<br><span class="hljs-keyword">from</span> <br>	city2 ci,<br>	country2 co <br><span class="hljs-keyword">where</span> <br>	city2.countrycode = country2.code;<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>复习 :<blockquote>
<ul>
<li>C&#x2F;S </li>
<li>Server : mysqld.exe </li>
<li>Clinet : mysql.exe </li>
<li>必须通过客户端才能使用服务器</li>
<li>需要提供IP, 端口, 用户名, 密码.</li>
<li>mysql -h127.0.0.1 -P3306 -uroot -p123456</li>
<li>mysql –host&#x3D;127.0.0.1 –port&#x3D;3306 –user&#x3D;root –password&#x3D;123456 默认数据库</li>
<li>mysql –host&#x3D;主机地址 –port&#x3D;端口 –user&#x3D;用户名 –password&#x3D;密码 默认数据库</li>
<li>查看所有库<br>show databases;</li>
<li>切换成当前工作数据库<br>use 数据库;</li>
<li>查看库中的表<br>show tables;</li>
<li>跨库查看表<br>show tables from 其他库</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<blockquote>
<blockquote>
<ul>
<li>查看表结构<br>desc 表名;</li>
</ul>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<ul>
<li>查看表的建表语句<br>show create table 表名;</li>
</ul>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<ul>
<li>存储引擎 : InnoDB(支持事务, 外键等高级特性), MyISAM(不支持事务, 不支持外键)</li>
</ul>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<ul>
<li>&#96;&#96;专门用于包围数据库对象的名称(数据库, 表, 列, 主键, 外键, 索引, 函数, 存储过程, 触发器)</li>
<li>查询表中的所有数据<br>select * from 表名; from关键字可以省略</li>
</ul>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<ul>
<li>相看当前数据库<br>select<br>  database(), – 函数<br>  now(),<br>  version()</li>
</ul>
</blockquote>
</blockquote>
<p>select<br>    100, – 常量<br>    ‘abc’<br>from<br>    dual;</p>
<blockquote>
<blockquote>
<ul>
<li>使用用户变量<br>set @var1 &#x3D; 100, @var2 &#x3D; ‘abc’;</li>
</ul>
</blockquote>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><span class="hljs-comment">--SQL99标准, 用内联代替逗号联接.</span><br><span class="hljs-keyword">select</span> <br>	ci.name cityName,<br>	ci.population cityPop,<br>	co.name countryName,<br>	co.population countryPop,<br>	co.continent<br><span class="hljs-keyword">from</span> <br>	city2 ci<br><span class="hljs-keyword">inner</span> <span class="hljs-keyword">join</span> <br>	country2 co <br><span class="hljs-keyword">on</span><br>	ci.countrycode = co.code <span class="hljs-comment">-- 联接条件</span><br><span class="hljs-keyword">where</span>  <br>	ci.population &gt; <span class="hljs-number">5000000</span> <span class="hljs-comment">-- 普通过滤</span><br>	<br><br><span class="hljs-comment">-- where 和 on在内联时可以混用, 但是千万不要.</span><br><span class="hljs-keyword">select</span> <br>	ci.name cityName,<br>	ci.population cityPop,<br>	co.name countryName,<br>	co.population countryPop,<br>	co.continent<br><span class="hljs-keyword">from</span> <br>	city2 ci<br><span class="hljs-keyword">join</span> <br>	country2 co <br><span class="hljs-keyword">on</span><br>	ci.countrycode = co.code <span class="hljs-comment">-- 联接条件</span><br><span class="hljs-keyword">where</span>  <br>	ci.population &gt; <span class="hljs-number">5000000</span> <span class="hljs-comment">-- 普通过滤</span><br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>注意：<blockquote>
<ul>
<li>内联接的逻辑是从笛尔集中取出来的是满足联接条件的记录. 有可能会导致某张表的数据不完整.</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">select</span> <br>	co.name countryName,<br>	ci.name capitalName,<br>	co.capital<br><span class="hljs-keyword">from</span> <br>	country2 co <br><span class="hljs-keyword">left outer join</span> <span class="hljs-comment">-- 左外联接, 保证左表数据完整</span><br>	city2 ci <br><span class="hljs-keyword">on</span> <br>	co.capital = ci.id ;<br>	<br><span class="hljs-keyword">select</span> <br>	co.name countryName,<br>	ci.name capitalName,<br>	co.capital<br><span class="hljs-keyword">from</span> <br>	country2 co <br><span class="hljs-keyword">right outer join</span> <span class="hljs-comment">-- 左外联接, 保证左表数据完整</span><br>	city2 ci <br><span class="hljs-keyword">on</span> <br>	co.capital = ci.id ;<br>	<br><span class="hljs-comment">--外联时可以省略outer关键字	</span><br><span class="hljs-keyword">select</span> <br>	co.name countryName,<br>	ci.name capitalName,<br>	co.capital<br><span class="hljs-keyword">from</span> <br>	country2 co <br><span class="hljs-keyword">left join</span><br>	city2 ci <br><span class="hljs-keyword">on</span> <br>	co.capital = ci.id ;<br></code></pre></td></tr></table></figure>
<h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><h4 id="单行函数"><a href="#单行函数" class="headerlink" title="单行函数"></a>单行函数</h4><figure class="highlight oxygene"><table><tr><td class="code"><pre><code class="hljs oxygene">单行函数 -- 作用于结果集中的每一条记录的.<br><span class="hljs-keyword">select</span> <br>	upper(name),<br>	now(),<br>	<span class="hljs-keyword">concat</span>(continent, code2)<br><span class="hljs-keyword">from</span> <br>	country<span class="hljs-punctuation">;</span><br>	<br>查询国家表中的数据, 把国家名称,大洲, 国家代码 连接起来, 中间使用<span class="hljs-string">&#x27;=&gt;&#x27;</span>连接.<br>--<span class="hljs-keyword">concat</span>(<span class="hljs-keyword">concat</span>(<span class="hljs-keyword">concat</span>(<span class="hljs-keyword">concat</span>(name, <span class="hljs-string">&#x27;=&gt;&#x27;</span>), continent), <span class="hljs-string">&#x27;=&gt;&#x27;</span>), code)<br><span class="hljs-keyword">select</span> <br>	<span class="hljs-keyword">concat</span>(name, <span class="hljs-string">&#x27;=&gt;&#x27;</span>, continent, <span class="hljs-string">&#x27;=&gt;&#x27;</span>, code)<br><span class="hljs-keyword">from</span> <br>	country<span class="hljs-punctuation">;</span><br></code></pre></td></tr></table></figure>
<h4 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h4><figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">组函数（聚合函数） -- 作用于一组数据, 最终针对一组只有一个结果. 也称为统计处理<br><span class="hljs-function"><span class="hljs-title">avg</span><span class="hljs-params">()</span></span> 平均<br><span class="hljs-function"><span class="hljs-title">max</span><span class="hljs-params">()</span></span> 最大<br><span class="hljs-function"><span class="hljs-title">min</span><span class="hljs-params">()</span></span> 最小<br><span class="hljs-function"><span class="hljs-title">count</span><span class="hljs-params">()</span></span> 计数<br><span class="hljs-function"><span class="hljs-title">sum</span><span class="hljs-params">()</span></span>求和<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>获取表中的记录数, 使用count(*)最好.</li>
<li>如果有group by , 必须让分组依据的列放在select中.</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">select</span> <br>	<span class="hljs-comment">-- name, 代表个体信息的列</span><br>	continent,<br>	max(population)<br><span class="hljs-keyword">from</span> <br>	country <br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <br>	continent;<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>对分组的虚表进行过滤, 必须使用having</li>
<li>having的执行晚于select, 所以可以使用列的别名</li>
</ul>
</blockquote>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">select</span> <br>	GovernmentForm,<br>	<span class="hljs-built_in">count</span>(*) ct<br><span class="hljs-keyword">from</span> <br>	country<br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <br>	GovernmentForm<br><span class="hljs-keyword">having</span><br>	ct &gt; <span class="hljs-number">10</span><br><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> <br>	ct;<br></code></pre></td></tr></table></figure>
<p>sql语句一般的执行顺序：</p>
<ol>
<li>from 		确定基表</li>
<li>join 		如果一张基表不够, 再联接其他表</li>
<li>on 			如果有联接表 必须要有on</li>
<li>where 		过滤总基表中的行</li>
<li>group by 	分组, 分组依据的列.</li>
<li>select 		把分组依据的列放在select后, 再考虑要选择哪些列, 及进行哪些函数调用….</li>
<li>having 		进一步把分组后的虚表行过滤</li>
<li>order by 	最终表的一个排序显示.</li>
</ol>
<h3 id="子查询"><a href="#子查询" class="headerlink" title="子查询"></a>子查询</h3><blockquote>
<ul>
<li>子查询 : 通常需要多步执行的简单查询</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>针对表中的数据进行的操作, 这样的语言称DML(数据操纵语句)<blockquote>
<ul>
<li>select R</li>
<li>update U </li>
<li>delete D </li>
<li>insert C</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>针对数据库中的对象的操作, 这样的语言称DDL(数据定义语言)</li>
<li>数据库 </li>
<li>表</li>
<li>列 </li>
<li>约束 </li>
<li>索引 </li>
<li>预编译</li>
<li>函数</li>
<li>存储过程 </li>
<li>触发器</li>
<li>事件…..</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql">创建数据库 <br><span class="hljs-keyword">create</span> <span class="hljs-keyword">database</span> 数据库名 charset 字符集;<br><br><span class="hljs-keyword">create</span> <span class="hljs-keyword">database</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> school charset utf8;<br><br>修改数据库 <br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">database</span> school charset gbk;<br><br>丢弃数据库. 数据库中的所有内容全部丢弃. 慎重!!!!<br><span class="hljs-keyword">drop</span> <span class="hljs-keyword">database</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">exists</span> school;<br><br>常用数据类型<br><span class="hljs-type">int</span> 			<span class="hljs-number">4</span>字节整数<br><span class="hljs-type">bigint</span>			<span class="hljs-number">8</span>字节整数<br><span class="hljs-type">char</span>(长度)		定长字符串<br><span class="hljs-type">varchar</span>(字符数) // 最多<span class="hljs-number">65535</span>字节<br><span class="hljs-type">double</span>			<span class="hljs-number">8</span>字节双精度浮点<br><span class="hljs-type">decimal</span>			定点数<br><span class="hljs-type">date</span> 			日期<br>datetime		日期时间<br>longtext 		长文本<br><br><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> teacher(<br>	id <span class="hljs-type">int</span> auto_increment,<br>	<span class="hljs-type">name</span> <span class="hljs-type">varchar</span>(<span class="hljs-number">20</span>),<br>	age <span class="hljs-type">int</span>,<br>	phone <span class="hljs-type">varchar</span>(<span class="hljs-number">20</span>),<br>	address <span class="hljs-type">varchar</span>(<span class="hljs-number">100</span>),<br>	gender enum(<span class="hljs-string">&#x27;男&#x27;</span>, <span class="hljs-string">&#x27;女&#x27;</span>) <span class="hljs-keyword">default</span> <span class="hljs-string">&#x27;男&#x27;</span>,<br>	<span class="hljs-keyword">primary key</span>(id)<br>) engine innodb charset gbk;<br><br><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> classes(<br>	id <span class="hljs-type">int</span> auto_increment,<br>	<span class="hljs-type">name</span> <span class="hljs-type">varchar</span>(<span class="hljs-number">30</span>),<br>	student_count <span class="hljs-type">int</span>,<br>	room <span class="hljs-type">char</span>(<span class="hljs-number">3</span>),<br>	master <span class="hljs-type">int</span>, <span class="hljs-comment">-- 班主任</span><br>	begindate <span class="hljs-type">date</span>,<br>	<span class="hljs-keyword">primary key</span>(id)<br>);<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>子查询题目例子</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><span class="hljs-number">5</span> 查询所有国家的首都和使用率最高的官方语言(选做)<br><span class="hljs-keyword">select</span> <br>	co.name,<br>	ci.name,<br>	cl3.<span class="hljs-keyword">language</span>,<br>	cl3.percentage<br><span class="hljs-keyword">from</span> <br>	country co <br><span class="hljs-keyword">left join</span> <br>	city ci <br><span class="hljs-keyword">on</span> <br>	co.capital = ci.id <br><span class="hljs-keyword">left join</span> <br>	(<span class="hljs-keyword">select</span> <br>		cl.countrycode,<br>		cl.<span class="hljs-keyword">language</span>,<br>		cl.percentage,<br>		cl.isofficial<br>	<span class="hljs-keyword">from</span> <br>		countrylanguage cl <br>	<span class="hljs-keyword">join</span>  <br>		(<span class="hljs-keyword">select</span> countrycode, max(Percentage) maxPer <span class="hljs-keyword">from</span> countrylanguage <span class="hljs-keyword">where</span> isofficial = <span class="hljs-string">&#x27;T&#x27;</span> <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> countrycode) cl2<br>	<span class="hljs-keyword">on</span> <br>			cl.countrycode = cl2.countrycode <br>		<span class="hljs-keyword">and</span> <br>			cl.percentage = cl2.maxPer<br>	<span class="hljs-keyword">where</span> <br>		cl.isofficial = <span class="hljs-string">&#x27;T&#x27;</span><br>	) cl3<br><span class="hljs-keyword">on</span> <br>	co.code = cl3.countrycode <br><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> <br>	cl3.percentage;<br></code></pre></td></tr></table></figure>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql">创建数据库 <br><span class="hljs-keyword">create</span> <span class="hljs-keyword">database</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> 数据库名 charset utf8;<br><br>修改数据库 只能修改字符集<br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">database</span> 数据库名 charset 新字符集;<br><br>丢弃数据库<br><span class="hljs-keyword">drop</span> <span class="hljs-keyword">database</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">exists</span> 数据加名;<br><br>查看库或表<br><span class="hljs-keyword">show</span> <span class="hljs-keyword">create</span> <span class="hljs-keyword">database</span>(<span class="hljs-keyword">table</span>) 数据库名或表名<br><br><span class="hljs-keyword">SQL</span>语言分类<br>	<span class="hljs-number">1</span>) DML 数据操纵语言, 主要处理数据<br>		<span class="hljs-keyword">insert</span> <span class="hljs-keyword">select</span> <span class="hljs-keyword">update</span> <span class="hljs-keyword">delete</span> <br>		<br>	<span class="hljs-number">2</span>) DDL 数据定义语言, 主要处理数据库对象<br>		<span class="hljs-keyword">create</span> <span class="hljs-keyword">show</span> <span class="hljs-keyword">alter</span> <span class="hljs-keyword">drop</span> <br>		<br>	<span class="hljs-number">3</span>) DCL 数据控制语句, 主要用于控制事务<br>		<span class="hljs-keyword">commit</span> <span class="hljs-keyword">rollback</span><br><br></code></pre></td></tr></table></figure>
<h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><blockquote>
<ul>
<li>全新方式创建表</li>
</ul>
</blockquote>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-number">1</span>) 全新方式建表<br><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> if <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> 表名(<br>	列<span class="hljs-number">1</span> 数据类型<span class="hljs-number">1</span>(长度) 其他选项,<br>	列<span class="hljs-number">2</span> 数据类型<span class="hljs-number">2</span>(长度) 其他选项,<br>	......,<br>	<span class="hljs-keyword">primary</span> key(列) <span class="hljs-comment">-- 表级主键</span><br>) engine 数据库引擎 charset 字符集;<br><br>数据库引擎 : <br>	InnoDB : 缺省引擎, 支持事务, 外键等高级特性, 速度慢<br>	MyIsam : 速度快, 早期的缺省引擎, 不支持事务,外键等高级特性<br><br>其他选项 : auto_increment, <span class="hljs-keyword">default</span> 缺省值, <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span>, unique.<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>基于子查询, 可以复制数据，不能复制各种约束（key）…</li>
</ul>
</blockquote>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> if <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> 表名 <br>子查询 <br><br><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> country3 <span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> world.country <span class="hljs-keyword">where</span> continent <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;asia&#x27;</span>;<br><br><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> country4 <span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> world.country;<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>完全复制表结构，不能复制数据</li>
</ul>
</blockquote>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> if <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> 表名 <span class="hljs-keyword">like</span> 已有表名<br></code></pre></td></tr></table></figure>
<h3 id="修改表结构"><a href="#修改表结构" class="headerlink" title="修改表结构"></a>修改表结构</h3><figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql">表结构的修改<br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> 表名 <br><span class="hljs-comment">--子句</span><br><br>添加新列<br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> 表名 <br><span class="hljs-keyword">add</span> 新列名 数据类型 其他选项;<br><br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> teacher <br><span class="hljs-keyword">add</span> gender enum(<span class="hljs-string">&#x27;男&#x27;</span>, <span class="hljs-string">&#x27;女&#x27;</span>) <span class="hljs-keyword">default</span> <span class="hljs-string">&#x27;男&#x27;</span>;<br><br>修改列<br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> 表名 <br>modify 列名 新数据类型 新其他选项;<br><br>修改列名<br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> 表名 <br>change 老列名 新列名 新数据类型 新其他选项;<br>丢弃一个列, 此列对应的所有数据都会删除<br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> 表名<br><span class="hljs-keyword">drop</span> <span class="hljs-keyword">column</span> 列名 <br><br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> teacher<br><span class="hljs-keyword">drop</span> <span class="hljs-keyword">column</span> address;<br>丢弃表 <br><span class="hljs-keyword">drop</span> <span class="hljs-keyword">table</span> if <span class="hljs-keyword">exists</span> 表名<span class="hljs-number">1</span>, 表名<span class="hljs-number">2</span>, ....;<br><br>清空表数据<br><span class="hljs-keyword">truncate</span> <span class="hljs-keyword">table</span> 表名; <span class="hljs-comment">-- 它是一个DDL语句, 一旦清除,就不能回滚, 效率高.</span><br><br><span class="hljs-keyword">delete</span> <span class="hljs-keyword">from</span> 表名; <span class="hljs-comment">-- 它是一个DML语句, 意味着是可以回滚的. 效率低.</span><br><br>修改表名<br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> 表名<br>rename <span class="hljs-keyword">to</span> 新表名<br></code></pre></td></tr></table></figure>
<h3 id="插入数据的方式"><a href="#插入数据的方式" class="headerlink" title="插入数据的方式"></a>插入数据的方式</h3><ol>
<li>全新方式插入<figure class="highlight lasso"><table><tr><td class="code"><pre><code class="hljs lasso">insert <span class="hljs-keyword">into</span> 表名 (<br>	列<span class="hljs-number">1</span>,<br>	列<span class="hljs-number">2</span>,<br>	列<span class="hljs-number">3</span>,<br>	<span class="hljs-params">...</span>..<br>) values (<br>	值<span class="hljs-number">1</span>,<br>	值<span class="hljs-number">2</span>,<br>	值<span class="hljs-number">3</span>,<br>	<span class="hljs-params">...</span>.<br>)<br></code></pre></td></tr></table></figure></li>
<li>使用子查询插入<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> students(<br>	<span class="hljs-type">name</span>,<br>	age,<br>	mobile,<br>	gender,<br>	address<br>) <span class="hljs-keyword">select</span> <br>	<span class="hljs-type">name</span>,<br>	age, <br>	mobile,<br>	gender,<br>	<span class="hljs-string">&#x27;北京&#x27;</span><br><span class="hljs-keyword">from</span> <br>	teachers <br><span class="hljs-keyword">where</span> <br>	id <span class="hljs-keyword">in</span> (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>);<br><span class="hljs-comment">-- 克隆表</span><br><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> 新表 <span class="hljs-keyword">like</span> 旧表;<br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> 新表 <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> 旧表 <br>克隆城市表到当前库下新成中国城市表 (chinaCity)<br><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> chinaCity <span class="hljs-keyword">like</span> world.city; <br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> chinacity <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> world.city <span class="hljs-keyword">where</span> countrycode = <span class="hljs-string">&#x27;chn&#x27;</span>;<br></code></pre></td></tr></table></figure></li>
<li>插入一条数据 insert into 表名 set 数据<figure class="highlight n1ql"><table><tr><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <br>	teachers <br><span class="hljs-keyword">set</span> <br>	name = <span class="hljs-string">&#x27;丁老师&#x27;</span>,<br>	age = <span class="hljs-number">25</span>,<br>	mobile = <span class="hljs-string">&#x27;123234234&#x27;</span>;<br>修改数据 <br><span class="hljs-keyword">update</span> 表名 <span class="hljs-keyword">set</span> <br>	列<span class="hljs-number">1</span> = 值<span class="hljs-number">1</span>,<br>	列<span class="hljs-number">2</span> = 值<span class="hljs-number">2</span>,<br>	列<span class="hljs-number">3</span> = 值<span class="hljs-number">3</span>,<br>	....<br><span class="hljs-keyword">where</span> <br>	行过滤<br>	<br><span class="hljs-keyword">update</span> teachers <span class="hljs-keyword">set</span> <br>	age = <span class="hljs-number">40</span>,<br>	mobile = <span class="hljs-string">&#x27;135342342&#x27;</span><br><span class="hljs-keyword">where</span> <br>	id = <span class="hljs-number">1</span>;<br>	<br>删除数据 <br><span class="hljs-keyword">delete</span> <span class="hljs-keyword">from</span> 表名 <br><span class="hljs-keyword">where</span> 行过滤<br><br><span class="hljs-keyword">delete</span> <span class="hljs-keyword">from</span> teachers <br><span class="hljs-keyword">where</span> id &gt; <span class="hljs-number">3</span><br></code></pre></td></tr></table></figure></li>
</ol>
<h3 id="数据库事物"><a href="#数据库事物" class="headerlink" title="数据库事物"></a>数据库事物</h3><blockquote>
<ul>
<li>定义：让数据从一种状态到另一种状态</li>
<li>ACID特性</li>
<li>让一组逻辑操作单元当成一个单个的命令来执行.<blockquote>
<ul>
<li>A 原子性 : 不可分割 </li>
<li>C 一致性 : 数据前后是一致</li>
<li>I 独立性 : 事务间, (独立性有等级)</li>
<li>D 持久性 : 事务一旦提交, 数据持久化.</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql">设置提交状态：<span class="hljs-keyword">SET</span> AUTOCOMMIT = <span class="hljs-keyword">FALSE</span>;<br>或者显式的执行 <span class="hljs-keyword">start</span> <span class="hljs-keyword">transaction</span><br>            或 <span class="hljs-keyword">begin</span><br>			<br>以第一个 DML 语句的执行作为开始<br><br>以下面的其中之一作为结束:<br><span class="hljs-keyword">COMMIT</span> 或 <span class="hljs-keyword">ROLLBACK</span> 语句<br>DDL 语句（自动提交）<br>用户会话正常结束, 提交<br>系统异常终止      回滚<br><br><span class="hljs-keyword">SET</span> AUTOCOMMIT = <span class="hljs-keyword">true</span>;<br><br><span class="hljs-keyword">set</span> autocommit = <span class="hljs-keyword">false</span>;<br><br>用一个客户端在事务中删除表数据, 另一个客户端查询??<br><br>预编译 : 提前把<span class="hljs-keyword">SQL</span>编译成可执行的, 在执行时只需要调用它即可.<br><span class="hljs-keyword">prepare</span> 预编译名 <span class="hljs-keyword">from</span> <span class="hljs-string">&#x27;SQL&#x27;</span>; <br><br><span class="hljs-keyword">prepare</span> p1 <span class="hljs-keyword">from</span> <span class="hljs-string">&#x27;select * from teachers&#x27;</span><br><br>执行预编译 <br><span class="hljs-keyword">execute</span> p1; <br><br>丢弃预编译<br><span class="hljs-keyword">drop</span> <span class="hljs-keyword">prepare</span> p1;<br></code></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">prepare</span> p2 <br><span class="hljs-keyword">from</span> <br>	<span class="hljs-string">&#x27;insert into teachers(</span><br><span class="hljs-string">		name, </span><br><span class="hljs-string">		age, </span><br><span class="hljs-string">		mobile</span><br><span class="hljs-string">	) values (</span><br><span class="hljs-string">		?,</span><br><span class="hljs-string">		?,</span><br><span class="hljs-string">		?</span><br><span class="hljs-string">	)&#x27;</span>;<br>	<br><span class="hljs-keyword">prepare</span> p3 <span class="hljs-keyword">from</span> <br><span class="hljs-string">&#x27;delete from teachers </span><br><span class="hljs-string"> where id = ?&#x27;</span>;<br><br>在执行时预编译时, 代替?的实参必须要用用户变量<br><br><span class="hljs-keyword">set</span> @变量名 <span class="hljs-operator">=</span> 值, @变量名<span class="hljs-number">2</span> <span class="hljs-operator">=</span> 值<span class="hljs-number">2</span>;<br><br><span class="hljs-keyword">execute</span> 预编译 <span class="hljs-keyword">using</span> @变量名, @变量名<span class="hljs-number">2</span>;<br><br>? 只能代替值的部分, 表名, 列名绝不可以.<br>下面是错误<span class="hljs-operator">!</span><span class="hljs-operator">!</span><br><span class="hljs-keyword">prepare</span> p4 <span class="hljs-keyword">from</span> <br><span class="hljs-string">&#x27;insert into teachers(</span><br><span class="hljs-string">	?, </span><br><span class="hljs-string">	?, </span><br><span class="hljs-string">	?</span><br><span class="hljs-string">) values (</span><br><span class="hljs-string">	?,</span><br><span class="hljs-string">	?,</span><br><span class="hljs-string">	?</span><br><span class="hljs-string">)&#x27;</span>;<br><br>在<span class="hljs-keyword">SQL</span>中要想使用<span class="hljs-string">&#x27;, 必须再加一个&#x27;</span>, 起到转义的作用<br><span class="hljs-keyword">prepare</span> p2 <span class="hljs-keyword">from</span> <br><span class="hljs-string">&#x27;insert into teachers(</span><br><span class="hljs-string">	name, </span><br><span class="hljs-string">	age, </span><br><span class="hljs-string">	mobile,</span><br><span class="hljs-string">	gender</span><br><span class="hljs-string">) values (</span><br><span class="hljs-string">	?,</span><br><span class="hljs-string">	?,</span><br><span class="hljs-string">	?,</span><br><span class="hljs-string">	?</span><br><span class="hljs-string">)&#x27;</span>;<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>有以下六种约束:<blockquote>
<ul>
<li>NOT NULL 非空约束，规定某个字段不能为空, 必须列级约束 </li>
<li>UNIQUE  唯一约束，规定某个字段在整个表中是唯一的</li>
<li>PRIMARY KEY  主键(非空且唯一)</li>
<li>FOREIGN KEY  外键</li>
<li>DEFAULT  默认值, 必须是列级</li>
<li>check</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><br><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> test(<br>	id <span class="hljs-type">int</span> auto_increment,<br>	<span class="hljs-type">name</span> <span class="hljs-type">varchar</span>(<span class="hljs-number">20</span>),<br>	phone <span class="hljs-type">varchar</span>(<span class="hljs-number">20</span>) <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span>, <span class="hljs-comment">-- 必须列级约束</span><br>	<span class="hljs-keyword">unique</span>(<span class="hljs-type">name</span>), <span class="hljs-comment">-- 可以表级约束</span><br>	<span class="hljs-keyword">primary key</span>(id)<br>);<br><br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> test(<br>	<span class="hljs-type">name</span>,<br>	phone<br>) <span class="hljs-keyword">values</span> (<br>	<span class="hljs-string">&#x27;aaa&#x27;</span>,<br>	<span class="hljs-string">&#x27;234234&#x27;</span><br>);<br><br><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> test <br><span class="hljs-keyword">drop</span> key <span class="hljs-type">name</span>; <span class="hljs-comment">-- 丢弃唯一键约束</span><br><br><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> test2(<br>	id <span class="hljs-type">int</span> auto_increment,<br>	<span class="hljs-type">name</span> <span class="hljs-type">varchar</span>(<span class="hljs-number">20</span>),<br>	phone <span class="hljs-type">varchar</span>(<span class="hljs-number">20</span>) <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span>, <span class="hljs-comment">-- 必须列级约束</span><br>	<span class="hljs-keyword">constraint</span> myunique <span class="hljs-keyword">unique</span>(<span class="hljs-type">name</span>, phone), <span class="hljs-comment">-- 可以表级约束, 联合键</span><br>	<span class="hljs-keyword">primary key</span>(id)<br>);<br><br></code></pre></td></tr></table></figure>
<h3 id="外键"><a href="#外键" class="headerlink" title="外键"></a>外键</h3><blockquote>
<ul>
<li>外键 : 让一个表中的记录的值要引用到另一张表中的数据…</li>
<li>一旦有了外键, 子表中插入数据必须要引用到真实的父表中数据</li>
<li>一旦父表中的记录被子表引用, 当删除父表中的相关记录时, 不允许删除.</li>
</ul>
</blockquote>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">foreign key</span>(本表的外键列) <span class="hljs-keyword">references</span> 父表(父表主键)<br><br><span class="hljs-keyword">drop</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">exists</span> classes;<br><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> classes(<br>	id <span class="hljs-type">int</span> auto_increment,<br>	<span class="hljs-type">name</span> <span class="hljs-type">varchar</span>(<span class="hljs-number">30</span>),<br>	student_count <span class="hljs-type">int</span>,<br>	room <span class="hljs-type">char</span>(<span class="hljs-number">3</span>),<br>	master <span class="hljs-type">int</span>, <br>	begindate <span class="hljs-type">date</span>,<br>	<span class="hljs-keyword">primary key</span>(id),<br>	<span class="hljs-keyword">foreign key</span>(master) <span class="hljs-keyword">references</span> teachers(id)<br>);<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>丢弃外键, 必须要知道外键名.</li>
</ul>
</blockquote>
<figure class="highlight sas"><table><tr><td class="code"><pre><code class="hljs sas"><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> classes <br><span class="hljs-keyword">drop</span> <span class="hljs-keyword">foreign</span> <span class="hljs-keyword">key</span> classes_ibfk_1;<br></code></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>添加外键</li>
<li>on delete do nothing, 是默认选项, 在删除父表被引用的记录时不允许</li>
<li>on delete cascade 级联删除, 当删除父表中的相关记录时, 子表中引用此记录的所有记录也会被删除.</li>
<li>on delete set null 级联置空.当删除父表中的相关记录时, 子表中引用此记录的所有记录会被置为空.</li>
</ul>
</blockquote>
<h4 id="limit"><a href="#limit" class="headerlink" title="limit"></a>limit</h4><blockquote>
<ul>
<li>limit n, 把结果集截断成n条记录.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><p>limit m, n 把结果集中的m条略过, 再截断成n条记录</p>
</li>
<li><p>limit子句必须放在整个查询语句的最后！</p>
</li>
<li><p>offset n 表示去掉n个值</p>
</li>
<li><p>一般用法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> employees<br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> hire_date <span class="hljs-keyword">DESC</span>      <span class="hljs-comment">-- 倒序</span><br>LIMIT <span class="hljs-number">1</span> <span class="hljs-keyword">offset</span> <span class="hljs-number">2</span>;       <span class="hljs-comment">-- 去掉排名倒数第一第二的时间，取倒数第三;   </span><br>        <br></code></pre></td></tr></table></figure></li>
</ul>
</blockquote>
<h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><blockquote>
<ul>
<li>重点：DML数据操纵语言，主要就是查询</li>
<li>标准SQL<blockquote>
<ul>
<li>select </li>
<li>from </li>
<li>left join </li>
<li>on </li>
<li>where </li>
<li>group by </li>
<li>having </li>
<li>order by</li>
</ul>
</blockquote>
</li>
<li>注意点<blockquote>
<ul>
<li>注意多表连接的内联，外联 </li>
<li>函数的使用，where中不能使用</li>
<li>别名的使用，从group by开始可以使用select中的别名了</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h2 id="SQL优化"><a href="#SQL优化" class="headerlink" title="SQL优化"></a>SQL优化</h2><ul>
<li><p>先找到运行慢的SQL</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-comment">-- 查看慢SQL日志是否可用</span><br><span class="hljs-keyword">show</span> variables <span class="hljs-keyword">like</span> <span class="hljs-string">&#x27;log_slow_queries&#x27;</span>;<br><br><span class="hljs-comment">-- 查看执行慢于多少秒的SQL会记录到日志文件中</span><br><span class="hljs-keyword">show</span> variables <span class="hljs-keyword">like</span> <span class="hljs-string">&#x27;long_query_time&#x27;</span>;<br><br><span class="hljs-comment">-- 查看慢SQL存在的位置</span><br><span class="hljs-keyword">show</span> variables <span class="hljs-keyword">like</span> <span class="hljs-string">&#x27;slow_query_log_file&#x27;</span>;<br><br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="字段"><a href="#字段" class="headerlink" title="字段"></a>字段</h3><ul>
<li>尽量使用 <code>TINYINT</code> 、 <code>SMALLINT</code> 、 <code>MEDIUM_INT</code> 作为整数类型而非 <code>INT</code> ，如果非负则加上 <code>UNSIGNED</code></li>
<li>varchar的长度只分配真正需要的空间</li>
<li>使用枚举或整数代替字符串类型</li>
<li>尽量使用TIMESTAMP而非DATETIME</li>
<li>单表不要有太多字段，建议在20以内</li>
<li>避免使用NULL字段，很难查询优化且占用额外索引空间</li>
<li>用整型来存IP</li>
</ul>
<h3 id="索引-1"><a href="#索引-1" class="headerlink" title="索引"></a>索引</h3><ul>
<li>索引并不是越多越好，要根据查询有针对性地创建，<font color=red>考虑在where和order by命令上涉及地列建立索引</font>，根据explain来查看是否用了索引还是全表扫描。</li>
<li>应尽量避免在where子句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫描。</li>
<li>值分布很稀少地字段不适合建索引</li>
<li>字符串字段只建前缀索引</li>
<li>字符字段最好不要做主键</li>
<li>不用外键，由程序保证约束</li>
<li>尽量不用 <code>UNIQUE</code> ，由程序保证约束</li>
<li>使用多列索引时主意顺序和查询条件保持一致，同时删除不必要的单列索引</li>
</ul>
<h3 id="查询SQL"><a href="#查询SQL" class="headerlink" title="查询SQL"></a>查询SQL</h3><ul>
<li>可通过开启慢查询日志来找出较慢的SQL</li>
<li>不做列运算</li>
<li>sql语句尽可能简单，一条SQL只能在一个cpu运算，大语句拆小语句，减少锁时间；一条大SQL可以堵死整个库。</li>
<li>不用select *，要尽量避免使用 select *，而是查询需要的字段，这样可以提升速度，以及减少网络传输的带宽压力</li>
<li><font color=red>or改写成in</font>,or的效率是n级别，in的效率是log(n)级别，in的个数建议在200以内</li>
<li>不用函数和触发器，在应用程序实现</li>
<li>少用join</li>
<li>使用同类型进行比较</li>
<li><font color=red>尽量避免在where子句中使用!&#x3D;或&lt;&gt;操作符</font>，否则引擎将放弃使用索引而进行全表扫描。</li>
<li>对于连续型数值，使用BETWEEN 不用 in</li>
<li>列表数据不要拿全表，要使用limit来分页，每页数量也不要太大</li>
<li>小表驱动大表</li>
</ul>
<h3 id="系统参数调优"><a href="#系统参数调优" class="headerlink" title="系统参数调优"></a>系统参数调优</h3><ul>
<li>wait_timeout：数据库连接闲置时间，闲置连接会占用内存资源。可以从默认的8小时减到半小时</li>
<li>max_user_connection: 最大连接数，默认为0无上限，最好设一个合理上限</li>
<li>back_log：back_log值指出在MySQL暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中。也就是说，如果MySql的连接数据达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源。可以从默认的50升至500</li>
<li>thread_concurrency：并发线程数，设为CPU核数的两倍</li>
</ul>
<h2 id="存储过程"><a href="#存储过程" class="headerlink" title="存储过程"></a>存储过程</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ul>
<li>如果在实现用户的某些需求时，需要编写一组复杂的SQL语句才能实现的时候，那么我们就可以将这组复杂的SQL语句集提前编写在数据库中，由JDBC调用来执行这组SQL语句。把编写在数据库中的SQL语句集称为存储过程。</li>
<li>存储过程：(procedure) 是事先经过编译并存储在数据库中的一段SQL语句的集合。调用存储过程可以简化应用开发人员的很多工作，减少数据在数据库和应用服务器之间的传输，对于提高数据处理的效率是很有好处的。<font color=red><strong>存储过程就是数据库SQL语言层面的代码封装与重用</strong></font></li>
<li>存储过程就类似于Java中的方法，需要先定义，使用时需要调用。存储过程可以定义参数，参数分为IN，OUT，INOUT三种类型<ul>
<li>IN类型的参数表示接受调用者传入的数据</li>
<li>OUT类型的参数表示向调用者返回数据</li>
<li>INOUT类型的参数既可以接受调用者传入的参数，也可以向调用者返回数据</li>
</ul>
</li>
</ul>
<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><ul>
<li><p>优点</p>
<ul>
<li>简化复杂操作，简化对变动的管理</li>
<li><strong>通常存储过程有助于提高应用程序的性能</strong>。当创建的存储过程被编译之后，就存储在数据库中。</li>
<li>存储过程有助于减少应用程序和数据库服务器之间的流量。</li>
<li>可重用，透明，安全的。</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li><p>如果大量使用存储过程，那么使用这些存储过程的每个连接的内存使用量将大大增加。此外，如果在存储过程中过度使用大量的逻辑操作，那么CPU的使用率也在增加，因为MySQL数据库最初的设计就侧重于高效的查询，而不是逻辑运算。</p>
</li>
<li><p>开发维护，调试代码困难，<strong>对数据库依赖程度高，移植性较差</strong>。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql">DELIMITER $$<br><br><span class="hljs-keyword">CREATE</span><br>    <span class="hljs-comment">/*[DEFINER = &#123; user | CURRENT_USER &#125;]*/</span><br>    <span class="hljs-keyword">PROCEDURE</span> 数据库名.存储过程名([<span class="hljs-keyword">in</span>变量名 类型,<span class="hljs-keyword">out</span> 参数 <span class="hljs-number">2</span>，...])<br>    <span class="hljs-comment">/*LANGUAGE SQL</span><br><span class="hljs-comment">    | [NOT] DETERMINISTIC</span><br><span class="hljs-comment">    | &#123; CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA &#125;</span><br><span class="hljs-comment">    | SQL SECURITY &#123; DEFINER | INVOKER &#125;</span><br><span class="hljs-comment">    | COMMENT &#x27;string&#x27;*/</span><br>	<span class="hljs-keyword">BEGIN</span><br>		[<span class="hljs-keyword">DECLARE</span> 变量名 类型 [<span class="hljs-keyword">DEFAULT</span> 值];]<br>		存储过程的语句块;<br>	<span class="hljs-keyword">END</span>$$<br><br>DELIMITER ;<br><br></code></pre></td></tr></table></figure>
</li>
<li><p>in代表输入参数（默认情况下为in参数），表示该参数的值必须由调用程序指定。</p>
</li>
<li><p>out代表输出参数，表示该参数的值经过存储过程计算后，将out参数的计算结果返回给调用程序</p>
</li>
<li><p>inout代表即时输入参数，又是输出参数，表示该参数的值既可以由调用程序制定，又可以将inout参数的计算结果返回给调用程序。</p>
</li>
</ul>
</li>
<li><p>存储过程中的语句必须包含在BEGIN和END之间</p>
</li>
<li><p>DELIMITER是分割符，默认情况下DELIMITER是分号，遇到分号就执行。</p>
</li>
<li><p>DECLARE中用来声明变量，变量默认赋值使用DEFAULT，语句块中改变变量值，使用SET变量&#x3D;值</p>
</li>
<li><p>例子</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql">DELIMITER $$<br><br><span class="hljs-keyword">CREATE</span><br>    <span class="hljs-keyword">PROCEDURE</span> `demo`.`demo1`()<br>	<span class="hljs-comment">-- 存储过程体</span><br>	<span class="hljs-keyword">BEGIN</span><br>		<span class="hljs-comment">-- DECLARE声明 用来声明变量的</span><br>		<span class="hljs-keyword">DECLARE</span> de_name <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">10</span>) <span class="hljs-keyword">DEFAULT</span> <span class="hljs-string">&#x27;&#x27;</span>;<br>		<br>		<span class="hljs-keyword">SET</span> de_name <span class="hljs-operator">=</span> &quot;jim&quot;;<br>		<br>		<span class="hljs-comment">-- 测试输出语句（不同的数据库，测试语句都不太一样。</span><br>		<span class="hljs-keyword">SELECT</span> de_name;<br>	<span class="hljs-keyword">END</span>$$<br><br>DELIMITER ;<br><span class="hljs-comment">-- 调用</span><br><span class="hljs-keyword">CALL</span> demo1();<br></code></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql">DELIMITER $$<br><br><span class="hljs-keyword">CREATE</span><br>    <span class="hljs-keyword">PROCEDURE</span> `demo`.`demo2`(<span class="hljs-keyword">IN</span> s_sex <span class="hljs-type">CHAR</span>(<span class="hljs-number">1</span>),<span class="hljs-keyword">OUT</span> s_count <span class="hljs-type">INT</span>)<br>	<span class="hljs-comment">-- 存储过程体</span><br>	<span class="hljs-keyword">BEGIN</span><br>		<span class="hljs-comment">-- 把SQL中查询的结果通过INTO赋给变量</span><br>		<span class="hljs-keyword">SELECT</span> <span class="hljs-built_in">COUNT</span>(<span class="hljs-operator">*</span>) <span class="hljs-keyword">INTO</span> s_count <span class="hljs-keyword">FROM</span> student <span class="hljs-keyword">WHERE</span> sex<span class="hljs-operator">=</span> s_sex;<br>		<span class="hljs-keyword">SELECT</span> s_count;<br>		<br>	<span class="hljs-keyword">END</span>$$<br>DELIMITER ;<br><span class="hljs-comment">-- 调用</span><br><span class="hljs-comment">-- @s_count表示测试出输出的参数</span><br><span class="hljs-keyword">CALL</span> demo2 (<span class="hljs-string">&#x27;男&#x27;</span>,<span class="hljs-variable">@s_count</span>);<br><br></code></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-comment">-- if判断</span><br>DELIMITER $$<br><span class="hljs-keyword">CREATE</span><br>    <span class="hljs-keyword">PROCEDURE</span> `demo`.`demo3`(<span class="hljs-keyword">IN</span> `<span class="hljs-keyword">day</span>` <span class="hljs-type">INT</span>)<br>	<span class="hljs-comment">-- 存储过程体</span><br>	<span class="hljs-keyword">BEGIN</span><br>		IF `<span class="hljs-keyword">day</span>` <span class="hljs-operator">=</span> <span class="hljs-number">0</span> <span class="hljs-keyword">THEN</span><br>		<span class="hljs-keyword">SELECT</span> <span class="hljs-string">&#x27;星期天&#x27;</span>;<br>		ELSEIF `<span class="hljs-keyword">day</span>` <span class="hljs-operator">=</span> <span class="hljs-number">1</span> <span class="hljs-keyword">THEN</span><br>		<span class="hljs-keyword">SELECT</span> <span class="hljs-string">&#x27;星期一&#x27;</span>;<br>		ELSEIF `<span class="hljs-keyword">day</span>` <span class="hljs-operator">=</span> <span class="hljs-number">2</span> <span class="hljs-keyword">THEN</span><br>		<span class="hljs-keyword">SELECT</span> <span class="hljs-string">&#x27;星期二&#x27;</span>;<br>		<span class="hljs-keyword">ELSE</span><br>		<span class="hljs-keyword">SELECT</span> <span class="hljs-string">&#x27;无效日期&#x27;</span>;<br>		<span class="hljs-keyword">END</span> IF;<br>		<br>	<span class="hljs-keyword">END</span>$$<br>DELIMITER ;<br><span class="hljs-comment">-- case when判断</span><br>DELIMITER $$<br><span class="hljs-keyword">CREATE</span> <br>    <span class="hljs-keyword">PROCEDURE</span> demo5(<span class="hljs-keyword">IN</span> num <span class="hljs-type">INT</span>)<br>	<span class="hljs-keyword">BEGIN</span><br>		<span class="hljs-keyword">CASE</span> num  <span class="hljs-comment">-- 条件开始</span><br>		<span class="hljs-keyword">WHEN</span> <span class="hljs-number">1</span> <span class="hljs-keyword">THEN</span> <br>			<span class="hljs-keyword">SELECT</span> <span class="hljs-string">&#x27;输入为1&#x27;</span>;<br>		<span class="hljs-keyword">WHEN</span> <span class="hljs-number">0</span> <span class="hljs-keyword">THEN</span> <br>			<span class="hljs-keyword">SELECT</span> <span class="hljs-string">&#x27;输入为0&#x27;</span>;<br>		<span class="hljs-keyword">ELSE</span> <br>		<span class="hljs-keyword">SELECT</span> <span class="hljs-string">&#x27;不是1也不是0&#x27;</span>;<br>		<span class="hljs-keyword">END</span> <span class="hljs-keyword">CASE</span>; <span class="hljs-comment">-- 条件结束</span><br>	<span class="hljs-keyword">END</span>$$<br>DELIMITER;<br><br></code></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql"><span class="hljs-comment">-- while循环</span><br>DELIMITER $$<br><span class="hljs-keyword">CREATE</span> <br>    <span class="hljs-keyword">PROCEDURE</span> demo6(<span class="hljs-keyword">IN</span> num <span class="hljs-type">INT</span>,<span class="hljs-keyword">OUT</span> SUM <span class="hljs-type">INT</span>)<br>	<span class="hljs-keyword">BEGIN</span><br>	     <span class="hljs-keyword">SET</span> SUM <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br>	     WHILE num<span class="hljs-operator">&lt;</span><span class="hljs-number">10</span> DO <span class="hljs-comment">-- 循环开始</span><br>	         <span class="hljs-keyword">SET</span> num <span class="hljs-operator">=</span> num<span class="hljs-operator">+</span><span class="hljs-number">1</span>;<br>	         <span class="hljs-keyword">SET</span> SUM <span class="hljs-operator">=</span> SUM<span class="hljs-operator">+</span>num;<br>	         <span class="hljs-keyword">END</span> WHILE; <span class="hljs-comment">-- 循环结束</span><br>	<span class="hljs-keyword">END</span>$$<br>DELIMITER;<br><br><span class="hljs-comment">-- REPEAT...UNTIL语句的语法和Java中的do...while语句类似，都是先执行循环操作，再判断条件，区别就是REPEAT表达式值为false时才执行循环操作，直到表达式值为true停止</span><br><span class="hljs-comment">-- 创建过程</span><br>DELIMITER $$<br><span class="hljs-keyword">CREATE</span> <br>    <span class="hljs-keyword">PROCEDURE</span> demo7(<span class="hljs-keyword">IN</span> num <span class="hljs-type">INT</span>,<span class="hljs-keyword">OUT</span> SUM <span class="hljs-type">INT</span>)<br>	<span class="hljs-keyword">BEGIN</span><br>	     <span class="hljs-keyword">SET</span> SUM <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br>	     REPEAT<span class="hljs-comment">-- 循环开始</span><br>		<span class="hljs-keyword">SET</span> num <span class="hljs-operator">=</span> num<span class="hljs-operator">+</span><span class="hljs-number">1</span>;<br>		<span class="hljs-keyword">SET</span> SUM <span class="hljs-operator">=</span> SUM<span class="hljs-operator">+</span>num ;<br>		UNTIL num<span class="hljs-operator">&gt;=</span><span class="hljs-number">10</span><br>		<span class="hljs-keyword">END</span> REPEAT; <span class="hljs-comment">-- 循环结束</span><br>	<span class="hljs-keyword">END</span>$$<br>DELIMITER;<br><br><span class="hljs-comment">-- LOOP循环语句，用来重复执行某些语句</span><br><span class="hljs-comment">-- 执行过程中可使用LEAVE语句或者ITERATE来跳出循环，也可以嵌套IF等判断语句</span><br><span class="hljs-comment">-- LEAVE语句效果对于Java中的break,用来终止循环</span><br><span class="hljs-comment">-- ITERATE语句效果相当于Java中的Continue，用来跳过此次循环。进入下一次循环，且ITERATE之下的语句将不在进行。</span><br>DELIMITER $$<br><span class="hljs-keyword">CREATE</span> <br>    <span class="hljs-keyword">PROCEDURE</span> demo8(<span class="hljs-keyword">IN</span> num <span class="hljs-type">INT</span>,<span class="hljs-keyword">OUT</span> SUM <span class="hljs-type">INT</span>)<br>	<span class="hljs-keyword">BEGIN</span><br>	     <span class="hljs-keyword">SET</span> SUM <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;<br>	     demo_sum:LOOP<span class="hljs-comment">-- 循环开始</span><br>		<span class="hljs-keyword">SET</span> num <span class="hljs-operator">=</span> num<span class="hljs-operator">+</span><span class="hljs-number">1</span>;<br>		IF num <span class="hljs-operator">&gt;</span> <span class="hljs-number">10</span> <span class="hljs-keyword">THEN</span><br>		    LEAVE demo_sum; <span class="hljs-comment">-- 结束此次循环</span><br>		ELSEIF num <span class="hljs-operator">&lt;</span> <span class="hljs-number">9</span> <span class="hljs-keyword">THEN</span><br>		    ITERATE demo_sum; <span class="hljs-comment">-- 跳过此次循环</span><br>		<span class="hljs-keyword">END</span> IF;<br>		<br>		<span class="hljs-keyword">SET</span> SUM <span class="hljs-operator">=</span> SUM<span class="hljs-operator">+</span>num;<br>		<span class="hljs-keyword">END</span> LOOP demo_sum; <span class="hljs-comment">-- 循环结束</span><br>	<span class="hljs-keyword">END</span>$$<br>DELIMITER;<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>使用存储过程插入信息</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><code class="hljs sql">DELIMITER $$<br><span class="hljs-keyword">CREATE</span> <br>    <span class="hljs-keyword">PROCEDURE</span> demo9(<span class="hljs-keyword">IN</span> s_student <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">10</span>),<span class="hljs-keyword">IN</span> s_sex <span class="hljs-type">CHAR</span>(<span class="hljs-number">1</span>),<span class="hljs-keyword">OUT</span> s_result <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">20</span>))<br>	<span class="hljs-keyword">BEGIN</span><br>	   <span class="hljs-comment">-- 声明一个变量 用来决定这个名字是否已经存在</span><br>	   <span class="hljs-keyword">DECLARE</span> s_count <span class="hljs-type">INT</span> <span class="hljs-keyword">DEFAULT</span> <span class="hljs-number">0</span>;<br>	   <span class="hljs-comment">-- 验证这么名字是否已经存在</span><br>	   <span class="hljs-keyword">SELECT</span> <span class="hljs-built_in">COUNT</span>(<span class="hljs-operator">*</span>) <span class="hljs-keyword">INTO</span> s_count <span class="hljs-keyword">FROM</span> student <span class="hljs-keyword">WHERE</span> `name` <span class="hljs-operator">=</span> s_student;	<br>	   IF s_count <span class="hljs-operator">=</span> <span class="hljs-number">0</span> <span class="hljs-keyword">THEN</span><br>	        <span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> student (`name`, sex) <span class="hljs-keyword">VALUES</span>(s_student, s_sex);<br>		<span class="hljs-keyword">SET</span> s_result <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;数据添加成功&#x27;</span>;<br>	   <span class="hljs-keyword">ELSE</span><br>                <span class="hljs-keyword">SET</span> s_result <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;名字已存在，不能添加&#x27;</span>;<br>                <span class="hljs-keyword">SELECT</span> s_result;<br>	   <span class="hljs-keyword">END</span> IF;<br>	<span class="hljs-keyword">END</span>$$<br>DELIMITER;<br><span class="hljs-comment">-- 调用</span><br><span class="hljs-keyword">CALL</span> demo9(&quot;Jim&quot;,&quot;女&quot;,<span class="hljs-variable">@s_result</span>);<br><br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><pre><code class="sql">SHOW PROCEDURE STATUS -- 显示存储过程
SHOW PROCEDURE STATUS WHERE db = &#39;db名字&#39; AND NAME = &#39;name名字&#39;;-- 显示特定数据库的存储过程
SHOW PROCEDURE STATUS WHERE NAME LIKE &#39;%mo%&#39;;-- 显示特定模式的存储过程
SHOW CREATE PROCEDURE 存储过程名;-- 显示存储过程源码
DROP PROCEDURE 存储过程名;-- 删除存储过程
<figure class="highlight nsis"><table><tr><td class="code"><pre><code class="hljs nsis"><br><span class="hljs-comment">## MySQL函数</span><br><br><span class="hljs-comment">### 函数定义</span><br><br>* MySQL的函数定义语法如下：<br><br>  <span class="hljs-string">``</span><span class="hljs-string">``</span>sql<br>      CREATE  <br>          [DEFINER = &#123; <span class="hljs-literal">user</span> | CURRENT_USER &#125;]<br>          <span class="hljs-keyword">FUNCTION</span> <span class="hljs-title function_">functionName</span> ( varName varType [, ... ] )<br>          RETURNS returnVarType<br>          [characteristic ...] <br>          routine_body<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>参数含义：</p>
<ul>
<li>functionName：函数名，同MySQL内置函数一样，大小写不敏感</li>
<li>varName：形参类型，其与varName配对使用。形参数量不限</li>
<li>returnVarType：返回值类型。函数必须有且只能有一个返回值</li>
<li>characteristic：函数特性</li>
<li>routine_body：函数体。函数体中必须含有return 语句，当函数体为复合结构时，需要使用begin….end语句</li>
</ul>
</li>
<li><pre><code class="sql">    create
        function myfun_getMax(num1 int, num2 int)
        returns int        
    begin
        declare res int;
        if(num1 &gt; num2) then
            set res = num1;
        elseif (num1 &lt; num2) then
            set res = num2;
        else
            set res = num1;
        end if;
        return res;
    end;
<figure class="highlight less"><table><tr><td class="code"><pre><code class="hljs less"><br>### 表格值函数<br><br>* 内联表格值函数:返回一个表格<br><br>  ````<span class="hljs-selector-tag">sql</span><br>  <span class="hljs-selector-tag">create</span> <span class="hljs-selector-tag">function</span> <span class="hljs-selector-tag">tabcmess</span>(<span class="hljs-variable">@title</span> <span class="hljs-built_in">VARCHAR</span>(<span class="hljs-number">10</span>)) <span class="hljs-selector-tag">RETURNS</span> <span class="hljs-selector-tag">TABLE</span> <span class="hljs-selector-tag">as</span> <br>  <span class="hljs-selector-tag">return</span> (select title,des from product where title like <span class="hljs-string">&#x27;%&#x27;</span>+<span class="hljs-variable">@title</span>+<span class="hljs-string">&#x27;%&#x27;</span>)<br></code></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>多句表格值函数</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><code class="hljs pgsql">  ````<br><br>  <br><br><br><br>## 常见问题<br><br>* [<span class="hljs-number">42000</span>][<span class="hljs-number">1118</span>] <span class="hljs-keyword">Row</span> size too <span class="hljs-keyword">large</span>. The maximum <span class="hljs-keyword">row</span> size <span class="hljs-keyword">for</span> the used <span class="hljs-keyword">table</span> <span class="hljs-keyword">type</span>, <span class="hljs-keyword">not</span> counting BLOBs, <span class="hljs-keyword">is</span> <span class="hljs-number">65535.</span> This includes <span class="hljs-keyword">storage</span> overhead, <span class="hljs-keyword">check</span> the manual. You have <span class="hljs-keyword">to</span> change <span class="hljs-keyword">some</span> <span class="hljs-keyword">columns</span> <span class="hljs-keyword">to</span> <span class="hljs-type">TEXT</span> <span class="hljs-keyword">or</span> BLOBs<br>  - 行大小太大。所使用的表类型(不包括blob)的最大行大小是<span class="hljs-number">65535</span>。这包括存储开销，请参阅手册。您必须将一些列更改为<span class="hljs-type">TEXT</span>或blob<br>* 在MYSQL数据库中**一条记录**的**最大长度是<span class="hljs-number">65535</span>字节**，以下以Innodb和mysiam存储引擎为例，做了相关演示，不管任何存储引擎，都不能超过这个范围，即存储引擎支持一行存储更长的数据。<br>* Inodb存储引擎，对于<span class="hljs-number">4</span>K,<span class="hljs-number">8</span>K,<span class="hljs-number">16</span>K和<span class="hljs-number">32</span>K的页面大小，限制一条记录最多使用半个页面，，<span class="hljs-number">64</span>K页面比<span class="hljs-number">16</span>KB页面限制稍小一些。默认情况下，Innodb页面大小是<span class="hljs-number">16</span>KB<br>* mysql 字符所占字节数<br>  - utf<span class="hljs-number">-8</span>:中文占用<span class="hljs-number">3</span>个字节的存储，英文一个字节，中文三个字节。**UTF<span class="hljs-number">-8</span>则包含全世界所有国家需要用到的字符。**<br>  - utf8mb4最多支撑四个字节的存储，如emoji表情。中文三个字节。**utf8mb4专门用来兼容四字节的unicode。utf8mb4是utf8的超集，除了将编码改为utf8mb4外不需要做其他转换。**<br>  - GBK:中英文都是双字节。**GBK包含全部中文字符**<br>  - ASCII编码：一个英文字母（不分大小写）占用一个字节的空间，一个中文汉字占用两个字节的空间。<br>* 结论<br>  - **一条记录最大长度<span class="hljs-number">65535</span>字节**是MySQL数据库<span class="hljs-keyword">Server</span>层面的限制<br><br><br><br>* utf8mb4编码集中：<br>  - **utf8mb4_general_ci     不区分大小写** ci是<span class="hljs-keyword">case</span> insensitive的意思<br>  - utf8mb4_general_cs     区分大小写<br>  - utf8mb4_bin:字符串每个字符串用二进制数据编译存储。 区分大小写，而且可以存二进制的内容<br>*  utf8mb4_ unicode_ ci 与 utf8mb4_ general_ ci 如何选择<br>  字符除了需要存储，还需要排序或比较大小，涉及到与编码字符集对应的 排序字符集（<span class="hljs-keyword">collation</span>）。ut8mb4对应的排序字符集常用的有 utf8mb4_unicode_ci 、 utf8mb4_general_ci ，到底采用哪个在 stackoverflow 上有个讨论， What’s the difference <span class="hljs-keyword">between</span> utf8_general_ci <span class="hljs-keyword">and</span> utf8_unicode_ci<br>  **主要从排序准确性和性能两方面看**：<br>    **准确性**<br>    utf8mb4_unicode_ci 是基于标准的Unicode来排序和比较，能够在各种语言之间精确排序<br>    utf8mb4_general_ci 没有实现Unicode排序规则，在遇到某些特殊语言或字符是，排序结果可能不是所期望的。<br>    但是在绝大多数情况下，这种特殊字符的顺序一定要那么精确吗。比如Unicode把 ? 、 ? 当成 ss 和 OE 来看；而general会把它们当成 s 、 e ，再如 àá??ā? 各自都与 A 相等。<br>    **性能**<br>    utf8mb4_general_ci 在比较和排序的时候更快<br>    utf8mb4_unicode_ci 在特殊情况下，Unicode排序规则为了能够处理特殊字符的情况，实现了略微复杂的排序算法。<br>    但是在绝大多数情况下，不会发生此类复杂比较。general理论上比Unicode可能快些，但相比现在的CPU来说，它远远不足以成为考虑性能的因素，索引涉及、<span class="hljs-keyword">SQL</span>设计才是。 我个人推荐是 utf8mb4_unicode_ci ，将来 <span class="hljs-number">8.0</span> 里也极有可能使用变为默认的规则。<br><br><br><br>* max_allowed_packet设置问题<br>  - 报错信息&lt;font color=red&gt;packet <span class="hljs-keyword">for</span> query <span class="hljs-keyword">is</span> too <span class="hljs-keyword">large</span> (<span class="hljs-number">5</span>,<span class="hljs-number">352</span>,<span class="hljs-number">304</span> &gt; <span class="hljs-number">4</span>,<span class="hljs-number">194</span>,<span class="hljs-number">304</span>)&lt;/font&gt;<br>  - 原因分析：**mysql根据配置文件会限制<span class="hljs-keyword">server</span>接受的数据包大小。**有时候大的插入和更新会受max_allowed_packet参数限制，导致写入或者更新失败，导致项目访问异常。<br>  - 解决问题<br>    - <span class="hljs-keyword">show</span> VARIABLES <span class="hljs-keyword">like</span> <span class="hljs-string">&#x27;%max_allowed_packet%&#x27;</span>;<br>      - 查看max_allowed_packet最大允许包<br>    - 方案<span class="hljs-number">1</span>：临时修改，**<span class="hljs-keyword">set</span> <span class="hljs-keyword">global</span> max_allowed_packet = <span class="hljs-number">2</span>*<span class="hljs-number">1024</span>*<span class="hljs-number">1024</span>*<span class="hljs-number">10</span>; **（注意，这里的大小只能填写字节。重启mysql服务后，配置将会失效！）然后关闭掉这此mysql <span class="hljs-keyword">server</span>链接，再进入查看。<br>    - 方案<span class="hljs-number">2</span>:修改my.ini文件在[mysqld]部分加入 max_allowed_packet=大小，这里的大小可以写M<br>      - mysql <span class="hljs-comment">--help|grep my.cnf</span><br>    - 方案<span class="hljs-number">3</span>：<br>    - **官网给出的解决办法是加启动参数，找到mysql的启动脚本，把启动参数贴上**<br>  <br>* mysql <span class="hljs-type">TIMESTAMP</span>类型自动设置默认值的问题<br><br>  - 在实际生成中发现，为<span class="hljs-keyword">null</span>的数据同步到mysql后变成了当前的时间，建表语句却并未指定默认值。<br><br>  - 复现<br><br>  - ````mysql<br>    <span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> dim.dim_test2<br>    (<br>        `id`              <span class="hljs-type">BIGINT</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;主键&#x27;</span>,<br>        `enterprise_name` <span class="hljs-type">TEXT</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;算法识别涉及企业，逗号拼接&#x27;</span>,<br>        `<span class="hljs-keyword">type</span>`            <span class="hljs-type">TEXT</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;算法识别资讯类型（科技资讯，其他）&#x27;</span>,<br>        `source`          <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">255</span>) <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;来源&#x27;</span>,<br>        `article_content` <span class="hljs-type">TEXT</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;文章内容&#x27;</span>,<br>        `update_time`     <span class="hljs-type">TIMESTAMP</span>  <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;更新时间&#x27;</span>,<br>        `title`           <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">255</span>) <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;标题&#x27;</span>,<br>        `origin_link`     <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">500</span>) <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;原文链接&#x27;</span>,<br>        `biz_unique_id`   <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">255</span>) <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;业务唯一id&#x27;</span>,<br>        `publish_time`    <span class="hljs-type">TIMESTAMP</span>  <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;发布时间&#x27;</span>,<br>        `evaluate`        <span class="hljs-type">TEXT</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;算法分析新闻影响（正/负面、中立）&#x27;</span><br>    ) ENGINE = InnoDB<br>      <span class="hljs-keyword">DEFAULT</span> CHARSET = utf8mb4 <span class="hljs-keyword">COMMENT</span> =<span class="hljs-string">&#x27;资讯新闻信息表&#x27;</span>;<br>      <br>      <br>      <span class="hljs-keyword">show</span> <span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> dim.dim_test2;<br>      <br>      <span class="hljs-comment">-- show create table 后的结果</span><br>    <br>      <span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> `dim_test2` (<br>      `id` <span class="hljs-type">bigint</span>(<span class="hljs-number">20</span>) <span class="hljs-keyword">DEFAULT</span> <span class="hljs-keyword">NULL</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;主键&#x27;</span>,<br>      `enterprise_name` <span class="hljs-type">text</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;算法识别涉及企业，逗号拼接&#x27;</span>,<br>      `<span class="hljs-keyword">type</span>` <span class="hljs-type">text</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;算法识别资讯类型（科技资讯，其他）&#x27;</span>,<br>      `source` <span class="hljs-type">varchar</span>(<span class="hljs-number">255</span>) <span class="hljs-keyword">DEFAULT</span> <span class="hljs-keyword">NULL</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;来源&#x27;</span>,<br>      `article_content` <span class="hljs-type">text</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;文章内容&#x27;</span>,<br>      `update_time` <span class="hljs-type">timestamp</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span> <span class="hljs-keyword">DEFAULT</span> <span class="hljs-built_in">CURRENT_TIMESTAMP</span> <span class="hljs-keyword">ON</span> <span class="hljs-keyword">UPDATE</span> <span class="hljs-built_in">CURRENT_TIMESTAMP</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;更新时间&#x27;</span>,<br>      `title` <span class="hljs-type">varchar</span>(<span class="hljs-number">255</span>) <span class="hljs-keyword">DEFAULT</span> <span class="hljs-keyword">NULL</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;标题&#x27;</span>,<br>      `origin_link` <span class="hljs-type">varchar</span>(<span class="hljs-number">500</span>) <span class="hljs-keyword">DEFAULT</span> <span class="hljs-keyword">NULL</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;原文链接&#x27;</span>,<br>      `biz_unique_id` <span class="hljs-type">varchar</span>(<span class="hljs-number">255</span>) <span class="hljs-keyword">DEFAULT</span> <span class="hljs-keyword">NULL</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;业务唯一id&#x27;</span>,<br>      `publish_time` <span class="hljs-type">timestamp</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span> <span class="hljs-keyword">DEFAULT</span> <span class="hljs-string">&#x27;0000-00-00 00:00:00&#x27;</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;发布时间&#x27;</span>,<br>      `evaluate` <span class="hljs-type">text</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;算法分析新闻影响（正/负面、中立）&#x27;</span><br>    ) ENGINE=InnoDB <span class="hljs-keyword">DEFAULT</span> CHARSET=utf8mb4 <span class="hljs-keyword">COMMENT</span>=<span class="hljs-string">&#x27;资讯新闻信息表&#x27;</span><br>    <br>    <span class="hljs-comment">-- 解决办法：将timestamp的字段默认值设置为null</span><br>    <span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> dim.dim_test2<br>    (<br>        `id`              <span class="hljs-type">BIGINT</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;主键&#x27;</span>,<br>        `enterprise_name` <span class="hljs-type">TEXT</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;算法识别涉及企业，逗号拼接&#x27;</span>,<br>        `<span class="hljs-keyword">type</span>`            <span class="hljs-type">TEXT</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;算法识别资讯类型（科技资讯，其他）&#x27;</span>,<br>        `source`          <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">255</span>) <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;来源&#x27;</span>,<br>        `article_content` <span class="hljs-type">TEXT</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;文章内容&#x27;</span>,<br>        `update_time`     <span class="hljs-type">TIMESTAMP</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">default</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;更新时间&#x27;</span>,<br>        `title`           <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">255</span>) <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;标题&#x27;</span>,<br>        `origin_link`     <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">500</span>) <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;原文链接&#x27;</span>,<br>        `biz_unique_id`   <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">255</span>) <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;业务唯一id&#x27;</span>,<br>        `publish_time`    <span class="hljs-type">TIMESTAMP</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">default</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;发布时间&#x27;</span>,<br>        `evaluate`        <span class="hljs-type">TEXT</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-string">&#x27;算法分析新闻影响（正/负面、中立）&#x27;</span><br>    ) ENGINE = InnoDB<br>      <span class="hljs-keyword">DEFAULT</span> CHARSET = utf8mb4 <span class="hljs-keyword">COMMENT</span> =<span class="hljs-string">&#x27;资讯新闻信息表&#x27;</span>;<br></code></pre></td></tr></table></figure>

<ul>
<li><p>可以发现TIMESTAMP默认加上了 NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP</p>
</li>
<li><p>为防止这种情况，可以将timestamp的字段默认值设置为null。</p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>shell介绍</title>
    <url>/2020/10/31/shell_introduction/</url>
    <content><![CDATA[<h1 id="shell介绍"><a href="#shell介绍" class="headerlink" title="shell介绍"></a>shell介绍</h1><h2 id="shell简介"><a href="#shell简介" class="headerlink" title="shell简介"></a>shell简介</h2><ul>
<li><p>shell是一个命令行解释器，它接受应用程序或者用户命令，然后调用操作系统内核</p>
</li>
<li><p>Linux提供的解释器有：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">[root@hadoop101 ~]$ cat /etc/shells <br>/bin/sh<br>/bin/bash<br>/sbin/nologin<br>/bin/dash<br>/bin/tcsh<br>/bin/csh<br></code></pre></td></tr></table></figure>
</li>
<li><p>Centos默认的解析器是bash</p>
</li>
<li><p>bash和sh的关系：sh是引用了bash，最终sh实际上还是bash命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">[root@hadoop101 bin]$ ll | grep bash<br>-rwxr-xr-x. 1 root root 941880 5月  11 2016 bash<br>lrwxrwxrwx. 1 root root      4 5月  27 2017 sh -&gt; bash<br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="shell脚本基本格式"><a href="#shell脚本基本格式" class="headerlink" title="shell脚本基本格式"></a>shell脚本基本格式</h2><ul>
<li><p>脚本以<font color=red>#!&#x2F;bin&#x2F;bash</font>开头</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">[root@hadoop101 datas]$ touch helloworld.sh<br>[root@hadoop101 datas]$ vi helloworld.sh<br><br>在helloworld.sh中输入如下内容<br><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br>echo &quot;helloworld&quot;<br></code></pre></td></tr></table></figure>
</li>
<li><p>执行方式</p>
<ul>
<li><p>采用bash或sh+脚本的相对路径或绝对路径（不用赋予脚本+x权限）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">sh helloworld.sh<br>bash helloworld.sh<br></code></pre></td></tr></table></figure>


</li>
<li><p>采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限+x）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">chmod +x helloworld.sh<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>第一种执行方法，本质是bash解析器帮你执行脚本，所以脚本本身不需要执行权限。第二种执行方法，本质是脚本需要自己执行，所以需要执行权限。</p>
</li>
</ul>
<h2 id="shell中的变量"><a href="#shell中的变量" class="headerlink" title="shell中的变量"></a>shell中的变量</h2><h3 id="系统变量"><a href="#系统变量" class="headerlink" title="系统变量"></a>系统变量</h3><ul>
<li><p>常用系统变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">[root@hadoop101 datas]$ echo $HOME<br>/home/atguigu<br><span class="hljs-meta prompt_"># </span><span class="language-bash">常用系统变量 <span class="hljs-variable">$HOME</span>、<span class="hljs-variable">$PWD</span>、<span class="hljs-variable">$SHELL</span>、<span class="hljs-variable">$USER</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">显示当前shell中所有变量</span><br>set<br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="自定义变量"><a href="#自定义变量" class="headerlink" title="自定义变量"></a>自定义变量</h3><ul>
<li><p>（1）定义变量：变量&#x3D;值 </p>
<p>（2）撤销变量：unset 变量</p>
<p>（3）声明静态变量：readonly变量，注意：不能unset</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">[root@hadoop101 datas]$ A=5<br>[root@hadoop101 datas]$ echo $A<br>5<br>[root@hadoop101 datas]$ unset A<br>[root@hadoop101 datas]$ echo $A<br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="特殊变量"><a href="#特殊变量" class="headerlink" title="特殊变量"></a>特殊变量</h3><figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">	<span class="hljs-variable">$n</span>	（功能描述：n为数字，<span class="hljs-variable">$0</span>代表该脚本名称，<span class="hljs-variable">$1</span>-<span class="hljs-variable">$9</span>代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如<span class="hljs-variable">$&#123;10&#125;</span>）</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-variable">$#</span>	（功能描述：获取所有输入参数个数，常用于循环）。</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">	$*	（功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体）</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">	<span class="hljs-variable">$@</span>	（功能描述：这个变量也代表命令行中所有的参数，不过<span class="hljs-variable">$@</span>把每个参数区分对待）</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">$？	（功能描述：最后一次执行的命令的返回状态。如果这个变量的值为0，证明上一个命令正确执行；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了。）</span><br><br></code></pre></td></tr></table></figure>



<h2 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h2><figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">（1）“$((运算式))”或“$[运算式]”</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">（2）<span class="hljs-built_in">expr</span>  + , - , \*,  /,  %    加，减，乘，除，取余</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">注意：<span class="hljs-built_in">expr</span>运算符间要有空格</span><br>expr 2 + 3<br></code></pre></td></tr></table></figure>



<h2 id="条件判断"><a href="#条件判断" class="headerlink" title="条件判断"></a>条件判断</h2><figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">[ condition ]（注意condition前后要有空格）</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">注意：条件非空即为<span class="hljs-literal">true</span>，[ atguigu ]返回<span class="hljs-literal">true</span>，[] 返回<span class="hljs-literal">false</span>。</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">两个整数之间比较</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">2. 常用判断条件</span><br>字符串比较<br><br>字符串比较：<br>    =       等于,如:if [ &quot;$a&quot; = &quot;$b&quot; ]<br>    ==     等于,如:if [ &quot;$a&quot; == &quot;$b&quot; ], 与=等价<br>               注意:==的功能在[[]]和[]中的行为是不同的,如下:<br>               1 [[ $a == z* ]]    # 如果$a以&quot;z&quot;开头(模式匹配)那么将为true<br>               2 [[ $a == &quot;z*&quot; ]] # 如果$a等于z*(字符匹配),那么结果为true<br>               3<br>               4 [ $a == z* ]      # File globbing 和word splitting将会发生<br>               5 [ &quot;$a&quot; == &quot;z*&quot; ] # 如果$a等于z*(字符匹配),那么结果为true<br><br>    !=      不等于,如:if [ &quot;$a&quot; != &quot;$b&quot; ]， 这个操作符将在[[]]结构中使用模式匹配.<br>    &lt;       小于,在ASCII字母顺序下.如:<br>               if [[ &quot;$a&quot; &lt; &quot;$b&quot; ]]<br>               if [ &quot;$a&quot; \&lt; &quot;$b&quot; ]     在[]结构中&quot;&lt;&quot;需要被转义.<br>    &gt;       大于,在ASCII字母顺序下.如:<br>           if [[ &quot;$a&quot; &gt; &quot;$b&quot; ]]<br>           if [ &quot;$a&quot; \&gt; &quot;$b&quot; ]  在[]结构中&quot;&gt;&quot;需要被转义.<br>    -z       字符串为&quot;null&quot;.就是长度为0.<br>    -n       字符串不为&quot;null&quot;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">（1）两个整数之间比较</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">-lt 小于（less than）			-le 小于等于（less equal）</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">-eq 等于（equal）				-gt 大于（greater than）</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">-ge 大于等于（greater equal）	-ne 不等于（Not equal）</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">（2）按照文件权限进行判断</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">-r 有读的权限（<span class="hljs-built_in">read</span>）			-w 有写的权限（write）</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">-x 有执行的权限（execute）</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">（3）按照文件类型进行判断</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">-f 文件存在并且是一个常规的文件（file）</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">-e 文件存在（existence）		-d 文件存在并是一个目录（directory）</span><br></code></pre></td></tr></table></figure>

<h2 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h2><ul>
<li>if语句</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">（1）[ 条件判断式 ]，中括号和条件判断式之间必须有空格</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">（2）<span class="hljs-keyword">if</span>后要有空格</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><br>if [ $1 -eq &quot;1&quot; ]<br>then<br>        echo &quot;banzhang zhen shuai&quot;<br>elif [ $1 -eq &quot;2&quot; ]<br>then<br>        echo &quot;cls zhen mei&quot;<br>fi<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>case语句</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">1)<span class="hljs-keyword">case</span>行尾必须为单词“<span class="hljs-keyword">in</span>”，每一个模式匹配必须以右括号“）”结束。</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">2)双分号“;;”表示命令序列结束，相当于java中的<span class="hljs-built_in">break</span>。</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">3)最后的“*）”表示默认模式，相当于java中的default。</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><br>case $1 in<br>&quot;1&quot;)<br>        echo &quot;banzhang&quot;<br>;;<br><br>&quot;2&quot;)<br>        echo &quot;cls&quot;<br>;;<br>*)<br>        echo &quot;renyao&quot;<br>;;<br>esac<br></code></pre></td></tr></table></figure>


</li>
<li><p>for循环</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">基本语法一：</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><br>s=0<br>for((i=0;i&lt;=100;i++))<br>do<br>        s=$[$s+$i]<br>done<br><span class="hljs-meta prompt_"># </span><span class="language-bash">基本语法二：</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">打印数字</span><br><br>for i in $*<br>    do<br>      echo &quot;ban zhang love $i &quot;<br>    done<br></code></pre></td></tr></table></figure>


</li>
<li><p>while循环</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-keyword">while</span>循环从1叫到100</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br>s=0<br>i=1<br>while [ $i -le 100 ]<br>do<br>        s=$[$s+$i]<br>        i=$[$i+1]<br>done<br><br>echo $s<br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="read读取控制台输入"><a href="#read读取控制台输入" class="headerlink" title="read读取控制台输入"></a>read读取控制台输入</h2><figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">	<span class="hljs-built_in">read</span>(选项)(参数)</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">	选项：</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">-p：指定读取值时的提示符；</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">-t：指定读取值时等待的时间（秒）。</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">参数</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">	变量：指定读取值的变量名</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"></span><br><span class="language-bash"><span class="hljs-comment">#</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><br>read -t 7 -p &quot;Enter your name in 7 seconds &quot; NAME<br>echo $NAME<br></code></pre></td></tr></table></figure>

<h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><ul>
<li><p>系统函数</p>
</li>
<li><p>basename [string &#x2F; pathname] [suffix]  （功能描述：basename命令会删掉所有的前缀包括最后一个（‘&#x2F;’）字符，然后将字符串显示出来。</p>
</li>
<li><p>选项：</p>
<p>suffix为后缀，如果suffix被指定了，basename会将pathname或string中的suffix去掉。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">[root@hadoop101 datas]$ basename /home/atguigu/banzhang.txt <br>banzhang.txt<br>[root@hadoop101 datas]$ basename /home/atguigu/banzhang.txt .txt<br>banzhang<br></code></pre></td></tr></table></figure>
</li>
<li><p>dirname 文件绝对路径		（功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分））</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">[root@hadoop101 ~]$ dirname /home/atguigu/banzhang.txt <br>/home/atguigu<br></code></pre></td></tr></table></figure>
</li>
<li><p>自定义函数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">1．基本语法</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">[ <span class="hljs-keyword">function</span> ] funname[()]</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">&#123;</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">	Action;</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">	[<span class="hljs-built_in">return</span> int;]</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">&#125;</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">funname</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">2．经验技巧</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">	（1）必须在调用函数地方之前，先声明函数，shell脚本是逐行运行。不会像其它语言一样先编译。</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">	（2）函数返回值，只能通过$?系统变量获得，可以显示加：<span class="hljs-built_in">return</span>返回，如果不加，将以最后一条#命令运行结果，作为返回值。<span class="hljs-built_in">return</span>后跟数值n(0-255)</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br>function sum()<br>&#123;<br>    s=0<br>    s=$[ $1 + $2 ]<br>    echo &quot;$s&quot;<br>&#125;<br><br>read -p &quot;Please input the number1: &quot; n1;<br>read -p &quot;Please input the number2: &quot; n2;<br>sum $n1 $n2;<br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="shell工具"><a href="#shell工具" class="headerlink" title="shell工具"></a>shell工具</h2><h3 id="cut"><a href="#cut" class="headerlink" title="cut"></a>cut</h3><ul>
<li><p>cut:cut的工作就是“剪”，具体的说就是在文件中负责剪切数据用的。cut 命令从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段输出。</p>
</li>
<li><p>cut [选项参数]  filename     说明：默认分隔符是制表符</p>
</li>
<li><table>
<thead>
<tr>
<th align="left">选项参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td align="left">-f</td>
<td>列号，提取第几列</td>
</tr>
<tr>
<td align="left">-d</td>
<td>分隔符，按照指定分隔符分割列</td>
</tr>
<tr>
<td align="left">-c</td>
<td>指定具体字符</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">[root@hadoop101 datas]$ touch cut.txt<br>[root@hadoop101 datas]$ vim cut.txt<br>dong shen<br>guan zhen<br>wo  wo<br>lai  lai<br>le  le<br><br>[root@hadoop101 datas]$ cut -d &quot; &quot; -f 2,3 cut.txt <br>shen<br>zhen<br> wo<br> lai<br> le<br> <br> [root@hadoop101 datas]$ cut -d &quot; &quot; -f 1 cut.txt <br>dong<br>guan<br>wo<br>lai<br>le<br><br>[root@hadoop101 datas]$ cat cut.txt | grep &quot;guan&quot; | cut -d &quot; &quot; -f 1<br>guan<br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h3><ul>
<li><p>sed是一种流编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”，接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。</p>
</li>
<li><ol>
<li>基本用法</li>
</ol>
<p>sed [选项参数]  ‘command’  filename</p>
<ol start="2">
<li>选项参数说明</li>
</ol>
<table>
<thead>
<tr>
<th>选项参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-e</td>
<td>直接在指令列模式上进行sed的动作编辑。</td>
</tr>
<tr>
<td>-i</td>
<td>直接编辑文件</td>
</tr>
</tbody></table>
<ol start="3">
<li>命令功能描述</li>
</ol>
<table>
<thead>
<tr>
<th>命令</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td><strong>a</strong></td>
<td>新增，a的后面可以接字串，在下一行出现,a前面可以指定行</td>
</tr>
<tr>
<td>d</td>
<td>删除</td>
</tr>
<tr>
<td>s</td>
<td>查找并替换</td>
</tr>
</tbody></table>
</li>
</ul>
<ul>
<li><p>command中可以匹配正则表达式</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">将“mei nv”这个单词插入到sed.txt第二行下，打印。</span><br>[root@hadoop102 datas]$ sed &#x27;2a mei nv&#x27; sed.txt <br>dong shen<br>guan zhen<br>mei nv<br>wo  wo<br>lai  lai<br>le  le<br><span class="hljs-meta prompt_"># </span><span class="language-bash">文件内容并没有改变</span><br>[root@hadoop102 datas]$ cat sed.txt <br>dong shen<br>guan zhen<br>wo  wo<br>lai  lai<br>le  le<br><span class="hljs-meta prompt_"># </span><span class="language-bash">注意：‘g’表示global，全部替换</span><br>[atguigu@hadoop102 datas]$ sed &#x27;s/wo/ni/g&#x27; sed.txt <br>dong shen<br>guan zhen<br>ni  ni<br>lai  lai<br>le  le<br><br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h3><ul>
<li><p>一个强大的文本分析工具，把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行分析处理。</p>
</li>
<li><ol>
<li>基本用法</li>
</ol>
<p>awk [选项参数] ‘pattern1{action1} pattern2{action2}…’ filename</p>
<p>pattern：表示AWK在数据中查找的内容，就是匹配模式</p>
<p>action：在找到匹配内容时所执行的一系列命令</p>
<ol start="2">
<li>选项参数说明</li>
</ol>
<table>
<thead>
<tr>
<th>选项参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-F</td>
<td>指定输入文件折分隔符</td>
</tr>
<tr>
<td>-v</td>
<td>赋值一个用户定义变量</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">（1）搜索passwd文件以root关键字开头的所有行，并输出该行的第7列。</span><br>[root@hadoop102 datas]$ awk -F: &#x27;/^root/&#123;print $7&#125;&#x27; passwd <br>/bin/bash<br><span class="hljs-meta prompt_">#</span><span class="language-bash">（2）搜索passwd文件以root关键字开头的所有行，并输出该行的第1列和第7列，中间以“，”号分割。</span><br>[root@hadoop102 datas]$ awk -F: &#x27;/^root/&#123;print $1&quot;,&quot;$7&#125;&#x27; passwd <br>root,/bin/bash<br><span class="hljs-meta prompt_">#</span><span class="language-bash">注意：只有匹配了pattern的行才会执行action</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">（3）只显示/etc/passwd的第一列和第七列，以逗号分割，且在所有行前面添加列名user，shell在#最后一行添加<span class="hljs-string">&quot;dahaige，/bin/zuishuai&quot;</span>。</span><br>[root@hadoop102 datas]$ awk -F : &#x27;BEGIN&#123;print &quot;user, shell&quot;&#125; &#123;print $1&quot;,&quot;$7&#125; END&#123;print &quot;dahaige,/bin/zuishuai&quot;&#125;&#x27; passwd<br>user, shell<br>root,/bin/bash<br>bin,/sbin/nologin<br>。。。<br>atguigu,/bin/bash<br>dahaige,/bin/zuishuai<br><span class="hljs-meta prompt_">#</span><span class="language-bash">注意：BEGIN 在所有数据读取行之前执行；END 在所有数据执行之后执行。</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">（4）将passwd文件中的用户<span class="hljs-built_in">id</span>增加数值1并输出</span><br>[root@hadoop102 datas]$ awk -v i=1 -F: &#x27;&#123;print $3+i&#125;&#x27; passwd<br>1<br>2<br>3<br>4<br></code></pre></td></tr></table></figure></li>
</ul>
<ul>
<li><p>awk的内置变量</p>
<table>
<thead>
<tr>
<th>变量</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>FILENAME</td>
<td>文件名</td>
</tr>
<tr>
<td>NR</td>
<td>已读的记录数</td>
</tr>
<tr>
<td>NF</td>
<td>浏览记录的域的个数（切割后，列的个数）</td>
</tr>
</tbody></table>
</li>
</ul>
<h3 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h3><ul>
<li><p>sort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。</p>
</li>
<li><p>基本语法</p>
<p>sort(选项)(参数)</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>依照数值的大小排序</td>
</tr>
<tr>
<td>-r</td>
<td>以相反的顺序来排序</td>
</tr>
<tr>
<td>-t</td>
<td>设置排序时所用的分隔字符</td>
</tr>
<tr>
<td>-k</td>
<td>指定需要排序的列</td>
</tr>
</tbody></table>
<p>参数：指定待排序的文件列表</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>shuju_qiinxie</title>
    <url>/2021/10/22/shuju-qiinxie/</url>
    <content><![CDATA[<h1 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h1><h2 id="数据倾斜描述"><a href="#数据倾斜描述" class="headerlink" title="数据倾斜描述"></a>数据倾斜描述</h2><ul>
<li>正常的数据分布理论上都是倾斜的，就是常说的2-8原则。不同的数据字段可能的数据倾斜一般有两种情况：<ul>
<li>唯一值非常少</li>
<li>唯一值比较多</li>
</ul>
</li>
</ul>
<h2 id="数据倾斜产生的原因"><a href="#数据倾斜产生的原因" class="headerlink" title="数据倾斜产生的原因"></a>数据倾斜产生的原因</h2><ul>
<li>数据倾斜在MapReduce编程模型中十分常见,用最通俗易懂的话来说,数据倾斜无非就是大量的相同key被partition分配到一个分区里,造成了’一个人累死,其他人闲死’的情况,这种情况是我们不能接受的,这也违背了并行计算的初衷,首先一个节点要承受着巨大的压力,而其他节点计算完毕后要一直等待这个忙碌的节点,也拖累了整体的计算时间,可以说效率是十分低下的。</li>
<li>产生的现象：<ul>
<li>其它的任务都执行完成了，一个任务一直卡在某个进度，一直没有完成。</li>
<li>本来应该能正常执行的任务，出现OOM(内存溢出)。</li>
</ul>
</li>
</ul>
<h3 id="数据倾斜的解决"><a href="#数据倾斜的解决" class="headerlink" title="数据倾斜的解决"></a>数据倾斜的解决</h3><ul>
<li>数据倾斜只会发生在shuffle中，在hive中，表的join,group by。在spark中常见可能会触发shuffle操作的算子：distinct，groupByKey,reduceByKey,aggregateByKey,join,cogroup,repartition等。</li>
<li>对于hive HQL来说解决的办法：<ul>
<li>group by 时的数据倾斜:解决办法：hive.groupby.skewindata&#x3D;true<ul>
<li>控制生成两个MRJob，第一个MR的map的输出结果随机分配到reduce中，减少某些key值条数过多，某些key 值条数过少造成的数据倾斜问题。</li>
<li>第二个MR再根据预处理的数据结果，按照group By Key分布到reduce中，（这个过程可以保证相同的Group By Key分布到同一个reduce中），最后完成最终的聚合操作。</li>
</ul>
</li>
<li>join时数据倾斜，对于一些热点key，他们所在的reduce运行较慢。解决方式：<ul>
<li>过滤不正常的记录</li>
<li>数据去重</li>
<li>使用map join</li>
<li>打开set Hive.optimize.skewjoin &#x3D; true;</li>
</ul>
</li>
<li>无法从sql层面优化时，可以从业务算法角度提升<ul>
<li>只读需要的列和分区</li>
<li>周，月任务按天累计计算</li>
<li>先聚合后join</li>
</ul>
</li>
<li>Reduce任务数不合理<ul>
<li>set mapred.reduce.tasks&#x3D;N</li>
</ul>
</li>
<li>业务数据本身存在倾斜<ul>
<li>配置参数，优化SQL语句，将数据值较多的数据分散到多个reduce中</li>
</ul>
</li>
<li>多维分析带来的低效<ul>
<li>当含有rollup和cube语句的from子句中包含子查询或者join时，将子查询或join的结果放到临时表，然后从临时表中读取数据做多维分析</li>
</ul>
</li>
</ul>
</li>
<li>对于Spark来说解决的办法<ul>
<li>使用hive  ETL预处理数据</li>
<li>过滤少量导致数据倾斜的key</li>
<li>提高shuffle操作的并行度</li>
<li>两阶段聚合<ul>
<li>第一次是局部聚合，先给每个key打上一个随机数，这样就不会数据倾斜了，先局部聚合</li>
<li>第二次是全局聚合，把随机前缀去掉，再进行全局聚合</li>
</ul>
</li>
<li>将reduce join 转为map join</li>
</ul>
</li>
<li>spark数据倾斜的解决详细内容可见博客<a href="https://blog.csdn.net/weixin_35353187/article/details/84303518">spark 数据倾斜</a></li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>数据倾斜</tag>
      </tags>
  </entry>
</search>
